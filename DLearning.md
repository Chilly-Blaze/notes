# Deep Learning

总之还是迈入深度学习的大坑了，不求学的很精，能拿来装装逼足以

注意，深度学习并不是一下子就能够学明白的，因此在此之前默认已经了解了一些前置知识，包括线代，概率论，微积分，一丢丢的机器学习，一丢丢的python

由于暂时还没找到让我十分满意的整套学习资料，要么过于理论，要么过于实战，导致可能没有系统归纳的记录，可能会使得某些内容不够流畅

## 开发环境入门

反正我也不知道别人是怎么开始学习的，对我来说，我只着急看看学完之后会产生一个什么玩意，因此咱也不搞那些枯燥的理论，不如一开始就直接给自己安装一个pytorch先再说别的

### Pytorch

这玩意看上去挺复杂，又是什么GPU又是什么anaconda的，反正我啥也没管，根本不想安装conda，直接使用pip装，也不知道后续会出现什么问题

1. 直接进入[官网](https://pytorch.org/get-started/locally/#start-locally)，然后我反正是这么选的

   ![image-20220930215544781](https://s2.loli.net/2022/09/30/Fx2f8T37piSH4OK.png)

   > 注意其中的CUDA部分，如果你有显卡，一般来说选最高版本就不会有问题，如果没有GPU就选后面的CPU，如果实在是你有一个老掉牙的GPU那就自己查去自己的CUDA版本，出现错误我也没辙

2. 记得给自己电脑下一个pip，怎么下自己搜去，然后打开控制台，没配置好镜像源的在他给的命令后面加`-i [镜像地址]`即可，例如用清华源，那么最后命令为`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116 -i https://pypi.tuna.tsinghua.edu.cn/simple`

3. 控制台输入python，然后`import torch`再`torch.__version__`验证是否装好`torch.cuda.is_available()`验证GPU是否可用，再大胆一些`torch.cuda.get_device_name()`输出你的显卡名称，比较装逼

### Jupyter

这玩意我也不知道为什么要装，但既然网上说是数据分析人士专用装逼工具，那装一个也无妨，不过我看那个网页版的越看越丑，于是安装的是VSCode里面的插件版的，稍微美观一丢丢吧

1. 插件市场找到Jupyter，下载安装
2. 新建一个工作目录，新建一个文件`xxx.ipynb`或者`Ctrl+Shift+P`先调出那个神秘玩意然后选择`Create: New Jupyter Notebook`也可以
3. 随便输一些python语句，`Win+Enter`执行，如果出现提示要选择内核或者安装插件什么的之间确认就行了

### 简单语法

我们都知道神经网络啥的说到底就是吧一堆矩阵搞来搞去，因此既然作为这方面扛把子的存在，自然也内置了一大堆处理矩阵相关的语法，了解一下就完了，对了，在这里面可不能叫矩阵了，对于1-n维的不同矩阵，有了一个洋气的名字——张量（Tensor），土一点就是多维数组

- `torch.tensor()`创建一个自定义张量，第一项传入的是一个张量阵，第二项传入张量的属性（可选），其中第二项可以指定张量成员的数据类型（`dtype`，默认为`torch.float32`），可以指定张量是否需要求解梯度（requires_grad，默认为false）

  > 我们注意到虽然默认张量的成员看上去好像是一个简单的浮点型，但是其实这是torch已经封装好的专属于张量的浮点型，于此类似的还有`torch.int32`、`torch.double`等等

- 除了通过人为指定的方式，torch还提供了快速生成规格化向量的方法，我们可以通过`torch.ones/zeros/rand/randn()`来快速生成一个指定张量，其中传入的是若干整数，通过逗号分隔，表示每一维度上的列数，而`rand`和`randn`的区别在于后者是在`(0,1)`上取正态分布

  ![image-20221003211225114](https://s2.loli.net/2022/10/03/1zcd2mvPYZal56s.png)
  
- 之后我们需要了解一下张量的三维维度关系，首先，对于二维来说默认行在前列在后，比如`torch.zeros(3,2)`就会生成一个三行两列的张量，而对于三维来说，我们假设有一个正面对我们的立方体，那么维度的先后顺序即为宽，高，长，比如`torch.zeros(2,3,4)`则会生成这么一个张量

  ![image-20221003212418485](https://s2.loli.net/2022/10/03/F5RewZYgzVTcWs6.png)

  对于显示来说，其显示方式为从后到前逐面展示数据，如上图先展示后面红色的部分二维张量，接着展示前面绿色部分的二维张量

  ![image-20221003222524628](https://s2.loli.net/2022/10/03/Bo9bQ1O7iaLYEwt.png)

  由于大于三维的张量比较少见，因此只需要掌握1,2,3维的张量即可

  除了生成张量方面，对于一个存在的张量，我们还可以对其做各种操作，而部分操作要求的张量维数不同，我们需要具体分析，以下我们先假设`x2 = torch.rand(3,4)`、`x3 = torch.rand(2,3,4)`

- `x2.view(2,6)`会将张量转化为2*6的形式，顺序保持先行后列转换

- `x2.t()/x2.t_()/x2.T`求转置张量，第一个会返回一个深拷贝的转置张量，第二不返回内容，会直接将x2转化为他自己的转置，第三个则为浅拷贝，只作为显示x2的一个属性存在

  > 在此处我们发现了张量很多的方法都在末尾会有一个下划线，所有的这种方法其实都是标识着该操作作用于指定张量自身，与此类似的还有`x2.zero_()/x2.add_()`等等，等价于`x2 = x2.add()`之后就不再列举类似方法，只需要掌握原生方法即可进行推导

- `x3.transpose(1,2)`会将x3的第二维和第三维所构成的二维张量进行转置，其中1,2分别代表二三维，由于大于二维张量无法直接转置因此需要使用特殊方法，二维张量使用这个方法会报错

  ![image-20221003224946200](https://s2.loli.net/2022/10/03/aOTqjMtA6wIcC4V.png)

- `x3.permute(2,0,1)`会将张量第三维变成第一维，第一维变成第二维，第二维变成第三维，和上面那个不一样的是这个可以进行三维张量这么一个立方体做类似翻转的操作，当然如果指定参数为`x3.permute(0,2,1)`那么他就等于`x3.transpose(1,2)`

  ![image-20221003225008687](https://s2.loli.net/2022/10/03/L65nAUxMq2ZQkvu.png)

当然张量的操作远远不止这些，如果我们尝试`x2.`就会发现多的数不清的方法和属性，不过大部分都是常规的张量操作，往往通过命名就可以大致猜到实际作用，不理解的部分也可以通过查阅[文档](https://pytorch.org/docs/stable/search.html)来解决，在这里提到的这些只是作为一些例子来进一步了解张量这个玩意而已

## 理论基础

对于完全不了解深度学习的人来说，虽然他们一定听说过什么神经网络，什么Alphago，但是往往对于这些名词都怀有一种敬畏之心，感觉是遥不可及的高大上词汇，也就不太愿意实际去了解一下其中的原理了，其实我们会发现，其实这些听上去很可怕的名词，背后的原理实际十分简单，于是我们就在此用最通俗的语言浅浅的剖析一下这些所谓的“专业术语”

### 线性回归

现在假设有这么一个机器，我告诉他今天的日期，他就会根据这个日期经过某些规则返回给我们今天的三餐应该吃什么，简单来说，就是存在某一个函数，我们知道他的输入和输出，但是我们不知道他的函数结构，但是突然某一天这个机器坏掉了，但是情况是如果没有这个机器，我们就会选择困难，不知道自己吃什么然后被饿死

为了不让我们被饿死，我们必须去寻找之前所有的日期和三餐之间的关系，去尝试重新构造这个机器，而这就是一个典型的回归问题

为了便于理解，我们就先假设所有的食物都可以用一个数来表示，所有的日期同样可以通过一个数表示，那么问题 就变成了，给我们一个数，我们需要去生成另一个数，使生成的这个数尽可能接近原来机器的输出

比如我们现在想起了前三天我们吃的内容，他们是$（1,3）$、$（2,5）$、$（3,7）$，意思可能是第一天我吃了3这么一个食物，第二天吃了5这个食物，第三天吃了7这个食物，先不关心我们究竟吃了啥，我现在想用一个f函数去预测第四天我应该吃什么

注意到，在这其中，我们称能够决定输出结果的输入值为特征值，称寻找什么参数和输出有关系的过程称作特征提取，在这个问题中就是日期这个因素，之后非常常见的是根据多个特征值产生一个输出的情况

这时候我可能并不是很愿意在吃饭这件事上面过于纠结，于是我希望尽可能简单的构造一个预测模型，去比较接近的还原原本机器提供的函数，于是我假设了一个$f=wx+b$，我们称之为预测函数，其中x就是第x天，f就是第x天应该吃什么，这个模型看上去很简单，我们完全可以把上面的三组数据带进去解出w和b，事实上我们确实可以完美的解出$w=2,b=1$这么一个结果，然后我们非常容易的得到了第四天我应该吃9这个食物，但是如果第三天我们吃到了8号食物，事情就变得有些麻烦了，我们无法找出一条完美经过所有点的直线，但是我们又懒得重新构造一个函数重新求解，这时候，问题就变成了如何找出一条直线能够尽可能的接近这三个点

![image-20221004154950500](https://s2.loli.net/2022/10/04/7vQIRJTN8Mws6UV.png)

尤其是在数据越来越多的情况下，比如现在我们知道了连续一百天的食物，那么就更加困难用一条直线完美契合所有的数据点，这时候我们就想到了，或许我们不需要十分严谨的准确预测每天吃的内容，或许7号食物和8号食物并没有什么差别，就算原本需要吃的是8号，但现在我们自己构造的预测机告诉我们吃7号其实也还凑合，差不多能够接近就已经很满意了

### 损失函数/均方误差

为了构造出最接近真实情况的直线，我们引入了一个损失函数$L(f)=\sum_{n=1}^{3} (\hat{y}_n -f(x_n))^2$，这玩意看上去挺复杂，实际上就是针对每一个w和b生成一个评判他们好坏的指标，这个指标由竖直方向上点和预测函数值做差平方，累和后得到，表示所有点和预测函数之间距离之和，对于上面那个$f=2x+1$来说，n=3时，$\hat{y}_3=8,f(x_3)=7$，而对所有的n，有
$$
L(f)=(3-3)^2+(5-5)^2+(8-7)^2=1
$$
于是我们所画的这条直线对于$（1,3）$、$（2,5）$、$（3,8）$这三个数据来说的误差值为1，看上去还不错，这条函数的误差累和起来仅和事实偏差了1个单位，但是我们如果令$f(x)=2.5x+\frac{1}{3} $，我们会发现损失函数的值变成了前所未有的$\frac{1}{6}$，这显然比前一个函数更加优秀，那还存不存在偏差更小的预测函数呢？

![image-20221004164245836](https://s2.loli.net/2022/10/04/7rMQ1HZSdAzwjuR.png)

### 梯度下降

我们似乎发现了，想要让我们的预测函数更加精准，我们就需要让$L(f)$越小越好，即去寻找他的最小值，这时候解出来的$f$大概率就能十分准确的满足我们的需求，而想要让$f$使得$L(f)$最小，我们就需要解出那个使其最小的$w$和$b$，幸运的是，已经有证明表示，当n够大的时候，这个令其最小的$w$和$b$是一定存在且唯一的，于是我们可以更加大胆的去求解所谓的$w$和$b$了

一眼看过去，这$L(f)$长得好奇怪，完全不知道如何下手，确实，这个函数似乎并不是那么好通过我们浅薄知识中的某一种公式求解他的最小值，但是我们还需要记住，我们可是还有计算机这个伟大的工具，这个工具别的不行，唯有机械化的操作他最为擅长，哪怕是穷举所有的$w$和$b$，只要你一声令下，他也会立马做给你看

在研究$L(f)$之前，我们先从简单一些的函数入手，比如我们有一个$f(x)=x^2$，这时候我们不妨思考一个问题，假设我们现在化身为一个$(1,1)$点，它显然是在这个函数上的，现在规定我们只允许在函数线上移动，怎么动可以让我们的f值更小呢

当然，因为我们知道$(0,0)$是这个函数的最小值，所以我们理所当然的会往0方向移动，但是假如我们根本不知道这个函数长什么鸟样，可能有聪明的小朋友会想到利用导数了，只要这个函数是可微的，那么我们在任何一点都能够求出来一个导数，而这个导数表明了函数在这一点的下降方向，而由于导数的局部性，我们完全不需要知道整个函数的图像，只需要知道函数的导函数，取到我们当前所位于的点，如果结果为正则说明我们只需要往左走一丢丢，那么百分百结果比当前的值小，为负则同理往右走一丢丢

对于上面的例子，$f(x)=x^2$在$(1,1)$处导数为2，显然是一个正值，意味着我们至少可以向左移动一丢丢，于是我们打算移动0.1看看，然后我们发现$f(x)$在0.9处函数值为0.81，显然比原本的1更小了，也进一步说明我们的方法是有效的，接着我们只需要不断重复这个过程，直到导数趋近于0，就可以顺利的找出函数的极小值

![image-20221004173056253](https://s2.loli.net/2022/10/04/Zx4KhLcC6u59MG7.png)

于是我们得到了一个函数求解极小值的通用方法，这就是所谓的梯度下降，之所以是极小值同样是由于这个方法最后收敛到的地方一定是局部最优而不是全局最优，其中原因相信就不需要我过多描述了，最简单的理解就是不是所有函数导数为0的地方都只有一个，而梯度下降到导数为0时终止，从而并不会穷举所有为0的点进行比较

不过这些都不需要担心，因为更加幸运的是，**已经被证明，对于上面我们提到的线性回归这种线性问题，乃至所有预测函数为单调函数的情况来说，其构造的损失函数，在偏导数梯度下降求解的极小值也就一定是全局的最小值**，妈妈再也不用担心我的梯度下降失败了

讲了那么多，现在我们重新审视一开始的$L(f)$，如今我们知道了，根据上面所推导的通用方法，在两个未知参数的情况下，我们需要获得他的最小值，稍微变通一下，其实就是求得$w$和$b$两个参数对于$L(f)$的偏导函数$\frac{\partial L}{\partial w} $和$\frac{\partial L}{\partial b} $，然后先随便取初始的$w$和$b$带入其中，根据获得结果的正负来分别对$w$和$b$进行变化，当$w$和$b$都为0时即取得了最优的结果$w$和$b$

于是，如果我们假设前一次的$w$和$b$是$w_k$和$b_k$，那么k+1次迭代时的$w_{k+1}$就有以下式子
$$
w_{k+1}=w_k-\eta\frac{\partial L}{\partial w}|_{w=w_{k}}
$$
其中的$\eta$表示学习率（Learning Rate），他由人为指定，表示每一个梯度下降的幅度，他的值越大就下降的越快，但是可能会出现在最小值附近徘徊的情况，他的值越小，下降的速度越慢，但是意味着最后的结果也会更加接近真实最低点

### 感知机

如今我们已经完全了解了线性回归是什么一个玩意，但是遗憾的是线性回归只能算是机器学习的范畴，而且还是基础中的基础，在学习接下来的内容之前，我们不妨先试想一个其他的情况

在上面的条件下，此时我发现我的机器可能并不是仅通过今天的日期来决定吃什么，毕竟这样设计机器也太low了，现在，我们已知，这台机器在每天会根据日期，湿度两个方面来生成一天的食谱，也就是此时的特征值有两个，此时显然不能够仅通过一个x日期来决定f输出，我们在模拟的时候也理应加上另外一个影响因素：湿度y

此时问题变成了$(x,y) \to f$这么一个问题，我们要根据三个可变的参数来生成一个f，因为此时仅通过一个$w$和$b$构造的线性模型显然有些不够用了，但是聪明的我们参照着原有的样式，构造出了$f(x,y)=w_1x+w_2y+b$这么一个函数，于是乎，问题也从求解$w$和$b$变成了求解$w_1,w_2,b$三个参数，虽然看上去参数挺多，但是由于不存在二次项，因此它仍旧只是一个线性回归问题，我们也同样可以通过无脑的梯度下降方法分别求解这几项的偏导从而获得f的最小值

如今，我们距离传说中大名鼎鼎的感知机（有些时候我们也可以称之为神经元），就只差一步之遥了，其实，感知机的全貌一点也不神秘，他就长这个样子

![image-20221006192135647](https://s2.loli.net/2022/10/06/7wztWpKDb6jxRIH.png)

通过和这图比对，我们发现了我们还缺了一个激活函数，而这个激活函数到底是什么玩意，我们还得重新回到最开头的问题上来

### 逻辑回归

上面的问题看似已经通过线性回归解决了，解决的方式也看上去十分的美好，但细心的小伙伴可能会发现一个小漏洞，就是你如何可以将所有的食谱一对一的映射到所有的实数上？甚至可以保证相邻之间的食谱还具有一定的相似性？

这显然是做不到的，因此上面的问题从假设开始就是一派胡言，我们得寻找一种更加科学的能够还原原有机器的办法

我们现在假设最简单的情况，假设这个机器只能告诉我们今天要不要吃米饭，我们给他一个日期和一个湿度，他只给我们返回两个结果，即“吃米饭”和“不吃米饭”，翻译成数学语言就是$(x,y) \to (-1,1)$，其中-1代表不吃，1代表吃

如何具象化这么一个问题呢，我们不妨假设现在有三组数据，他们是$(1,1)\to -1,(2,2)\to -1,(3,1)\to 1$，可以表示第一天湿度1不吃米饭，第二天湿度2不吃米饭，第三天湿度3吃米饭，从这三组数据中我们可以获得什么信息呢？比如第四天湿度5我们是否要吃米饭呢？乍一看似乎没什么思路，但是这种多输入单输出的模型让我们想到了，我们是否同样可以使用所谓的线性回归模型来求解呢？

说干就干，我们同样借助$f(x,y)=w_1x+w_2y+b$构造预测函数，同样通过均方误差的方式$L(f)=\sum_{n=1}^{3} (\hat{y}_n -f(x_n,y_n))^2$构造损失函数，这其中只需要改一下$\hat{y}$为我们定义的吃不吃米饭的输出-1和1即可，由于这求解出来是一个三维上的平面，因此也不太方便通过平面坐标图像表示，但我们可以知道的是，当$f(x,y)=-1$时，由x和y构成的平面直线一定是所有不吃米饭的数据点的最佳拟合直线，同理，当$f(x,y)=1$时输出的则是所有吃米饭数据点的最佳拟合直线

由于该吃米饭的数据只有一个，因此$w_1,w_2,b$存在多个解，我们随便取一个结果，将$f(x,y)=0$的这一条直线画出来，我们发现了一个有趣的现象

![image-20221007165319715](https://s2.loli.net/2022/10/07/gRdWPzUYo94FEtu.png)

这条直线将数据集分成了两块，直线上方的点全部是不吃米饭，直线下方的点则是要吃米饭，这是不是意味着我们似乎也完成了所谓的分类问题呢？比如现在有$(4,5)$这个点，我们很明显发现他在直线上方，或者我们将其带入$f(x,y)$，很明显发现他的值小于0，因此我们可以秒给出今天不吃米饭的结论

你可能会说，这也太简单了，为啥还需要重新讲那么多乱七八糟的东西，这不还是线性回归吗，但是假如现在我多加了一个第四天的数据放进去训练，但是第四天的湿度是1000，按我们这种分类方法，湿度那么大显然不应该吃米饭，但是这么一个训练数据如果放到了我们线性回归的模型中，则会带来毁灭性的灾难，如这幅图所示

![image-20221007171906025](https://s2.loli.net/2022/10/07/Ez9gru3tqiPGMF2.png)

这显然看起来就不太美好了，而且事实是随着数据集维数的扩大，这种问题会越来越明显，意味着我们无法通过回归问题的模式求解分类问题

如何解决这个问题呢，非常简单，只需要在$f(x,y)$的外面套一个sigmoid函数$\sigma(f(x,y))$作为新的预测函数，然后将损失函数变成$L(f)=\sum_{n=3}-\left[\hat{y}_{n} \ln \sigma(f\left(x_n,y_n\right))+\left(1-\hat{y}_{n}\right) \ln \left(1-\sigma(f\left(x_n,y_n\right))\right)\right]$就行了，注意此时损失函数中的$\hat{y}$变成了由0和1控制的信号量，简单来说可以把上面的-1改成0即可，而其他步骤像是什么梯度下降的过程完全一致，看上去十分容易不是吗

等等等等，上面一大通玩意是什么东西，完全看不懂，事实上，背后的推导过程实在是比较复杂，一时半会也讲不清楚，其中从高斯分布开始一路推导出的结果光光[视频](https://www.bilibili.com/video/BV1Wv411h7kN/?p=16)就讲了快一个小时，感兴趣的可以学习一下，简单来说，给$f(x,y)$套的这个sigmoid函数就是所谓的激活函数，这个函数实际上就是把他从一个拟合问题变成了一个概率问题，因为sigmoid的函数图像长这样

![](https://bkimg.cdn.bcebos.com/pic/d009b3de9c82d158dfb4e7218a0a19d8bc3e426f?x-bce-process=image/resize,m_lfit,w_536,limit_1/format,f_jpg)

他的符号是$\sigma$，很容易可以发现他将函数值控制在了0-1的范围内，同时以函数值0.5作为分界，区分了其中的$f$的正负，如果把他的输出看成一个概率，那么对于吃不吃米饭这件事情来说，要吃米饭的概率大于0.5，也就是机器判断此时要吃米饭的概率比不要吃米饭的概率大，则我们就判断输出要吃米饭，这种结果也是十分直观的

另外我们想一想就可以知道由于对于函数值的限制导致他可以有效的避免之前出现的分界线因为数据集的数据极化导致的偏移问题，事实上，在更换了预测函数和损失函数之后，训练得到的结果，如果我们和之前一样将$\sigma(f(x,y))=0.5$这条直线画出来，的确十分接近我们期望的绿线

### 对数似然损失/交叉熵误差

之后就是损失函数的变化，这么一大通公式实际上叫做交叉熵误差，他在形式上并没有之前的均方误差公式一样直观，但是如果了解一下信息论会发现，这个公式在概率处理方面可谓是十分擅长，通过观察公式也能发现，由于sigmoid的函数值一定在$(0,1)$之间，且$\hat{y}$的取值恒为0或1，因此当某组数据的输出为0时，在上面的例子中就是某天某个湿度不吃米饭的结果，此时损失函数的第一项由于$\hat{y}$为0因此只剩下第二项，变成了$-\ln \left(1-\sigma(f\left(x_n,y_n\right))\right)$，而这个表达式就叫做对数似然损失，他的输出即为sigmiod函数输出接近0的程度，同时由于对数函数的存在，在sigmiod函数靠近1时输出会爆炸性增长，使得他对于表现不好的参数$w$和$b$也能够做出十分正确的反馈，同理当$\hat{y}$为1时，损失函数输出的结果即为sigmoid函数接近1的程度，使得在不同的数据集输入和输出中损失函数能够进行动态性的变化反馈

有些人就不是很服气了，这交叉熵公式那么复杂，凭什么不能继续使用均方误差呢，虽然你上面一大通解释似乎有点道理，但是我用均方误差也能解决啊，你看$(\hat{y}_n -f(x_n,y_n))^2$这玩意，在$\hat{y}=0$时输出的不也是sigmoid函数接近0的程度吗，怎么就不行了？实际上由于现在我们的$f$外面套了一个sigmoid，因此如果我们对这个损失函数求一个导，就会发现这么一个情况

![image-20221008150529941](https://s2.loli.net/2022/10/08/EYarjUSdbcZAKN6.png)

> 注意sigmoid函数的导数为$\sigma'(x)=\sigma(x)(1−\sigma(x))$

此时假如$\hat{y}=0,f(x,y)=1$时，按理说此时我们完全偏离了需要的结果，理论上来说应该产生一个巨大的梯度，但是一代进去，不对劲了，尼玛求出来$w$的偏导居然是0，这显然不符合我们的预期，如果用梯度下降，则程序会判断我们已经到达了最低点，直接停止运行了，事实上如果使用均方误差，如果把三维图画出来，就会变成这样

![image-20221008153625697](https://s2.loli.net/2022/10/08/bDX8VvHgwl9qsFE.png)

可以发现说由于均方误差的函数值域被限制在0-1之间，使得必然会在无穷远处函数的导数无限逼近0，这么一来梯度下降将无法进行，但是反观交叉熵因为我们之前也提到过log函数的存在，导致他的表现比起均方误差要好很多，通过这个例子，我们也可以发现其实梯度下降也并不是无敌的，只有面对上适宜的给定函数才能够发挥出他的优势

### 激活函数

讲了那么多乱七八糟的东西，还记得一开始我们要干嘛来着，没错，就是了解感知机当中的激活函数，通过这么一个分类问题我们引入了激活函数这么一个概念，但是也就仅仅是一个概念而已，需要注意的是虽然都叫激活函数，但是在感知机中的激活函数和逻辑回归中的激活函数所代表的含义并不一样，后者的来由是因为我们要解决分类这么一个问题，然后我们从逻辑分布的模型开始推着推着推出来的这么一个必需的函数，但是在感知机中中，激活函数的作用也不只是表达某一类结果的概率那么简单了

同样，为了引入这个激活函数的作用，借用吃不吃米饭的例子，我们考虑一种情况，假设我们现在有四组数据，他们是$(1,1)\to -1,(2,0)\to 1,(2,2)\to 1,(3,1)\to -1$这么一个样子

![image-20221008170234590](https://s2.loli.net/2022/10/08/QfApXDt7boN5RYB.png)

让我们采用分类逻辑回归的方法试试看，然后我们就发现我们手动都无法将这些数据点通过一条直线分隔开来，这就比较麻烦了，我们人类智能都无法区分这些数据点，那么人工智能更不可能能够对这个问题进行分类求解

这时候应该怎么办呢，我们想到了一个方法，为什么不在输入和输出之间再加一层，这一层中将输入数据点的两个输入值经过某些变换，变成另外的两个点，从而实现数据的划分呢

比如，现在新增的这一层的含义是取数据点到$(2,2)$和$(2,0)$两个点的距离分别作为该层输出的x和y，则上述的四个点经过变换后变成了$(1,1)\to (\sqrt{2},\sqrt{2})\to -1,(2,2)\to(0,2)\to 1,(2,0)\to (2,0)\to 1,(3,1)\to (\sqrt{2},\sqrt{2})\to -1$，画成图就是这样

![image-20221008181832193](https://s2.loli.net/2022/10/08/1vF9fnkLlzwg7dK.png)

如果只考虑变换后的四个点，我们发现可以通过上面这条橙色的线进行逻辑回归中进行过的划分，这么一来就真的很好的区分了原本无法通过一条直线区分的四个点，不过在实际的训练过程中，肯定不能一直笨笨的通过人工指定的方式来确定添加的这一层应该如何映射，毕竟现在看上去这个求距离的方式能够区分我们这一堆数据集，但是实际上谁也不知道下一个数据集可不可以通过距离方式分离数据点，这时候我们就要去思考这一个距离函数为什么能够区分原本无法区分的数据点，他到底有什么性质

直接说结论，其实很简单，只需要保证这个函数不是线性函数就行了，不信你们可以试一试尝试对这四个点经过同样的线性变换看看是否可以分开他们，总的来说，经过了相同的非线性变换的数据集可以从线性不可分的状态转化为线性可分，线性可分就是可以像上面一样用一条直线根据不同输出分离成两堆，还是比较好理解的

然后我们回过头来看看这个感知机，为了不让你翻来翻去我重新贴一遍图

![image-20221006192135647](https://s2.loli.net/2022/10/06/7wztWpKDb6jxRIH.png)

我们都知道其中的$f(x,y)$一定是一个线性函数，那么加上一个激活函数，如果这个激活函数不是线性函数，好像输出结果一定是对$(x,y)$两个输出进行的一次非线性变换诶，那么我们是不是会产生一个大胆的想法，如果像这样

![image-20221008185228767](https://s2.loli.net/2022/10/08/MrhRYx72HOPcDmJ.png)

是不是就能让本来不能进行逻辑回归的数据集可以进行逻辑回归了呢？而这么一个结构，像不像一层传说中的神经网络呢？至此，我们了解到了激活函数在深度学习，尤其是神经网络的每个感知机中的一个重要作用——改变数据的线性关系

除此之外，激活函数当然也不能只有非线性那么简单，由于无论是把他看成神经网络还是加了一层的逻辑回归，在构造出了这么一个预测函数模型之后为了确定其中的$w$和$b$，我们还是得老老实实的构造损失函数然后用梯度下降慢慢求解，还记得之前说过如果需要进行梯度下降需要满足的预测函数条件吗？没错，他必须是单调并连续可微的，如果不单调，就有可能训练到局部最优而不是全局最优，如果不连续可微则更恐怖了，导都求不了还梯度下降个屁

于是，**我们总结出了激活函数的三大性质：可微，单调，非线性**

也正是由于他和逻辑回归中的激活函数的含义不一样，因此并不和逻辑回归问题一样必须用sigmoid函数，甚至之后会提到，虽然sigmoid函数确实满足上面三大性质，但是在实际情况中如果用sigmoid做感知机的激活函数不仅效率低而且表现还不好，可以说除了满足条件之外一无是处，常规来说，我们都会使用更为简单的ReLU函数，他的函数表达式十分粗暴不讲道理，长这样$f(x)=max(0,x)$，图像画出来长这样子

![See the source image](https://miro.medium.com/max/1280/0*vazKypn8WYZL1D99.png)

是的，他用分段函数的方式阴险的满足了非线性的条件，同时由于他求导也实在简单，又满足了所有的激活函数条件，因此可以说在一般的深度学习的环境下十分无敌，在犹豫的时候无脑用就完了

### 非线性拟合

在很长一段时间当中，我都在纠结凭什么ReLU激活函数函数具有和sigmoid激活函数一样的作用，而又为什么不同线性函数的组合无法实现非线性的拟合，进一步的，线性函数经过了激活函数之后再进入线性函数到底结果是怎么样的，有没有一种直观的方法可以一眼就理解多层网络中的各层叠加之后的结果呢？

![image-20221008170234590](https://s2.loli.net/2022/10/08/QfApXDt7boN5RYB.png)

回到这张图，这回我们用另一种思维来思考这个分类问题，此刻，我们不再要求仅通过直线来区分两种颜色的点，而是可以使用曲线，这样一来，似乎问题就不再像之前那样无法求解了，我们可以这么画一条曲线

![image-20230113195602402](https://s2.loli.net/2023/01/13/VaeWDdL3lInH5xB.png)

我们不管这个函数究竟是什么，但是我们可以明显的看出在这个函数的划分下，蓝红两种颜色的点分别落在了函数两侧，这同样完成了对于两种类别的分类问题，而这就是对于激活函数的另一种解释方式，**你既可以把激活函数看成是一种对原本分类点进行坐标变换的方式，也可以把激活函数看成是一种对原本线性函数进行非线性扭曲的方式**，有了这么一个理解，我们回过头来看看一个最简单的两层网络，我们试着跟着网络的计算方式将结果反映到实际的图中，而激活函数就使用我们刚刚介绍的ReLU

![image-20230113203656660](https://s2.loli.net/2023/01/13/HomdPep5U4NILzq.png)

根据计算，我们可以得到$f_3$的函数表达式
$$
f_1 = max(w_1x_1+b_1,0)\\
f_2 = max(w_2x_1+b_2,0)\\
f_3 = max(w_3f_1+w_4f_2+b_3,0)
$$
此时我们可以乱定一些$w$和$b$，比如我们直接让$w_1=1,w_2=-1,w_3=1,w_4=1,b_1=-0.5,b_2=0.5,b_3=-1$这样，然后我们将$f_1,f_2,f_3$三个函数关于$x_1$的图像都画出来，看看他们长什么样

![image-20230113213115596](https://s2.loli.net/2023/01/13/ACRTsdeSH6Zr314.png)

我们会发现，经过了一层网络之后，$f_3(x_1)$这个函数的图像明显呈现出比标准的ReLU多了一折，如果改变$w$和$b$的取值，会发现$f_3$的变化更加多种多样了，而如果我们增加隐藏层的数量或者隐藏层的层数，则会发现在正常情况下最后的输出函数的弯折更多了，输出函数可以变化的程度也更高了，这不是正好满足了我们之前提出的无法使用一条直线区分样本点的问题吗？而想象一下如果我们增加$x$的数量，则最后弯弯曲曲的输出函数图像就会展现在一个超平面上，帮助我们区分所给定的训练样本，从而实现所需的分类工作

你可能会问，ReLU的特性会使输出函数恒大于0呀，假如给定的训练样本需要在负空间内区分又该怎么办呢，实际上网络输出层的激活函数可不是ReLU，正是最后一层的网络保证了整个网络在所有实数空间上的灵活变化

### 反向传播

在了解了激活函数和感知机，并且在稍微了解了一下所谓的神经网络到底在干什么之后，我们再来想一个问题，面对着多层的神经网络，假设我们也已经构造出了损失函数，接下来按理说应该是开始求偏导然后梯度下降的节奏了，但是我们一看损失函数有点傻了，倒不是损失函数的导数不好求，而是损失函数中的预测函数，由于其中包含了多层包括激活函数和线性函数和一堆参数（毕竟每一个感知机都会包含若干个w和一个b，如果有多个感知机显然需要偏导的参数量十分庞大），对于每一层的每一个参数求一次偏导求求完就算是计算机来求都够呛，有没有一种方法能够方便的求解这么多参数的偏导呢？

从简单的模型入手，假设我们现在构造出了一个由两层感知机组成的神经网络，他长这样

![image-20221008204221106](https://s2.loli.net/2022/10/08/WHe69vXT3NimnYU.png)

> 第三层的$f(u_1)$和$g(v_1)$写错了，应该把1改成2

通过这个图的箭头一步步计算，我们很容易就可以得到OUT的结果，现在假设这个OUT就是我们需要进行梯度下降的对象，我们该如何求解$w_1\textasciitilde w_{10},b_1\textasciitilde b_4$的偏导数呢？

我们不妨一层一层来看，如果你感觉一下子求解$w_1$实在有点困难，那我们就先从$w_9$和$w_{10}$或者是$b_3$和$b_4$开始

根据求导的链式法则（$\frac{\partial z}{\partial x}=\frac{\partial z}{\partial u} \frac{\partial u}{\partial x}+\frac{\partial z}{\partial v} \frac{\partial v}{\partial x}$），我们可以这么求解$w_9$和$b_4$

![image-20221008211451874](https://s2.loli.net/2022/10/08/RCNM58IrkJnKhTE.png)

如图的红线部分，由于在计算OUT的时候我们已经计算过所有的$g$,$f$函数值及$u_1,v_1$等了，因此最后一层的所有$w$和$b$的偏导都十分容易求得，比如图中的$\frac{\partial OUT}{\partial w_9}=f(u_2),\frac{\partial OUT}{\partial b_4}=1*g'(v_2)*w_{10}$，同时我们在最后一层中可以得到$\frac{\partial OUT}{\partial u_2}$和$\frac{\partial OUT}{\partial v_2}$，拿着这两个结果，我们看看倒数第二层的$w$和$b$应该怎么求

![image-20221008213610687](https://s2.loli.net/2022/10/08/lwyhHc1kKEZQ6Dx.png)

可以发现当我们后一层的偏导数都求完之后，有了链式法则的帮助。对于前一层各个参数的偏导数也就十分容易求解了，唯一需要注意的只有在层与层的连接处，也就是激活函数前的求导时，应该保证每一条从激活函数指出去的运算都取得一次偏导，之后将所有的偏导结果加起来得到激活函数的偏导，如上图$\frac{\partial OUT}{\partial f(u1)}=w_9*f'(u_2)*w_5+w_{10}*g'(v_2)*w_7$这个样子

这种在完成了一次前向传递之后在求每一个参数的梯度之前从后往前传递的过程就叫做反向传播，这种求偏导的方法由于从后往前层层求导，没有任何重复的迭代操作因此效率极高

## 从线性回归开始（Linear Regression）

经过了让人头痛的理论学习之后，我们总算是对于整个神经网络的相关部分有了一定程度的了解，我们需要知道的是上述的内容真的只是基础中的基础，甚至说到底大部分也都还停留在机器学习的阶段，何况我还在基础的基础上省略了为了节省篇幅省略了大量的理论推导，因此理解他们是必须的，而难度大一些的知识和一些需要注意的知识点则会在之后的实际操作过程中涉及到时再做补充，如果做好了准备，就让我们开始吧

### 函数支持

事实上，上面讲的那么多理论内容并不会全部让你在编程的过程中全部实现一遍，pytorch已经封装了大多数可以直接方便使用的函数供我们调用，其中对如今的我们最重要的就是内置的反向传播函数`backward()`

对于任何深度学习的模型，一定逃不开的肯定是求偏导数的过程，由于梯度下降对于整个模型参数训练的重要性，使得求偏导，也就是获取每一个参数在某一数值的梯度这个操作非常频繁的被调用，而我们在理解了反向传播的过程之后，我们了解到获取参数梯度的过程可以通过对于复杂代数式的解构层层完成，于是在pytorch中提供给我们了一个强大的`backward()`函数，他帮助我们完成反向传播的过程，并且获取其中所有参数的梯度保存在该参数的`.grad`属性中（在pytorch中可以理解所有的参数都是张量）

但是这个方法也并不是随便使用的，想要使用他我们就得为我们将要求梯度的参数在定义时指定配置`requires_grad=True`这样pytorch会自动跟踪该张量在之后的所有计算操作，以便于反向传播的时候能够正确的完成偏导求解

```python
a = torch.tensor([1,2,3],dtype=torch.float32,requires_grad=True)  # 指定a需要获取他的梯度
print(a.grad)  # 初始值为None
e = torch.tensor([1,2,3],dtype=torch.float32)  # 如果这个参数不需要获取他的梯度就不需要指定requires_grad
b = 3*a+1+e
c = torch.sigmoid(b)  # 进行各种操作，每一个操作都会被pytorch记录，在反向传播的调用中逐层求偏导
d = c.sum()  # 需要注意的是之后backward()调用的tensor必须是一个标量，对应到我们的损失函数输出的结果也一定是一个值而不是一个张量，sum函数会返回一个张量所有元素的累加和，模拟比如均方误差的结果，返回的正是一个标量
print(d)
d.backward()  # 反向传播，更新被跟踪的所有参数的grad值
a.grad  # 获取a的梯度

---------------------
None
tensor(2.9932, grad_fn=<SumBackward0>)
tensor([1.9944e-02, 3.7010e-04, 6.7949e-06])
```

可以发现由于pytorch隐藏了函数的实现过程，因此我们可以把精力集中到流程的框架架构上，对于反向传播和求偏导的细节过程一概可以放到一边

### 手动实现

如果看完了那么多的理论，发现自己对于pytorch什么张量已经忘得差不多了，那么不妨让我们先手动通过pytorch最基础的张量和方法来实现一下上一节刚刚学习过的线性回归吧

我们首先回顾一下线性回归实现的基本流程

1. 准备一堆用以训练的数据集，其中包含若干组特征值$x$和若干组预期输出$\hat{y}$，为了方便我们可以直接借用上面的例子$（1,3）$、$（2,5）$、$（3,8）$这三个数据点进行拟合，注意在真实的环境中可能我们在这一步之前需要先进行特征提取和特征编码获取特征值
2. 构造出预测函数，由于这里只是最简单的线性回归问题，因此不涉及神经网络的构架和激活函数的选择，而数据集又是一维的，因此直接选择$f=wx+b$即可
3. 确定了预测函数，那么下一步就是确定损失函数，他是为了评判每一次的$w$和$b$的好坏，同时我们也需要通过他对$w$和$b$求偏导，确定下一次$w$和$b$的变化方向和幅度，在pytorch中，我们理应可以直接使用`backward()`函数简化实现操作
4. 所有的函数模型准备完毕，下一步就是初始化$w$和$b$，由于之前提到单调模型的损失函数不存在局部最优，因此我们可以随意初始化$w$和$b$，在训练次数足够的情况下结果必然是相同的
5. 顺便我们也可以先随意指定一个学习率，这玩意现在我们完全可以直接人为取一个比如0.01之类的不要紧，在这个简单模型中影响不大
6. 由于训练的过程必然是重复的，因此我们应该预想到每一个重复过程一定执行的操作，以便于之后进行快速编程，在每一次参数的迭代过程中，我们都需要对新产生的$w$和$b$重新求解一遍预测函数和损失函数，并且重新更新新的$w$和$b$值进入下一次循环

分析完毕，代码编写起来也十分简单，如下

```python
import torch

# 模拟数据集
x = torch.tensor([[1.0, 2.0, 3.0]])
y_hat = torch.tensor([[3.0, 5.0, 8.0]])  # (1,3)/(2,5)/(3,8)
# 学习率
learning_rate = 0.1

# 首次随机指定w和b
w = torch.rand(1, 1, requires_grad=True)
b = torch.rand(1, 1, requires_grad=True)  # 保证之后可以求导
# 获取预测函数和损失函数

# 循环迭代求w和b
for i in range(0, 1000):  # 先训练个一千次
    f = w@x + b  # @表示矩阵乘法
    loss = (y_hat-f).pow(2).mean()  # 取均方误差，mean表示获取平均值
    # 获取w梯度
    loss.backward()
    # 迭代下一次w和b
    w.data = w.data-learning_rate*w.grad  # 因为在添加了requires_grad之后w和b不再是一个纯洁的tensor了，因此我们如果需要获取他们作为张量数值的那一部分就需要使用data属性，在之后当需要更新跟踪参数值的时候我们都尽量使用data而不是直接调用w，但是注意做运算的时候不需要加data
    b.data = b.data-learning_rate*b.grad
    # 清空本次梯度
    if w.grad is not None:
        w.grad.zero_()
    if b.grad is not None:
        b.grad.zero_()

print('拟合直线=>{}x+{},损失值:{}'.format(w.item(), b.item(), loss.item()))  # 当tensor中仅有一个数值时可以通过item()直接输出这个值

------------------------
拟合直线=>2.4999992847442627x+0.33333468437194824,损失值:0.055555447936058044
```

可以发现拟合结果的精度还是比较完美的，不过其中需要注意的是我们将特征值和预期输出进行了拆分，同时由于二维以上的张量才可以进行叉乘操作，因此此处虽然只有一组特征值和一组预期输出，但是我们仍然用两个二维张量存储，对于之后增加的特征值和输出值，只需要继续以行为单位添加至$x$和$\hat{y}$中即可，只这也是之后对于数据集处理的基本步骤，通过这个简单的例子，我们可以以小窥大，其实之后任何复杂的学习模型，在根本上都逃不开这几个步骤

### API实现

我们都知道，一个框架一般都有专属于这个框架的设计流程，除了基本的函数封装之外，一个功能强大的框架往往能够从普通的编程过程中剥离出来自成一体，这也得益于其拥有封装功能完善的对象和模块，在pytorch中同样也有这么一个专门用以构建神经网络的模块`torch.nn`和用以系统管理操作数据的`torch.optim`等等模块，我们今后几乎不需要自己写任何表达式和细节逻辑关系，所有的交给封装完善的接口来办就行

同样是实现线性回归，虽然这么简单的一个问题或许使用API完成就像大炮打蚊子，但是我们同样可以通过这么一个案例来初步了解这些模块中的各种功能，在详细说明各个模块之间的功能之前，我们不妨先来看看用API实现和之前手动实现在代码上有什么样的区别

```python
import torch
import torch.nn as nn
import torch.optim as optim


# 创建继承nn.Module的自定义类，为了生成我们的预测函数
class LR(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        # nn中提供了线性模型，第一个参数是w数量，第二个参数是输出数量
        # 这一行相当于之前w=torch.rand(1,1,requires_grad=True)和b
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        # 当我们实例化该类之后，每次调用实例是都会调用这个方法，在这里我们可以编写网络的所有结构，返回值就是我们的预测函数
        # 在这里只指定了一层，即只对传入参数进行一次线性计算，相当于f = w@x + b
        out = self.linear(x)
        # 比如我们还可以进行一次sigmoid操作，每次对out的操作等于都给网络多加了一层结构
        # out = torch.sigmoid(out)
        return out


# 准备数据集
x = torch.tensor([[1.0, 2.0, 3.0]])
y_hat = torch.tensor([[3.0, 5.0, 8.0]])

# 实例化模型
lr = LR()
# 实例化一个梯度下降的优化器类用以帮助我们管理需要跟踪的数据(在此就是w和b，通过lr.parameters()传入给优化器实例)，第二个参数是学习率
# 至于SGD属于随机梯度下降模型，还有一些复杂一些的之后会提到
optimizer = optim.SGD(lr.parameters(), 0.01)
# 同样实例化一个损失函数，MSELoss就是均方误差
loss_f = nn.MSELoss()
# loss_f实例化对象也和我们自定义的LR类一样有一个初始化方法和forward调用方法，之后我们只需要直接调用实例化对象其实就是调用他的forward方法

# 循环逻辑和手动实现并没有什么区别
for i in range(10000):
    # 获取预测值，调用了LR类的forward方法
    y_predict = lr(x.t())  # 特征值的不同取值应该竖排，种类横排，需要转置一下
    # 获取损失值，调用了loss_f的forward方法，相当于loss = (y_hat-f).pow(2).mean()
    loss = loss_f(y_predict, y_hat.t())
    # 通过优化器类统一梯度置零，相当于那一大串if语句
    optimizer.zero_grad()
    # 反向传播
    loss.backward()
    # 相当于w.data = w.data-learning_rate*w.grad，b同理
    optimizer.step()

print(optimizer.param_groups[0]['params'])

------------------------
[Parameter containing:
tensor([[2.5000]], requires_grad=True), Parameter containing:
tensor([0.3334], requires_grad=True)]  # 第一项是w,第二项是b
```

虽然看上去篇幅比较繁琐，而且相对来说也没有之前手动实现起来那么直观自然，但是优点就在于整篇代码不存在任何一个公式计算过程，预测函数的生成交给自定义类`LR()`，其中的函数内容和参数内容由`nn.Linear()`帮我们生成计算，损失函数由`nn.MSELoss()`计算，梯度归零和参数更新则由优化器`optim.SGD()`帮助我们完成，在预先完成实例化之后，我们只需要根据逻辑傻瓜式的调用这些实例对象或者其中的方法即可

至此我们的简单线性回归模拟任务算是终于完成了，不过在进入下一章节之前我们还需要紧急插播几条知识点

### 层次区别

我们应该都知道神经网络分为输入层，隐藏层和输出层，但是这几个层次之间明明从图上看好像都是一个个神经元，他们到底有什么区别呢

![](https://pic1.zhimg.com/v2-6bf12ea3bb8a6e399000168e22455c67_r.jpg)

首先我们应该先对$w$参数作一个扩展，在此之前我们都是使用的看上去是二维数据集，但我们知道他只有一个特征值，无论这个特征值有多少组数据，相对应的$w$永远只有一个，当特征值有多个时（比如图像识别每一个像素都是一个特征值），那么$w$也自然会有多个，如果用$x,y,z$表示三个不同的特征值，用$w_{xk},w_{yk},w_{zk}$分别表示第k个感知机的$x,y,z$三个不同特征值的$w$，而用比如$x_1,y_1,z_1$表示第一组训练数据，如果我们有五组训练数据，那么两个感知机的输出表达式可能长这样
$$
f_1 = \begin{bmatrix}
 x_1 & y_1 & z_1\\
 x_2 & y_2 & z_2\\
 x_3 & y_3 & z_3\\
 x_4 & y_4 & z_4\\
 x_5 & y_5 & z_5
\end{bmatrix}*
\begin{bmatrix}
w_{x1} \\
w_{y1} \\
w_{z1}
\end{bmatrix}+b_1=
\begin{bmatrix}
x_1*w_{x1}+y_1*w_{y1}+z_1*w_{z1}+b_1 \\
x_2*w_{x1}+y_2*w_{y1}+z_2*w_{z1}+b_1 \\
x_3*w_{x1}+y_3*w_{y1}+z_3*w_{z1}+b_1 \\
x_4*w_{x1}+y_4*w_{y1}+z_4*w_{z1}+b_1 \\
x_5*w_{x1}+y_5*w_{y1}+z_5*w_{z1}+b_1
\end{bmatrix}\\
f_2 = \begin{bmatrix}
 x_1 & y_1 & z_1\\
 x_2 & y_2 & z_2\\
 x_3 & y_3 & z_3\\
 x_4 & y_4 & z_4\\
 x_5 & y_5 & z_5
\end{bmatrix}*
\begin{bmatrix}
w_{x2} \\
w_{y2} \\
w_{z2}
\end{bmatrix}+b_2=
\begin{bmatrix}
x_1*w_{x2}+y_1*w_{y2}+z_1*w_{z2}+b_2 \\
x_2*w_{x2}+y_2*w_{y2}+z_2*w_{z2}+b_2 \\
x_3*w_{x2}+y_3*w_{y2}+z_3*w_{z2}+b_2 \\
x_4*w_{x2}+y_4*w_{y2}+z_4*w_{z2}+b_2 \\
x_5*w_{x2}+y_5*w_{y2}+z_5*w_{z2}+b_2
\end{bmatrix}
$$
画出来的图长这样

![image-20221013171018557](https://s2.loli.net/2022/10/13/MRO2NTstSzUi1LB.png)

可以发现，在图中其实是体现不出来我们的数据集中数据的多少的，而如果我们看到公式部分，可以发现单个感知机的输出$f$其实是一个$5*1$的矩阵，拿着这个矩阵，我们来分别分析一下$f1,f2$是隐藏层和是输出层的区别：

- 如果$f1,f2$是隐藏层，那么我们需要先将这个$f$通过一个激活函数，然后就可以将这$5*1$的一个矩阵看作是新的一组数据集的一个输入值，然后我们会将多个$f$并在一起变成$5*x$的矩阵，如果我们单看这个矩阵，是不是和上一层的输入内容无比类似，只是他的取值范围由于经过了激活函数在一定程度上就被限制住了，因此其实每一个隐藏层在功能方面可以近似看作是一个新的输入层

- 如果$f1,f2$是输出层，那么我们同样需要合并多个$f$，然后同样经过一个激活函数，只不过这个激活函数大概率和隐藏层中的激活函数不一样，然后我们把这个结果和$\hat{y}$做损失函数计算，注意到应该保证$\hat{y}$的组数和训练数据的组数应该相同，接着我们拿到了最终的损失值，注意到无论输出层有多少神经元，通过损失函数计算出来的损失值永远是一个唯一的值，接着我们结束本次的正向传播，准备开始反向传播

  ![image-20221019205455173](https://s2.loli.net/2022/10/19/pUHvFDNu3QsjK1J.png)

于是，综合上面的内容，我们知道了，层次之间的最大区别就是在本层通过函数的类型，对于输入层来说他不需要通过任何函数直接进行$w$和$b$的计算进入下一层，而对于隐藏层则都需要通过一个指定好的激活函数再进入下一层，而输出层则是需要对输出矩阵的每一列，都通过一个不同损失函数再进行输出不同值，而由于在平常神经网络的图像中大多省略了中间的函数项，因此可能会造成一定程度的困扰

### 梯度下降优化

上面我们介绍了梯度下降和在多维数据集下的神经网络结构，但是如果你思考一下就会发现在效率方面会存在一定的问题，尽管我们都说计算机的速度多快多快，但是想想他每次求一次梯度都要先把几十几百次的几百维的矩阵乘来乘去，想想也是够呛，如果我们还不加节制的求一大堆次梯度，结果能跑出来才怪，虽然说可以通过增加学习率的方法减少梯度的运算次数，但是之前也提过，同样会遇到震荡的情况导致求不到最小值，因此梯度求解方式的改变势在必行

#### 批量选择

一个非常明显的减少计算量的方法就是减少数据量，于是就诞生了随机梯度下降法（SGD），也就是我们上面API实现过程中调用的优化器，他每次会偷工减料，少取一些数据组，因为特征量已经定死了当然不能减去列，因此只能在输入数据矩阵相乘的过程中随机减掉一些行，拿上一节的例子来说，使得原本比如$5*3$的矩阵突然就变成$2*3$了，这显然会减少单次的计算量，而在下一次梯度的计算过程中再取不一样的一批数据，虽然数据量变少了显然最后产生的结果也会造成偏差甚至会造成计算梯度的次数变多，但是已经被证明对于常规的神经网络所涉及的凸函数来说，只要数据集足够大，随机梯度下降法比起普通的梯度下降法（BGD）是具有显著的效率优势的

如果你还不信，你可以去找找`torch.optim`里面找找还有没有BGD的方法，对于pytorch来说，他已经抛弃了普通梯度下降的实现了，这也从侧面证明随机梯度下降的方法是几乎全面可代替普通梯度下降的

但是单纯的减少数据量从长远角度而言总归是治标不治本的，想要进一步减少数据量，我们就得从梯度下降的算法本身入手

#### 指数加权平均值

在进行下一步的优化前同样需要一丢丢的前置知识，我们先来了解一下指数加权平均是什么玩意，其实他就是一个递推关系式，他长这样
$$
v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}
$$
我们可以先暂时理解为我们拥有一个$\theta$的一维数据集，其中这个$\theta_{t}$自然就是他的第$t$位数据，而$\beta$则是人为定义的，$v_t,v_{t-1}$自然就是这个公式需要本次求的东西和上次求的东西，那么这个需要求的东西到底是什么玩意呢，我们不妨自己假设一组数据试试，我们就取$\theta=[7,4,2,-2,1],\beta=0.2$好了

- 第一轮显然因为没有$v_{t-1}$因此不参与计算，$v_1=7$
- $v_2=0.2*7+0.8*4=4.6$我们发现$v_2$的值似乎比起$\theta_2$要大上一些，我们再往后求求看
- $v_3=0.2*(0.2*7+0.8*4)+0.8*2=2.52$我故意把$v_2$的算式重新代了一遍其实就是为了方便观察，假如我们把这个算式看成是一个求平均值的算式，我们会发现其中给$1,4,7$前面加了不同的权重，同时$1$的权重最大是$0.8$，而$4,7$的权重根据在$\theta$中的位置与$1$的远近被赋予了不一样的权重，距离越远所持有的权重越小，减少的比重是以指数下降的，这也是为什么他叫指数加权平均的原因了，与此同时，由于平均数的特性，我们会发现只要在$\theta$中的数是同符号递减的，那么最后得出的$v_2,v_3$都是大于本身的$\theta_2,\theta_3$的，大的幅度同样由他前面的递减幅度所控制
- 然后我们看看遇到不一样的符号时会发生什么，$v_4=0.2*2.52+0.8*(-2)=-1.096$，理所当然的由于至少也存在一定的平均数权重，因此当第一次符号改变时输出结果一定是比原有数更靠近坐标0点的，如果从输出绝对值的角度来看，那么这个算式会在给定数据符号变化时给予一定的修正量，使得输出的绝对值比起原有值更接近零点，如果把符号变化看成震荡的话，那么这个算式会将震荡幅度控制住

#### 动量法（Momentum）

如果了解了指数加权平均的功能，我们就可以发现他的性质似乎十分贴合我们的梯度下降法，我们不如尝试将其加到我们的梯度下降的过程中，我们一开始的梯度下降公式是这样的
$$
w_{k+1}=w_k-\eta\frac{\partial L}{\partial w}|_{w=w_{k}}
$$
我们对其稍加改进，他就变成了这样
$$
w_{k+1}=w_k-\eta(\beta w_k+(1-\beta)\frac{\partial L}{\partial w})=\gamma w_k-\alpha \frac{\partial L}{\partial w}
$$
一般来说，他的$\beta $一般会取0.9或者0.8，如果对其进行分析，就可以发现他在选择下一次的下降梯度的过程中只取了当前梯度值的0.1或0.2，大部分的梯度值还是受前十几次的梯度累加所控制（$0.9^{40}\approx 0.01$因此至少前20-30次梯度都会对当前的梯度存在影响），也正因为如此因此在动量法的过程中学习率$\eta$一般都取值比较小，虽然在迭代的前十几次可能下降速率还不如普通梯度下降，但是一旦让指数平均的部分累加起来，下降的幅度还是非常猛烈的，而反观震荡处由于梯度符号反复变化导致梯度无法累积，从而会使得震荡幅度显著减小

这其实就是梯度下降法中十分出名的动量法（Momentum），如果你留心在前几节我们用API实现代码中的优化器类，我们在实例化的语句中调用的`optim.SGD()`这个API，除开第二个学习率之外还可以传入第三个参数`momentum`正是动量法的$\beta$参数，虽然看最终的公式来说甚至就只是在$w_k$前多加了一个$\gamma$参数而已，但是已经被证明他在梯度下降的性能上，尤其是在批量选择为基础的随机梯度下降模型中，性能是普遍好于未使用动量的梯度下降方法的

#### RMSProp

前面我们显然是拿梯度值开刀，通过以往的梯度值来决定这一次的梯度值，但是有一部分人还是感觉效率不够高，毕竟学习率没有变呀，虽然你说你这动量法减少了震荡的幅度，但是我们为什么不换一种角度，从学习率入手，直接通过动态减少学习率来避免震荡呢

于是有了以下公式
$$
w_{k+1}=w_k-\frac{\eta }{\sqrt{\delta+s_t } }\frac{\partial L}{\partial w},s_t=\beta s_{t-1}+(1-\beta)(\frac{\partial L}{\partial w})^2,s_0=0
$$
他就是RMSProp，看起来好像很复杂，但是我们也看到了我们熟悉的身影，也就是可爱的指数加权平均，虽然他的名字叫平均，但是在这条公式里我们可以将其直接以累和的含义对待他，因为和动量法的$\beta$取个0.8，0.9不同，在这里一般$\beta$会取到0.999这个量级的，也就是基本上就是对以往所有梯度的平方加了一个小的权重然后直接就累加了这样子，相当于每次的值都是前几百次的梯度累加结果（虽然每一轮加的不多），意味着一般上来说$\frac{\eta }{\sqrt{\delta+s_t } }$这个玩意是会恒减的，与最开始的梯度下降公式比对，就可以发现学习率这一项在这个公式的控制下随着计算梯度次数的增加递减，这也同样从另一方面解决了在一般凸函数下梯度下降在极小值附近震荡的问题

当然你可能也会有问题，比如为什么要搞那么麻烦，如果要累加，我直接让$s_t$等于以往所有梯度累加不就好了，你说的没错，这就是比RMSProp更加暴力一些的AdaGrad方法，由于他比RMSProp计算量更小因此也用的不少，但是如果面对几千几万次的计算梯度之后，AdaGrad由于不会动态减少很久很久以前的梯度值的权重，因此在训练次数过多的情况下可能会出现学习率无限接近于0的情况，那么梯度下降就下降不了了，显然也是一种隐患

当然可能还有一些问题，比如$\delta$是什么玩意，$(\frac{\partial L}{\partial w})^2$为什么要加平方等等，其实这些都不是这个算法最根本能够出众的根本原因，在实际的过程中也并不是特别重要（比如你完全也可以不加平方改成绝对值，算法照样能跑），况且现在已经有了封装完善的算法使用接口，不需要我们自己背这些公式，我们如果需要使用直接调用接口即可，而对我们来说更重要的是了解什么时候应该使用什么样的方法

> 不过$\delta$还是可以说一下的，由于$s_0=0$因此为了避免分母为零加的这个玩意，并不是什么重要的东西，至于平方问题，除开为了避免梯度为负抵消前几次的梯度累加之外，其他的原因就得问数学家了

下面则是这两种方法包括其他方法在可视化图中的速度对比

![img](https://pic1.zhimg.com/80/5d5166a3d3712e7c03af74b1ccacbeac_720w.webp)

![img](https://pic1.zhimg.com/80/4a3b4a39ab8e5c556359147b882b4788_720w.webp)

#### Adam

如果把上面的两个方法合在一起，就变成了大名鼎鼎的Adam算法，许多人甚至可能都不了解Adam的原理是什么玩意，反正就是一遇到梯度下降的问题无脑用就完了，虽然这玩意确实好使，但是由于其实在某种程度还是违背了某些人的算法洁癖思想因此也有很多人会专门写一篇论文给这个算法挑刺，不过这些都不是我们这种菜鸟应该关注的问题，至少在我们这个阶段，完全可以把他当作是一个不知道怎么反正就是全面比普通梯度下降算法表现优秀的算法无脑使用就行了，既然他是上面两个方法的结合，因此他的表达式也十分好写，就长这样
$$
w_{k+1}=w_k-\frac{\eta }{\sqrt{\delta+s_t } }(\beta w_k+(1-\beta)\frac{\partial L}{\partial w}),s_t=\alpha s_{t-1}+(1-\alpha)(\frac{\partial L}{\partial w})^2,s_0=0
$$
如果我们需要使用，我们可以直接调用`torch.optim.Adam`对象构造，顺便我们可以发现在pytorch中的默认构造参数，假如我们什么都不填，他就是这样的
$$
lr=0.001,\beta = 0.9,\alpha = 0.999,\delta=10^{-8}
$$
我们可能会惊讶的发现这两个方法一融合之后还挺和谐，在一开始由于梯度还没开始累积，因此属于动量法的那一部分显然每次生成的梯度都比较小，但是属于RMSProp的那一部分由于$s_t$同样也没有开始累积，因此学习率反而会比较大，使得即使是梯度不够大在一开始也可以稳住下降速率，缓解了我们在动图中看到的动量法比起别的方法在一开始总是慢悠悠的过一段时间才起步的问题

## 传统神经网络（DNN）

了解了一大串乱七八糟的知识之后，我们总算是有那么一点能力可以试着自己搭一搭神经网络了，在这一部分我们需要研究如何通过pytorch提供的API加载文件项数据并搭建简单的神经网络实现最简单的手写数字识别的功能，手写数字识别可以说是深度学习`Hello World!`级别的经典例题，因为他在一般意义上只需要最简单的神经网络架构就可以显示出不错的成果，我们不妨一起来看一看

### 数据集类

之前我们都是通过自己定义随便定义的一组张量来模拟输入特征数据和预期输出内容，然而在实际的场景中，我们往往需要读取在硬盘上的一组文件，从中获取数据集的输入和输出，如何将文件的输出内容抽象成一个个张量也就成了一个非常重要的问题，pytorch提供了Dataset抽象类方便我们存储数据集信息，我们不妨来试一试

这里推荐一个获取测试数据集的[网站](https://archive.ics.uci.edu/ml/index.php)，只需要输入关键字就可以获取别人整理好的带特征数据和结果的数据文件，这里我们使用一个骚扰短信的[数据集](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/)作为熟悉Dataset数据集类的例子

下过来解压之后是一个名为`SMSSpamCollection`的文件，我们将其放到我们的python项目路径下，他的数据长这样

```text
ham	Ok lar... Joking wif u oni...
spam	Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's
```

其中ham表示这条是有效信息，spam表示是骚扰短信，预期输出和输入之间用一个tab隔开，每一行都是一个数据，可以发现实际上是一个二分类问题，但是我们现在主要的目标还是如何把这样的数据交由pytorch管理

pytorch的Dataset数据集类是一个抽象类，我们在继承他的同时还需要实现两个方法分别是`__getitem__`和`__len__`，我们在其中需要分别编写如何获取数据以及数据集的长度如何获取

![image-20221015150755510](https://s2.loli.net/2022/10/15/AVeqXriYpQfFjxn.png)

> 虽然最新的Dataset父类删去了`__len__`方法，但是并不意味着我们不用实现了，他删去只是为了避免出现一些异常情况，我们还是得老老实实实现`__len__`方法

详细程序如下

```python
from torch.utils.data import Dataset

data_path = './datasets/sms/SMSSpamCollection'


class SMSDataset(Dataset):
    # 由于Dataset类并没有实现__init__方法，因此这里我们也不需要调用父类方法
    def __init__(self):
        # 读取文件数据
        self.sms = open(data_path).readlines()

    def __getitem__(self, index):
        # 分开标签和内容（y_hat和x）
        label = self.sms[index][:4].strip()
        content = self.sms[index][4:].strip()
        return label, content

    def __len__(self):
        return len(self.sms)


dataset = SMSDataset()
print(dataset[0])  # 由于实现了getitem所以可以直接[]取值
print(len(dataset))  # 实现了len于是可以使用len函数获取长度

---------------------
('ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')
5574
```

### 数据加载器类

数据集类只是用来帮助我们存储需要进行学习的数据而已，如果我们需要将保存的数据进行操作和分组，在之前介绍SGD方法的时候肯定会有人有疑问，我也没有看见你对数据集进行分割呀，而且在进行前向传播的过程中明明就是所有的数据一起参与运算的，最后反向求梯度的过程怎么就变成批量求了？

所以这中间肯定有一个用于分批次管理数据的方式，将所有的数据拆成一个个批次（batch），每次运算都只取一个一个批次参与运算

他就是数据加载器类，同样在`torch.utils.data`内的`DataLoader`，我们可以尝试使用他

```python
dataset = SMSDataset()
dataLoader = DataLoader(dataset=dataset, batch_size=2,
                        shuffle=True)  # 每一个batch大小为2，随机自动打乱
for index, (label, content) in enumerate(dataLoader):
    print(index, label, content)
    
-------------------------
...
2786 ('ham', 'ham') ('Apps class varaya elaya.', 'Hi good mornin.. Thanku wish u d same..')
...
```

可以发现数据加载器类在我们在数据集类中区分过标签和内容的情况下在每一个批次中自动帮我们合并了标签和内容项，让我们之后导入和操作数据更为方便

### 准备数据

我们除了可以自己在网络下载数据集，自己继承Dataset然后一步步编写自己的数据集类，对于在深度学习中比较典型的数据集类型和一些典型的案例，pytorch甚至贴心的帮我们封装好了一些傻瓜包专门供我们这些涉世未深的菜鸡学习，其中`torchvision`就是一个典型，于此类似的还有`torchtext`，前者专门是方便我们进行图像识别学习的，而后者则涵盖了大部分文字识别的经典案例和封装好的Dataset顶层类

比如我们将要解决的成功人士的第一个深度学习问题手写数字识别这种已经快被玩坏了的问题，早就已经在`torchvision`中被安排的明明白白了

```python
import torchvision

mnist = torchvision.datasets.MNIST(root='./datasets/mnist/')
print(mnist)
```

只需要这两句话，我们就会发现60000个数据就已经被下载到你的电脑上了，但你可能说我怎么打不开下过来的这些文件，我怎么知道里面有60000个数据，其实这只能怪他帮你做完了所有的事情，你甚至不用重新对每一个数据进行打包成python的img对象，如果你打印`mnist[0]`就会发现里面已经帮你包装好了img对象和他对应的预期输出label，当然如果你真的不放心要看看里面的图片究竟长什么样，也可以通过`mnist[0][0].show()`查看一下

### 图像标准化

虽然说图片本身就是一个个像素点的集合，但是如今图片的质量越来越高，我们看的是过瘾了，然而面对一张图片几千几万像素点的数据，要是我们什么都不处理直接扔给神经网络，那么可能就是想让他死，于是虽然神经网络的确很强大，但是在丢给他之前进行人工的预处理也是在图像识别过程中必不可少的一环，的我们的一张张图片包含了大量的冗余数据，如果我们能够人为的剥离一部分，那么就可以大大减轻神经网络学习的负担

在我们从`torchvision`中下过来的数据可以说已经经过了一层较为完整的预处理，比如压缩了其中的像素，每一张图片在其中只有28\*28个像素，同时由于手写数字只需要捕捉轮廓特征，因此所有的数据采用的都是黑白照片，甚至还为我们做了归一化处理，进一步减少了每一张图片的数据量和加强了收敛特性，可以说已经没我们什么事了，但是我们发现前几种预处理的手段我们似乎也听得懂，什么压缩像素，彩色照片变灰度，这些都是十分明显的减少每一个图片包含数据的方式，但是后面的归一化可能就有点懵了，于是我们不妨借助这个机会顺便讲一讲归一化，标准化这些听上去高大上的名字他们到底有什么用

首先我们需要明确我们需要最终运行的目标是完成对于图像的识别，而识别的特征值一定是和图像的单个像素值无关的，于是我们知道了既然是从整体的特征下手，那么单个像素点的值大小似乎并不是很重要，比如我把所有的像素点值全部减1，那么原本识别出来应该是5的数字会变成4吗？显然是不会的，因为他最重要的特征——像素点的分布没有改变，这个5的轮廓就不会变

那么我们是否可以从梯度下降的原理角度入手，在不改变像素点分布的前提下，通过改变整体像素点的值的大小，使得之后在进行梯度下降的过程中能够使下降的过程更加迅速呢

要知道我们之前讲了那么一大通的引出大名鼎鼎的Adam算法实际上也只是为了让梯度下降的过程快速和顺利那么一丢丢，要是我们可以直接从数据集下手改变梯度下降的速率岂不是十分无敌？

看上去似乎不可思议，但这也正是标准化处理厉害之处，在此之前，我们来看一张我们之前看过的动图

![img](https://pic1.zhimg.com/80/4a3b4a39ab8e5c556359147b882b4788_720w.webp)

你可能会有疑问，测试各种梯度下降算法的优劣为什么要拿这么一个水渠一样的3D模型测试呢，其实正是因为这种水渠结构对于普通的梯度下降算法十分的不友好，我们期望的模型应该是像一个碗一样，四周高中间低，这样无论什么梯度下降算法都会快速朝着最低处下降，而像这种水渠结构，由于两边高中间平，正常的梯度下降不仅下降慢而且还震来震去，达不到良好的效果，但是如果我们能将原本这种水渠结构改成碗型结构，是不是就可以显著提高梯度下降的速率了呢

这就是标准化的魅力，标准化有很多个公式，我们先来看看比较有代表性的Z-Score公式，他长这样
$$
z =\frac{x-μ}{σ}
$$
$\mu$是所有数据的平均值，$\sigma$则是所有数据的标准差，$x$指原始数据，$z$为映射后的数据，如果你还记得概率论里面的正态分布，那么你对这个式子应该比较熟悉，在概率论中，他可以将一个非标准正态分布转化为标准正态分布，可以发现经过了他的运算之后，至少正态分布的分布情况并不会改变，使得整体的均值为0，标准差为1，但是即使是面对非正态分布的问题，他也能将一批数据在不改变分布的情况下映射到坐标零点周围

另一个也是十分常用的标准化公式是归一化公式，他长这样
$$
z = \frac{x-min}{max-min}
$$
有些人认为归一化不应该被涵盖在标准化之内，但是我觉得他们的目的都是相同的，因此可以归为一类，与上面类似，他也不会改变数据的分布情况，仅通过一个特殊的映射关系将数据映射到了$[0,1]$上，而只要不同维度的数据都被映射到了同一个值域空间之下，那么各个维度之间的数值差异就被无限减小了，神经网络终于可以着眼于各个维度之间的特征关系了

我们通过一张标准化前后的对比图就可以显著看出他对于梯度下降效率的优化程度（第一张是未标准化之前，第二张是标准化之后）

![归一化前求解](https://img-blog.csdn.net/20170701145223409?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRm9udFRocm9uZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![归一化后求解](https://img-blog.csdn.net/20170701145246369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRm9udFRocm9uZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### Softmax

还记得我们最最开始的话题吗，在[逻辑回归](#逻辑回归)一章中，当时我们为了直观的解决分类问题，我们引入了吃饭问题，通过逻辑回归的输出是否大于0.5来判断最后吃不吃饭，如今我们已经学了那么多内容，我们终于可以尝试扩展这个问题了，如今我们在也不只纠结吃不吃饭这么一个简单的二分类问题了，比如现在我们有三个选择，第一个还是吃米饭，第二个是吃面条，第三个是吃面包，同时和之前一样，我们希望输出的值还是吃某一个东西的概率，由于训练数据的输出值是确定的，因此比如第一天湿度一吃了米饭，那么对应的就是这么一个映射关系$(1,1)\to (1,0,0)$，意味着第一天湿度一吃米饭的概率是百分百吃其余东西的概率是0，同时需要注意的是由于现在可以说是三分类，因此输出也应该有三个，分别对应吃每一种东西的概率，还是比较好理解的，这种在训练集中把每一个分类都用01表示的编码方式也叫`one-hot`

但是我们可能也会发现一个问题，在二分类问题中，我们提到了可以使用sigmoid作为激活函数之后再通过交叉熵损失函数，但是此处如果我们对每一个输出再经过sigmoid，会发现尽管每一个输出确实控制在0-1之间，但是这三个输出加在一起可不是1，如果把他们每一个输出看作一个概率，那么可能在实际的神经网络输出的结果吃米饭和吃面条的概率加在一起就已经大于1了，显然是不符合现实的，同时就算我们不管这个问题，我们就直接看谁的概率大就说这次吃什么，那么面对比如输出为$10,100,1000$的时候，他们在sigmoid的输出可能都极度接近1，但是我们显然知道1000那一项应该被赋予更大的概率，如果此次的训练数据的输出是吃米饭，那么用交叉熵获得的在米饭上的损失函数由于$\sigma(10)$比较接近1，因此他的损失函数也并不大，神经网络很可能就觉得这次的结果不错，但是实际上效果是很差的

你可能会说不就是所有的分类输出概率等于1，且每一个输出的范围在0-1之间嘛，这还不简单，这不就是普通的归一化问题嘛，前一节刚讲过我会，不用什么牛马sigmoid了，我直接取每一个输出占所有输出之和的比例不就行了，比如输出了$1,2,3$，那我就取第一个概率为$x_1=\frac{1}{1+2+3}=\frac{1}{6}$，同理，$x_2=\frac{1}{3},x_3=\frac{1}{2}$，看吧，这还不简单完美符合了需求，如果这还不行，我还有前面刚刚提过的`min-max`方法，那个总可以了吧

甚至有人还会提出更大胆的设想，我直接使用max函数，学习训练数据的样子，对所有的输出最大的直接赋值1，其余全部赋值0，这样不是最暴力的解法吗

他们其实都没问题，虽然有些人可能也会给他们挑出一些毛病，比如有些方法经过交叉熵之后反向传播求导不好求甚至求不了等等，这的确也是对的，但是比起接下来需要介绍的softmax来说，这些普通的归一化的方法最大的缺点还是已经被证明他们在效率方面是不如softmax的

关于到底为什么不如，有的人说可以由广义线性模型推出来，有的人说可以用最大熵推出来，还有什么从最优化角度入手的，什么热力学里的boltzmann分布的，可以说是众说纷纭，可惜的是我们这种菜鸟可能一个都理解不了，当时理解为什么逻辑回归二分类要用sigmoid都够呛，多分类问题等于说是二分类问题的高阶版本，感觉咱贫穷的智商不太能支持我们理解这玩意

但是有一点是确定的，sigmoid一定是softmax的特殊版本，他们本质上是一样的，这也能让我们在理解了sigmoid为什么适合处理二分类问题之后能更放心的使用softmax处理多分类问题

讲了那么多，接下来就是softmax的公式部分，他长这样
$$
s(x_k)=\frac{e^{x_k}}{\sum^{n}_{i=1}e^{x_i}}
$$
其输出的s即为$x_k$输出最终的概率，他的公式部分看上去还是比较好理解的，就是在我们第一个归一化方法的基础上对每一个x先进行了一次$e$为底指数运算，以我们粗浅的理解，大概可以将softmax的优势理解成下述的三个方面：

- 因为$e$函数一定大于0，保证了输出存在为负数的情况下不至于反向归一化
- 同时由于指数函数增长速度极快，因此面对比如输出为$(1,2,3)$的时候，softmax会输出的概率为$(0.0900, 0.2447, 0.6652)$，比起之前的$(\frac{1}{6},\frac{1}{3},\frac{1}{2})$显然会使值大的概率更大，值小的概率更小，这在输出值更大的时候效果会更明显，这一差距在经过损失函数之后被进一步放大，和之前我们提到过均方误差和交叉熵误差的区别一样，可以增强模型的收敛性
- 另一方面，虽然没有写出softmax的导数，但是由于$e^x$的存在，使得他和交叉熵中的$log$对数的相性比较好，求解导数也很方便

总之，面对多分类问题，这个softmax完全替代了原本sigmoid的位置，同时由于他的值需要由所有的输出决定，因此在输出层之前我们还需要计算一下$\sum^{n}_{i=1}e^{x_i}$

接着我们来看看当问题变成多分类问题之后损失函数有什么变化，其实也非常简单，同样是交叉熵，由于输出神经元变多了，损失函数也会变得稍微复杂一丢丢，我们不妨先从一组训练参数的单一损失值入手，推出损失函数有什么变化，对于一组训练参数，他的公式长这样，这也可以作为在`one-hot`编码方式下交叉熵的通用形式
$$
l(s)=-\sum^n_{i=1}\hat{y}_ilog(s(x_i))
$$
其中n为分类数量，例如上面的吃饭问题$n=3$，对于一组训练参数来说，由于`one-hot`的原因，因此仅存在唯一$i$使得$\hat{y}_i=1$，其余$\hat{y}_i$均为0，因此损失值实际上就是单一的$-log(s(x_i)|_{\hat{y_i}=1})$，即只取softmax函数映射到预期值为1的概率再取$log$，如果输入存在多组数据，一般来说都是一个批次（batch）进入的神经网络，那么简单的进行累加即可，最终softmax的损失函数如下
$$
L(s)=\sum^m_{j=1}-log(s(x_i)|_{\hat{y_i}=1})
$$
注意到此处的m表示一批次的数据总数，j表示其中一组数据，i表示该组数据中的每一个预测输出，在实际运用的过程中请不要搞混了i和j

如果感觉有点晕，看看下面的图解，看看会不会变得清晰一些

![image-20221019205717578](https://s2.loli.net/2022/10/19/sFtlRfDyEHO7YrX.png)还是觉得晕？那就把$x$看作日期，$y$看作湿度，$f$吃米饭，$g$吃面条，$k$吃面包，所有下标代表第几组数据，$s(f)$是神经网络输出的吃米饭的概率，$\hat{y}$是预期吃啥玩意，拿一组数据$x_1,y_1$来说，就是我们在第一天($x_1$)湿度一($y_1$)情况预期吃米饭($\hat{y}_1=[1,0,0]$)，但是这些都是已知的，事实情况是我们要把第一天和湿度一丢进神经网络，然后他会返回给我们三个概率，分别是吃米饭的概率($s(f_1)$)，吃面条的概率($s(g_1)$)和吃面包的概率($s(k_1)$)，但是由于预期吃米饭，因此我们只需要比较$s(f_1)$和预期的目标概率1相差多少($-1*log(s(f_1))$)，而这个相差的值就作为这一组数据的损失值，与此同理我们求出其余三组($x_2,y_2...x_4,y_4$)的损失值累加即可

### 手写数字识别

在有了这些知识的积累之后，终于可以开始自己的神经网络搭建了，一些细节方面的问题我都会标注在注释中，下面就是代码的所有内容

```python
from torchvision.datasets import MNIST
from torchvision.transforms import Compose, ToTensor, Normalize
from torch.utils.data import DataLoader
from torch import nn, save, load, no_grad
import torch.nn.functional as F
from torch.optim import Adam
import numpy as np
import os


BATCH_SIZE = 128


# 获取数据加载器
def getDataloader(train=True):
    # 图片预处理，Compose内的内容会被顺序执行一遍(调用这些类的call方法)
    preprocessing = Compose([
        # 已知原始数据为img对象，ToTensor可以将其变成一个三维张量（通道，高，宽）
        ToTensor(),
        # Z-Score标准化，初始化时提供均值mean和标准差std
        Normalize(mean=(0.1307,), std=(0.3081,))
    ])
    # 利用pytorch内置的MNIST类加载输出，在实际情况中可能需要自己编写这个类
    # 第一个参数是训练数据位置，第二个参数如果提供false则会加载测试集否则加载训练集，第三个则是预处理函数
    dataset = MNIST(root='./datasets/mnist',
                    train=train, transform=preprocessing)
    # 加载数据
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    return dataloader


# 神经网络模型
class MnistModel(nn.Module):
    # 此处只采用了最简单的三层神经网络结构，即输入->一层隐藏层->输出
    def __init__(self):
        super(MnistModel, self).__init__()
        # 输入的是一个单通道长宽均为28像素的图片因此也有1*28*28个特征点，我们将其抽象为28个特征作为隐藏层输出(当然你可以根据自己喜欢调整输出神经元个数)
        self.hiddenLayer = nn.Linear(1*28*28, 28)
        # 隐藏层有28个神经元，输出层只能输出包含在0-9十个数字之间的概率，因此隐藏层到输出层=28->10
        self.outputLayer = nn.Linear(28, 10)

    def forward(self, input):
        # 首先我们要将输入的三维张量转化为一维输入神经元
        x = input.view([-1, 1*28*28])
        # 输出到隐藏层
        x = self.hiddenLayer(x)
        # 隐藏层激活函数，这里使用ReLU
        x = F.relu(x)
        # 输出到输出层
        x = self.outputLayer(x)
        # 输出层激活函数softmax，由于softmax需要联合几个数值共同计算概率，因此第二个参数用于指定以行还是列为单位计算概率，根据之前画的图我们知道我们需要根据行计算概率，-1就是表示把每一行的所有列做softmax(当然1也行，但0就变成以所有行做概率了)
        # x = F.softmax(x, dim=-1)
        # 但是在实际情况中我们可能并不会使用softmax而是他的优化版log_softmax，这是由于考虑到softmax中有指数存在，当输出的值过大的时候会出现越界的情况，log_softmax优化了这种情况，降低了输出的错误率
        x = F.log_softmax(x, dim=-1)
        return x


# 训练模式
def train(epoch):
    # 获取数据加载器
    dataloader = getDataloader()
    # 获取数据加载器的每一个batch，一个batch包含测试数据和预期输出，分别用input和target接收，同时input是一个四维张量，第一维是每一个batch中的一组数据，其余三维是图片数据，target则是一维张量只存放0-9数字，之所以不是二维的one-hot张量实际上也是为了节省空间，在计算过程中还是遵守one-hot的计算规范的
    for idx, (input, target) in enumerate(dataloader):
        # 梯度置零
        optimizer.zero_grad()
        # 获取神经网络输出
        output = model(input)
        # 交叉熵损失
        # loss = F.cross_entropy(output, target)
        # 同样的，由于在神经网络内返回的已经是log过的softmax了，因此这里我们没必要用交叉熵再log一次，nll_loss是带权损失，也就是交叉熵去掉了log之后的低阶版本
        loss = F.nll_loss(output, target)
        # 反向传播
        loss.backward()
        # 更新参数
        optimizer.step()
        # 每一百次输出一下损失值，并将当前模型保存到文件中长期存储
        if idx % 100 == 0:
            # torch.save可以直接保存神经网络模型的w,b和优化器类的最新参数，就算程序中断也不会使训练结果丢失，同时也为测试提供了方便
            save(model.state_dict(), './model/model.para')
            save(optimizer.state_dict(), './model/optim.para')
            print(loss.item())


# 测试模式
def test():
    lossList = []
    accList = []
    # 获取测试集
    dataloader = getDataloader(train=False)
    # 同样取出测试集的数据
    for idx, (input, target) in enumerate(dataloader):
        # 不需要计算梯度，仅测试求结果比对
        with no_grad():
            output = model(input)
            # loss = F.cross_entropy(output, target)
            loss = F.nll_loss(output, target)
            # 获取输出的每一行概率的最大值，就为本轮的结果，后一个-1表示取索引，因此可以直接取到预测输出的数字
            predict = output.max(dim=-1)[-1]
            # 看看预测输出和预期输出是否匹配，eq返回值为0或1，取平均就为当前预测的准确率
            accuracy = predict.eq(target).float().mean()
            # 每一个batch的平均存到列表中最后再取一次总平均
            lossList.append(loss)
            accList.append(accuracy)

    # 输出损失值和准确率
    print(np.mean(lossList), np.mean(accList))


# 初始化模型和优化器
model = MnistModel()
optimizer = Adam(model.parameters())
# 如果存在历史模型和优化器记录则恢复模型数据
if os.path.exists('./model/model.para'):
    model.load_state_dict(load('./model/model.para'))
    optimizer.load_state_dict(load('./model/optim.para'))

# 训练三组，即所有的数据在不同的batch划分下训练3次
for i in range(3):
    train(i)
# 测试准确率
test()
```

可以发现测试结果来看准确率已经达到了96%左右，这还只是我们使用了单层的28个隐藏层而已，能达到这样的效果可以说已经比较满意了，但是从另一个角度来说，我们在此处仅仅是判断10个不同的数字，对于一般意义上的我们可以想到的比如OCR（判断几千个文字词语），人脸识别（几万几亿种不同人脸）来说，就这么10个数字我们尚且无法实现百分百识别率，显然如果把这么一个模型套用到更复杂的问题中，很难说会有很好的表现

这时候就该轮到卷积神经网络登场了，而看到这，你终于可以说自己已经半只脚迈进了深度学习的大门，对于基础的神经网络的概念已经有了一定的了解，那么接下来，就该是对不同种类的神经网络进行更为深层次的学习了

## 卷积神经网络（CNN）

学习完了使用全连接的传统深度神经网络（DNN）完成手写数字识别的分类任务，接下来我们就要开始学习更进一步的卷积神经网络了，相信这个名字大家都十分熟悉但是可能也只是停留在知道的阶段了，在本节中我们同样从不至于特别深层次的原理入手，自己构建一个卷积神经网络完成一个花分类的问题，同时我们还可以了解到如何通过迁移学习，使用别人构建好的模型和神经网络架构来快速的完成任务量较大的任务

### DNN的缺陷

传统神经网络的方法还是十分给力的，但是如前一节所说，为什么说手写数字识别是深度学习中的`Hello World`级别的任务，就是因为他的输入和输出数量比较起其他经典的深度学习的案例来说实在是太少了，以至于我们可以通过整张图片全连接来生成隐藏层的每一个特征值，但是和我们想的不一样的是，即使是使用所有的像素生成特征，从结果来说似乎效果似乎并不是特别让人满意（96%准确率），难道说全连接的最复杂的神经网络还不如非全连接的看上去简单的网络吗？

在开始学习更为高级的卷积神经网络（CNN）之前，我们不妨先来反思一下前一节学习的传统神经网络存在着什么问题，之后我们就可以针对这几个问题看看在卷积神经网络中的解决方案

#### 参数爆炸

现在我们假设我们想要实现一个物品识别的功能，这个系统会传入一个$256*256*3$的图像，表示长宽都是256的三通道RGB图像，输出是一百个不同的物品，分别用序号表示$1\textasciitilde100$，假设我们有三个隐藏层，每一个隐藏层有1000个节点，我们可以试着算一算初始化参数的过程中我们需要定义多少个$w$和$b$

就拿输入层到第一层隐藏层来说，我们就需要$256*256*3*1000=196608000$个$w$，高达两亿个参数的参与导致整个神经网络计算起来十分的臃肿，虽然我们之前也提到反向传播的高效性，但是同样这也是相对性的，当参数的个数达到了一个量级，即使是数据的提取和调用都将会是变成一种负担，何况是求导、计算和更新，将极大拉慢神经网络的训练效率

其实这也是上一节我们的网络准确率不特别高的原因，最根本的原因其实是我们为了快速展示效果，设置的隐藏层数量（1）和神经元数目（28）太少了，导致可能并不能在输出层完全分离十种不同的数字，但是有些人可能会说，慢点就慢点怎么了，如果我知道最后训练出来的效果一定是好的，那么即使训练的速度不快我可能仍旧会使用全连接，毕竟算的不是我是电脑，有什么关系？这时候就需要引出全连接的第二个问题了

#### 过拟合

首先我们应该先明确，全连接网络一定包含了同等神经元数量下的所有非全连接网络，毕竟只需要将某一个神经元的$w$置0，那么这一条线的也就可以当作不存在了，那么又是什么导致最终我们并不采用它呢？

我们设想一个情况，现在我们的训练集内有100张面向左方的狗，然后我们拿全连接网络跑啊跑跑出了一个结果，比如这个结果的特征就是一个面朝左方的狗的轮廓，然而到了测试集出现问题了，此时出现了一个面朝右方的狗，这时候机器也傻眼了，首先我们明确的是它一定不会把他判定成狗，毕竟面朝右方的狗一定不满足面朝左方狗的轮廓，因此无论把他判断成什么我们都会发现他在这一个测试数据中出错了，但是你能说他训练效果不好吗，那可不一定，毕竟它描出了面朝左方狗的轮廓，或许他对于判断面朝左方的狗的判断正确率就是百分百

你可能说这还不简单，想要避免这个问题在训练的时候多搞一些面朝右方的狗的训练数据不就好了，但是我们也会发现这方法是有点治标不治本的，毕竟谁也不知道测试数据下一次又会出现什么幺蛾子，这次是出现一个朝右的狗，下次可能就是闭眼睛的或者是偏头的狗，总之你可能在自己的训练集上面跑出来近似百分百的正确率，但是到了测试集的正确率可能还不到百分之五十，这种训练和测试效果差距过大的情况就可以称之为过拟合（overfitting）

到底为什么会出现过拟合呢，第一个原因我们说过了就是训练数据集太少了，通俗的话说就是你拿一个平面区分空间中的两个点他显然是可分的，但是对于测试集的第三个点你就一定能保证他落在平面正确的一端吗？显然这个概率是不高的，但是如果训练数据是一万个点这样生成的平面显然在测试集上的效果就会好很多

而第二个原因，其实就是我们的参数数量太多了，之前我们也[提到过](#非线性拟合)，在神经网络中，每增加一层网络层，从一方面理解，就是对数据点进行了一次非线性变换，而从另一方面理解，就是对划分线进行了一次非线性的扭曲，如果我们能够把这条曲线画出来，他在不同复杂度的网络结构中可能是长这样的

![image-20221024203438615](https://s2.loli.net/2022/10/24/kHzRVfXEQ1PbID3.png)

可以发现第三张图的曲线好像在训练集当中直接分开了两个类别，正确率也就是百分百，但是这看上去非常复杂的曲线一定是通过某一堆参数控制的，比起第一张（直线，由两个参数$w$和$b$控制）和第二张（近似抛物线，由三个参数$a,b,c$控制），他所需要控制的参数一定不是只有三个那么简单，但是我们可以打赌说在测试集中效果和训练集一样好吗？比如我在图中画出的绿点，他在第二张图中能被正确分类，但是到了第三张图却被分为了五角星，而这个绿点会出现在测试集当中的概率就我们肉眼看上去是很高的，训练出了更多的参数，结果的效果却不好，这也能非常体现出过拟合的问题的例子

### 特征值

在揭示如何解决上述的问题之前，我们还得先了解一下特征和特征值的概念，这可不是线性代数里面的那个特征值，而是可以看作某一组数据的分类标准，我们都知道深度学习比起机器学习最厉害的地方就在于他可以自己去寻找数据集中的特征，根据自己训练出来的不同的特征去决定最后产生的结果，那么这些传说中的特征到底是什么东西呢

其实非常简单，所谓特征就是神经网络中每一个隐藏层的神经元，而特征值就是我们输出一组数据后每一个神经元输出的那一个值，没错，对于一组数据来说，特征值就是一个数值，而我们又知道每一个神经元实际上就表示一个维度，于是特征值就是某一组数据在这个维度上的映射值，而我们想要分类问题能够被理想的解决，其实我们需要不同类型的数据至少在输出层前一层的隐藏层所得出来的不同组的特征值在高维上是线性可分的，而为了能够可分，我们直观的想法就是要找出同组中不同数据的共性，把同组的共性部分经过某些运算放大到某些特征值中，同时保证这些共性部分尽量不被不同组的数据利用

比如数据集中有一个分类的第一和第三个输入值永远是1，而如果是别的分类的第一个和第三个输入值则永远是0，那么我们就可以在第一个隐藏层的神经元让其除了$w_1,w_3$以外的所有$w$都为0，此时无论输入的是什么数据，我们都可以通过这一个神经元输出是不是0来判断属不属于这一个分类，这一个神经元就是这一个分类的特征

既然特征值就是一组数据集中的共性部分所产生的，那么接下来我们还需要探讨一下所谓每一组的共性部分一般会怎么存在于数据之中呢，如今我们了解了一些关于特征的知识，不妨返回去再看看产生过拟合的第二个原因，在上一节我们提到是因为参数太多导致的过拟合，是因为参数太多导致特征提取的过于精确导致的过拟合吗？我认为不是的，而是参数的膨胀会导致可提取的特征值过多，导致神经网络只会提取对于给定的训练集来讲效果最好的特征，但是更可能的情况是这里提取出的在训练集中效果好的特征可能并不是这一种分类的特征，也就是全连接过拟合的最主要原因其实是他提取的特征本身就是错误的

于是我们就有了如何改进全连接网络的想法，我们是否可以先人为的规定好输入数据中哪几个输入单元在一起可能是会产生特征的，仅将这些会产生特征的输入单元连到下一层的某一个神经元中，而排除别的输入单元会产生的影响，如果拿前前一段的那个例子，我们如果知道了这个分类的特征，那么在设计神经网络的过程中完全可以对第一个隐藏层的神经元只连接第一个输入值和第三个输入值，这样就可以避免在全连接的训练的过程中可能会发生的其他输入值导致产生的变化

但是我们应该如何去寻找哪些输入单元可能会产生特征呢？

### 卷积核

于是就产生了卷积，不如我们来看看卷积是如何挑取输入值的

![image-20221025193432151](https://s2.loli.net/2022/10/25/6adDHvBl7FPM4Qw.png)

如图，从图中可以看出，单从参数量来说，卷积层通过两个方面大幅减少了参数量，第一个就是下一层的每一个神经元只接收指定数量的前一层神经元作为输入（如$f_1$只接收$a_1,a_2$作为输入），而第二个就是下一层的不同神经元之间部分神经元的线性函数相同，共用同一组$w$和$b$（如$f_1,f_2$），当然，如果有了解过卷积的人可能不太熟悉上面这个图，他们看到的版本大概是这样的

![image-20221025193807130](https://s2.loli.net/2022/10/25/EGq3KbvpTFINPxg.png)

这种说法是我们会拿着一个方框框$w_1,w_2$，然后将输入值也排成一个二维的方框框，然后拿着第一个方框框从左到右从上到下依次盖在每一个输入值排成的框框上，对每一个盖住的输入都和盖住他的$w$做乘法，之后再将乘法结果加起来就成为了输出值

这两种解释各有优劣，第一种的通过神经网络图的方式，让我们了解到卷积网络并不脱离原有神经网络的范畴，仅是在全连接的网络基础上去掉了一些连接线并共用了一些参数，从而减少了参数量，提高了训练效率，而第二个解释方式则告诉我们卷积网络的本质其实是只通过相邻输入值的关系生成特征，相比于全连接的网络，特征能够被抽取的范围被极大的缩小了

如果上面的图感觉还是不理解，也可以看下面的更高大上的动图

![动图](https://pic3.zhimg.com/v2-705305fee5a050575544c64067405fce_b.webp)

而其中的$w_1,w_2$所构成的方框框，在一般的神经网络中往往是像上面图一样以$3*3$排列的$w$，它叫做卷积核（Kernel），当然我个人认为这个名字有点太高大上了， 这玩意在图像处理中还有一个别名叫做滤波器（Filter），或许会更好理解一些

一张图片，能够让我们辨别这图像到底是什么东西的最关键问题，是其中某一块区域的内容，比如一条狗我们能够认出它是狗最主要的原因很可能是我们看到了一个狗头，而肯定不是因为我们看到了左上角的某一个像素和右下角的某一个像素，因此回到上一节中提出的问题，对于图像处理来说，可能会产生特征的输入单元组是非常好找的，就是一团围在一起的像素，因此我们只需要将每一团像素都映射成一个神经元，那么大概率能够训练出比较好的结果，这也是卷积核会产生的最主要原因

但是这只解决了卷积核产生的合理性，但是却没有说明为什么同一个卷积核还要沿着图像表面移动，也就是为什么会又想出通过参数共享的方式进一步降低参数量，我们也完全可以每一团像素都用一个不同的卷积核呀，这时候，就需要引出特征图这一个概念了

### 特征图

在考虑卷积核为什么要移动之前，我们不妨先来考虑另一个问题，之前我们提到我们要通过一团像素来映射出一个特征，但是我们也没有规定这一团像素有多大呀，比如对于一个鸟来说，我们只需要辨识他尖嘴巴就十有八九可以猜出他是鸟，但是对于一个孔雀来讲，我们可能需要辨识他的大尾巴才能够说他是孔雀，嘴巴和尾巴显然在图片中占有的像素大小是不一样的，你说这好啊，这不是刚好佐证了同一个卷积核不应该移动的理论吗，我只需要对判断尾巴的时候用一个大的卷积核，判断嘴巴的时候用一个小的卷积核，卷积核不复用，不就完美辨别出了鸟和孔雀吗

但是这还是会有一些问题，比如我们怎么知道哪些特征是大还是小，难不成每次训练神经网络之前都先拿一大堆大小不一的卷积核一个个试吗，这不就违背了深度学习能够自己寻找特征的理论了吗，此外，还有一个更加严重的问题，如果所有的卷积核都不一样，那么我们卷积完了的结果一定是一堆被打散了的特征值，对于输入层我们是知道相邻的像素点之间十分有可能有关系，但是对于这一堆被打散了的特征值，我们可判断不了哪些特征值之间还存在关系，也就是说，在这种卷积方式下，能且仅能做一次卷积，如果把卷积后的结果看作是新的一组输入值，由于并不能肯定相邻的各个输入值之间一定还存在特征关系，卷积的意义已经失效，对目前的我们来说，不得不回去乖乖的做全连接

但是如果采用多个相同的卷积核，且每一个卷积核都与整张图片做卷积运算，我们如果把一个卷积核和整张图片做完的结果重新排列在一起，我们会发现排列之后的结果似乎又变成了另一张图片

![这里写图片描述](https://img-blog.csdn.net/20180322161922874?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JyaWJsdWU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

先不管这个卷积核究竟长什么样，但是从结果我们可以发现，卷积之后的像素经过排列同样是一张至少我们肉眼能够分辨的图片，这张图片就叫做特征图，意味着我们单看这张图来说，这张图相邻的像素点之间仍旧十分有可能能够产生特征，一次卷积操作并没有破坏掉原有图片的空间关系，使得对其继续进行卷积具有十分的合理性，此外，由于一般来说我们还会在一层卷积网络中采用大小一样权值不同的不同卷积核对图片做多次卷积操作，意味着特征图也不止这么一张，我们用多少个卷积核，就会生成多少张不一样的特征图

### 感受野

之后我们就要来解决对于不同特征占有像素点不同的问题，事实上，我们上面也说过了，对于一般的卷积神经网络来说，卷积核的大小大多是$3*3$的，意味着在一层卷积网络中一个卷积核只能提取范围在九个像素围成的正方形之内的特征，这对于一般图片的特征来说显然是太小了，九个像素能看出来什么特征，但是我们上面也说过了，卷积操作是可以层层累加的，假如有这么两层的卷积层，我们看看对于经过两层卷积之后的其中一个特征图的一个像素而言，这个像素能够包含多少的初始图片像素信息

![img](https://pic2.zhimg.com/80/v2-e9bb5036f5be05aa9d592edf1eeb9839_720w.webp)

可以发现，对于第二层特征图的一个像素而言，他就能捕捉到初始图像中$5*5$个像素范围内的特征，而这$5*5$的范围就叫做这个点的感受野，你可能会说，那为什么我不直接一开始就用$5*5$的卷积核得了，用两个$3*3$的卷积核还增加了网络的层数，不是看上去更麻烦

第一点也是十分明显的优点就是能够减少参数量，由于一个卷积核的参数可以与整张图进行计算，因此使用$3*3$的卷积核两层也只需要两个卷积核$9+9=18$个w参数，但是如果使用$5*5$的话一个就需要25个参数了，其实这也十分容易理解，更变态的是，如果我们想提取两张用$5*5$卷积核卷积过的特征图，如果放在以前两个不同卷积核就得用$5*5*2=50$个参数，而现在我们只需要在第二层再加一个$3*3$的卷积核，最后生成的两个特征值分别就代表了由第一层的$5*5$生成的两个不同的特征值了，这在之后卷积层的后半段动辄就是几百几千个特征图来说是十分能够增加效率和防止过拟合的

而第二点其实也是不太容易理解的，虽然很多文章中提到好处其实就是增加了非线性的能力，但是并没有详细说明为什么提高了非线性的能力可以让小卷积核在效率上高于大卷积核，同时我们如果去看一些卷积网络应用于非图像处理的问题中，我们可能发现人家好像用的又全是大卷积核，如果只是因为参数量的不同的话，在某些领域为什么大卷积核又没有被完全淘汰呢？

### 再谈激活函数

我们考虑一个情况，假如某一张图的一个种类的特征真的就集中在这张图片的某$3*3$的区域内，那么我们只需要对其做一次卷积，就可以获得了一个非常准确的特征值，我们首先应该明确的是，一张特征图实际上经过了若干次卷积之后，包括之后我们会提到的池化操作，总之一通操作之后到最后可能只能够剩下个位数的特征值，而这个特征值就浓缩了这种分类的全部特征，而此时我们可能十分兴奋，因为等于说只经过一次卷积就直接找到了最后需要输出的特征值了，而神经网络也不傻，当他意识到可能这个特征已经足够明显了之后，虽然不至于一直始终不变的保留这个特征值，但也一定不会卷积卷着卷着就把这个特征给搞没了

同时不要忘了，到目前为止我们为了便于理解卷积都忽略了从神经网络本身的角度出发理解卷积层，但是只要他还是一个神经网络，那么逃不开的就是输出到下一层之前需要经过一个激活函数，因为我们之前也提到这个激活函数就是为了给神经网络增加非线性关系的，而在逻辑回归的那一章中，也明确说明了这一个非线性关系在分类问题中的重要性，没错，由于最后我们需要使得数据在高维空间内线性可分，因此需要对其中的每一个点进行非线性的映射，当然另一种解释是这个激活函数能够帮我们找出一条高维曲面来分类各个情况，无论是什么解释，这个非线性的激活函数都是为了能够增加某一个特征的区分度的，而经过越多层的非线性网络，从结果上来说这一个特征值在不同分类的情况下输出的特征表现是越明显的

知道了这一点，我们自然能够想到，假如一个特征在卷积层的早期就已经被确定了，那么在接下来的几层当中，当别的特征图还在找范围更大的特征的时候，这个已经被确定的特征值已经在通过非线性的方式开始寻找自己和别的特征之间的区分度了，也使得它能够在最后输出时已经很好的区分于其他不同的特征

如果把上面两个优势综合起来，其实可以发现他们是相辅相成的，小的卷积核可以最大程度减少参数量，但是它减少参数量也是有代价的，就是为了能够和大的卷积核达到同样的效果，它需要叠加更多的层才能够捕捉到较大的特征，但与此同时，在叠加多层的过程中，从另一方面来讲，尤其是对于在卷积早期就已经被确定的局部特征来说，会非常大程度的强化特征的非线性区分能力，从而获得更好的输出效果

回到开头的问题，那为什么在某些应用领域，小卷积核反而效果不如大卷积核呢？其实用上面的例子也能够解释，就是当我们明确某数据的特征不可能存在于某一小范围之内时，此时小卷积核的一个十分危险的缺点就体现出来了，就是小卷积核每一次的卷积操作实际上是会损失掉一些信息的，尤其是那些价值差不多的特征，由于最后只能输出一个值，因此明明某些特征的重要性差不多，然而卷积之后只有相对表现最好的特征能够被保留，而被舍弃掉的特征则不会被保留到下一层，但是下一层的卷积核可不知道这一决策，从而导致的结果不理想，只不过对于图片输入来说，往往相邻像素之间的差异很小，信息本身就有一定程度的冗余，因此我们才近似的将小卷积的叠加看作是一个大卷积核，但是对于一些相邻输入信息差异敏感的数据来说（比如语义识别），大卷积核更能够完整的掌握到全局的特征，从而在输出上获得更好的结果

*越大的 kernel 越擅長整合高階的信息，小 kernel 則比較擅長處理低階的信息，比如紋理等等   ——知乎*

### 步长

结束了对于卷积核大小的讨论，我们接下来需要考虑一下另一个问题，图片中怎么样的特征容易被提取出来呢，或者我们换一个说法，我们在识别一张图片的内容中，一般我们会根据什么特征去捕捉这个图片的种类

如果我们思考一下就可以发现，我们似乎大多数识别一个东西大多数看的都是他的轮廓，而往往对于图片中的物体进行分类的过程中物体的边缘部分往往包含了非常大的信息量，这个边缘不只是整体的轮廓，比如一只狗的眼睛轮廓，嘴巴轮廓都可以作为识别的特征，而这轮廓，用更专业一些的话来说就是纹理特征，了解了纹理在图片识别中的重要性，我们回过头来，再来重新分析分析卷积操作生成的特征图，看看所谓的卷积核提取的特征是否能够和物体的纹理挂上勾

纹理从另一个角度来说其实就是线条，我们拿最明显的方形举例子，看看卷积核操作过一次之后生成的特征图会有什么变化

![image-20221030203057862](https://s2.loli.net/2022/10/30/GMQ5ugz12hyc3vL.png)

通过观察输出特征图我们会发现，当且仅当卷积核和特征图上的纹理内容一致的时候所输出到特征图的数值大小越大，这也是非常好理解的，你可能会说，图片内的像素不都是连续的吗，像素周边的像素点往往都是接近的，为什么会出现像上面这样0，1差距那么大的范围，实际上不要忘了我们讨论的前提就是考虑图片的纹理，而对于一张图片来说往往我们肉眼就会自动的将颜色变化差异大的地方看作是纹理或者是物体边缘，而就算是彩色图片，其颜色变化激烈的地方也一定存在至少一个通道该处周围的值大小变化强烈的，在这个例子中只是将其进一步抽象为0和1罢了

因此我们知道了，单从辨别纹理的这个问题来说，我们只需要看看输出的结果有多大即可判断出该位置是否存在与卷积核较为匹配的边缘特征出现了，但是我们同样也发现了一个问题，就是在特征图中那个完全匹配的特征值旁边的像素点往往延续了和完全匹配差不多的大小（比如上面的8旁边的像素点值都不小）有些人可能就觉得没必要了，明明这个最明显的特征都已经完全被提取出来了，凭什么还需要它周边的点，之前不是一直说要提高效率吗，如果当匹配到产生8之后下一次卷积自动跳过它周边的像素点该多好，比如让卷积核每卷积完一次移动两格不行吗，凭什么就要乖乖一格一格移动，实在是太慢了

确实在之前我们都没有提过为什么卷积核卷积完一次之后只往右移动一格然后进行下一次运算，这其实也是在卷积网络中一个比较重要的预设参数，步长，如果我们将步长设为2，那么特征图就是长这样的

![image-20221030202726467](https://s2.loli.net/2022/10/30/HFmZeVLR21POczY.png)

移动的步长虽然不会减少各层的参数量（因为卷积核的大小和多少不会变），但是可以发现步长增加后由于特征输出减小，因此可以显著的减少每层的计算量，也可以算是变相的提高了效率，但是这种无脑加步长的方法真的好吗

### 最大池化

来看下面的例子

![image-20221030204323498](https://s2.loli.net/2022/10/30/L4Siq9UPJkvO1js.png)

可以发现只是变了一下轮廓的位置，但是步长为2输出的特征图却完全不同，如果我们拿步长为1输出又会怎么样呢

![image-20221030204756563](https://s2.loli.net/2022/10/30/t9QwN7qWHpPbSKE.png)

可以发现虽然一眼看上去不太相同，但是如图的红圈，可以发现在局部方面还是能具有一定的相似性的，但是这样的相似性会在下一层卷积或者全连接的过程中被学习到吗

可惜的是，卷积操作本身并不具有空间变换不变性，这其中包括平移不变性，旋转不变性和缩放不变性等，也就是说，你把一只狗图像平移，旋转，放大缩小之后神经网络学习到的实际上都不是一个东西，只不过他们在神经网络看来只是不同的图像都输出狗罢了，而这个也是为什么如果以后我们在实际解决图像识别的问题之前也就是预处理阶段都要进行的一步数据增强，即把同样的图片经过不同的空间变换后作为不同的输入提供给神经网络

究其原因实际上还是因为对于神经网络来说，每一层的特征值位置是不允许变化的，也就是说，在一层中即使是输出同一组的特征值，如果他们的顺序不一样那么神经网络就不会认为他们的输入是同一组数据，这在一定程度上保证了输入数据的结构规范，比如如果神经网络能够识别出狗头，狗身子和狗尾巴的特征组合，如果测试集内有一个狗头和狗尾巴单独连接在一起，然后狗身子跑到另一个地方去的图片，虽然满足了所有的特征，但是由于他们的位置不一样，因此神经网络并不会将其识别为一只狗，这也是合理的

那么有没有什么办法可以解决这个问题呢，如果对于纹理识别的要求来说还真有一个不是办法的办法，就是对输出内容进行最大池化，他的操作是这样的

![image-20221030212359648](https://s2.loli.net/2022/10/30/EYTgBIpKHiltGwF.png)

可以发现，对于经过了平移了一个像素点的图片来说，经过了池化之后的特征图相似度极高，就算是我们肉眼单看这四个输出的特征值都能轻松看出他们应该代表的就是同一张图片了，更重要的是他保证了输出特征值的位置关系，大大减轻了神经网络的训练负担

但是我们还是应该记住，这种方法只对图片特征是纹理的或者特征图中特征值最大值对结果影响更大的情况下可以使用，虽然池化操作增强了网络的平移不变性，但是代价就是他实在是损失了比较多的细节局部特征，以至于我们都很难看到池化范围大于$2*2$的网络结构

池化范围的限制导致池化操作有一个十分尴尬的弊端，就是他在每一次处理过程中只能纠正特征图平移范围在一个像素范围内的平移，要是移个两像素你试试输出的位置还一不一样，而这对于一张动辄几百像素的输入图片来说似乎这一不一像素的移动好像并没有起到什么关键性的作用，还不如我一开始就设置步长为2直接学习两个特征图来的方便，而且步长为2在计算量上效率比步长1之后再通过池化这么一套流程要少一倍还多

因此池化层到目前为止受到的质疑声也在不断增大，很多论文都指出事实上池化的操作是可以被省略或者通过增加步长来替代的，事实上利用卷积网络的大家熟知的开山鼻祖AlphaGo就没有用池化层，究其原因还是它蛮横的操作方式导致经过了它的数据特征损失过于严重，对于围棋这种每一个落子位置都十分重要的问题来说这种损失是不可接受的

事实上池化操作也还有很多不同的变形，比如说平均池化，什么金字塔池化，分数阶最大池化等等等等，你可以在`pytorch.nn`下找到各种乱七八糟的池化名字，他们有的是为了应对除了纹理特征不同的特征情况，有的是为了解决我们上述说的问题等，此处就不举例了

### 边缘填充

假设我们在某一层拿到了这样的一个特征图

![image-20221031140804169](https://s2.loli.net/2022/10/31/8qV4WxoCFL1rOhk.png)

我们肉眼可以非常容易的发现，左上角和右下角具有同样的纹理特征，按理说应该他们应该可以被同一个卷积核提取出来，但是对于一般的$3*3$卷积核，你会发现无论如何设置和放置都无法同时将这两个特征一起提取出来，但是如果同样的特征位于特征图中心，就不会存在这样的情况，对于位于特征图边缘的特征被卷积核忽略的这种情况就叫做特征丢失，为了避免这种情况，我们就引入了边缘填充操作

![image-20221031143509676](https://s2.loli.net/2022/10/31/dQk6mXSELYol8a3.png)

当我们在特征图外面加了一圈0之后，我们会发现无论卷积核长的是如图绿色这样还是红色这样都可以同时提取出这两个长相类似的纹理特征，同时由于填充的内容都是0，因此也不会影响卷积核的参数更新

边缘填充的另一个作用就是保持输出特征图的大小不变，为什么我们希望输出特征图大小不变呢，在感受野一章中我们就会发现每经过一次卷积，都会使输出的特征图减少两行两列，池化操作更是会让特征图大小直接减少一半，这导致假如原始输入的像素不够多，比如手写数字识别的$28*28$个像素来说，如果不进行边缘填充，可能没卷几次特征图就给卷没了，导致深层次的网络根本进行不下去，而如果提前进行了一单位的边缘填充，同样的卷积操作就可以保证输出的特征图尺寸不变

从另一方面讲我们也更希望特征图输出可以统一变小而不是逐层变小，这样在神经网络结构搭建完成之后，可以更大程度的允许我们在评估训练结果之后在原有架构上增加和删除卷积层

### CNN结构

有了这么多的内容储备，接下来我们就要开始正式介绍卷积神经网络的架构了，事实上在看完前面的内容你可能会有问题，你说特征图虽然可以卷积，但是无论你卷多少次不还都是特征图，而且由于卷积核还有不同种类的，意味着最后生成的特征图可能非常多，如何将这些特征图分类成指定的几种输出类型呢

同时根据我们目前掌握的知识来说，似乎一个卷积网络需要我们设定的参数十分多，比如卷积核大小，步长，边缘填充的大小，池化和卷积层的安排等等，不过对于目前我们来说这些都没什么需要担心的，因为有一大批的研究者替我们负重前行，他们研究出的效果好的神经网络可以作为我们自己设计的参考，以下就是在2014年的视觉识别挑战赛中获得了亚军的VGG网络的总体架构，我们会发现当我们有了一定的知识积累之后，即使是看这些似乎很复杂的网络也不会那么困难了

![img](https://pic4.zhimg.com/80/v2-ea924e733676e0da534f677a97c98653_720w.webp)

VGG严格上来说有六个不同的种类，对应着上面的A-E，为了方便起见我们就拿最简单的A，11层网络举例

首先我们看到他的输入为$224*224*3$的RGB图片，需要注意的是对于多通道图片来说，实际上每一个卷积核也应该是多通道的，也就是说对于第一层来说，每一个卷积核都是$3*3*3$的三维核，每一个三维核都会和一张图片三个通道的$3*3$像素相乘相加，而图中的`conv3-64`，前面的3代表卷积核大小是$3*3$，后64代表有64个卷积核，注意在卷积之前都提前加了1像素的边缘填充，因此经过了第一层的卷积之后，$224*224*3$的图片变成了$224*224*64$个输出，也就是一个64通道的特征图

接着我们对这个输出在经过一个ReLU激活函数之后再进行一个$2*2$的最大池化，注意到在卷积层当中所有的参数都包含在卷积核中，因此无论是激活函数和池化都不应该被归为单独的一层，我们之前也推过了，在池化之后特征图大小会减半，因此池化之后$224*224*64$变成了$112*112*64$

依此类推，经过了第一个池化之后的输出将经过第二次卷积层，别忘了卷积之前的边缘填充，同时在第二次的卷积中我们进一步增加了卷积核数量和每一个卷积核在通道维度，此处使用了128个$3*3*64$的卷积核，也就是在这一层需要训练的$w$数量为73728个，虽然看上去不少，但是比较起全连接的一兆多个（$112*112*64*112*112*128$）参数来说其实也就那样，经过了这一次卷积之后特征图的通道数继续扩展为128个，输出个数为$112*112*128$

接着继续进入池化，由于特征图大小不变，因此池化后到了输出为$56*56*128$这个量级，但是虽然总体输入变小了，但是卷积核数量也随之增大到256个，意味着输出的通道数也扩展为256，而且经过两次256个卷积核的卷积再进入池化

后面都同理，由于只有在池化过程中会减少输出，因此结果的输出数也十分好算，到了最后一个`conv3-512`卷积之后的池化后只剩下了$7*7*512$个输出，接着我们发现下一层变成了`FC-4096`，意思就是这些输出将要作为输入经过一个输出为4096的全连接层，至于如何进入全连接自然就是和我们之前做手写数字识别一样将其拉成一个长长的一维向量，由于此时特征图的每一个通道内仅包含49个特征值，因此在这一阶段实际上邻近的特征值虽然还有一定的关系但是更大的程度上已经在前面那么多的卷积层中特征都提取完毕了，因此完全可以将每一个特征值单独看作是一个特征输入，同时数据量也不大因此对于这些明显的单独特征显然还是全连接操作更能进一步区分他们，接着我们看到经过了两个输出4096的全连接，然后又经过一个1000输出全连接接着通过softmax输出1000个分类结果

可以发现，卷积神经网络实际上就是在传统的神经网络基础上插入了若干层的卷积层，单从输出的结果来看，卷积层的作用实际上就是在原有输入的图片的大量像素基础上重新抽取有限少量的更具有代表性的特征值输入到传统神经网络，从而减少了过拟合风险，提高了训练效率，同时他从图片本身具有的结构特征角度出手构建卷积操作，使网络本身具有了理论依据

### 优化手写数字识别

有了卷积神经网络的知识储备，我们也终于可以开始利用他来重写我们的手写数字识别案例了，以下就是代码部分

```python
from torchvision.datasets import MNIST
from torchvision.transforms import Compose, ToTensor, Normalize
from torch.utils.data import DataLoader
from torch import nn, save, load, no_grad
import torch.nn.functional as F
from torch.optim import Adam
import numpy as np
import os


BATCH_SIZE = 64  # 一个批次64个数据


# 获取数据加载器
def getDataloader(train=True):
    preprocessing = Compose([
        ToTensor(),
        Normalize(mean=(0.1307,), std=(0.3081,))
    ])
    dataset = MNIST(root='./datasets/mnist',
                    train=train, transform=preprocessing)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    return dataloader


class CNNMnistModel(nn.Module):
    def __init__(self):
        super(CNNMnistModel, self).__init__()
        # 第一层卷积层
        self.conv1 = nn.Sequential(
            nn.Conv2d(
                in_channels=1,  # 输入的通道数
                out_channels=16,  # 输出的通道数
                kernel_size=5,  # 卷积核大小5*5
                stride=1,  # 步长为1
                padding=2,  # 填充为2
            ),
            # 非线性
            nn.ReLU(),
            # 池化
            nn.MaxPool2d(kernel_size=2)  # 以2*2为单位池化
            # 此时输出为16层每层()(28-5+4)/1+1=28[卷积])/2=14[池化]，因此输出为(14,14,16)张量
        )
        # 第二层卷积层
        self.conv2 = nn.Sequential(
            nn.Conv2d(16, 32, 5, 1, 2),  # 同上
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
            # 输出为(7,7,32)张量
        )
        # 第三层输出层全连接
        self.out = nn.Linear(32*7*7, 10)  # 全连接同样和之前一样要将输入拉长成一维特征

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)  # x.size(0)是batch大小，-1为所有维度拉长为一维的大小
        x = self.out(x)
        return F.log_softmax(x, dim=-1)


# 训练模式
def train():
    dataloader = getDataloader()
    for idx, (input, target) in enumerate(dataloader):
        optimizer.zero_grad()
        output = model(input)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if idx % 100 == 0:
            save(model.state_dict(), './model/mnist/cnnmodel.para')
            save(optimizer.state_dict(), './model/mnist/cnnoptim.para')
            print(loss.item())


# 测试模式
def test():
    lossList = []
    accList = []
    dataloader = getDataloader(train=False)
    for idx, (input, target) in enumerate(dataloader):
        with no_grad():
            output = model(input)
            loss = F.nll_loss(output, target)
            predict = output.max(dim=-1)[-1]
            accuracy = predict.eq(target).float().mean()
            lossList.append(loss)
            accList.append(accuracy)

    print(np.mean(lossList), np.mean(accList))


# 初始化模型和优化器
model = CNNMnistModel()
optimizer = Adam(model.parameters())
if os.path.exists('./model/mnist/cnnmodel.para'):
    model.load_state_dict(load('./model/mnist/cnnmodel.para'))
    optimizer.load_state_dict(load('./model/mnist/cnnoptim.para'))

# 训练三组，即所有的数据在不同的batch划分下训练3次
for i in range(3):
    train()
# 测试准确率
test()
```

可以发现大体的代码框架基本都没有发生变化，仅仅是在模型中多加了两个卷积层，在总体训练时间不变的情况下，在测试集上的正确率差不多可以达到99%左右，还是十分优秀的，通过两个网络的效率和喷哦对比，我们也能够很明显的感受到卷积神经网络的强大

### ResNet

神经网络一定是越深越好吗？在很长一段时间里，大多数人认为在保证不过拟合的情况下，神经网络越深，代码训练的效果越好，于是当计算机的训练速度越来越快，许多人都尝试训练更深层次的网络，但是事实似乎并不如它们预料，当神经网络的深度越来越深，训练出来的结果却越来越差

![img](https://pic2.zhimg.com/80/v2-dcf5688dad675cbe8fb8be243af5e1fd_720w.webp)

可以发现56层相对于20层无论是在测试集还是训练集上的错误率都要高，这显然不是过拟合导致的问题，然而明明56层的网络其中就包含了20层网络，究竟是什么导致了这样的结果呢

在此之前我们先思考一个问题，假设我们有一个非常简单的网络

![image-20221103135343784](https://s2.loli.net/2022/11/03/DwQ1L4PtvG2gK6Z.png)

我们不妨来算算$f_3$，$f_4$于此同理就不列式了，其中激活函数用$\sigma$代替
$$
f_3 = w_5\sigma(w_1x_1+w_3x_2+b_1)+w_7\sigma(w_2x_1+w_4x_2+b_2)+b_3
$$
既然我们都知道高层次的神经网络是包含着低层次的神经网络的，但是结果发现高层网络的效果不如低层，从最简单的角度来讲，很可能由于面对某些低层网络已经输出较好结果的时候，高层网络无法重新模拟低层网络的输出

因此我们就来算算对于$f_3$，如何可以将其还原回相对应的$x_1$，即令$f_3=x_1,f_4=x_2$，这就代表着我们中间这层网络没用了

你说这个我会，不就是做一个恒等映射吗，我直接把交叉的两条线都断掉不就成功了，即令$w_2,w_3,w_6,w_7$和所有b都等于0其他w等于1不就好了，这还不简单，我都会的题目难不成计算机还不会吗

遗憾的是，或许计算机真的不会，于是有些人可能会问了，你不是说过正常的训练方法一定最后会落到全局最优的地方吗，既然之前的网络都训练出更好的结果了，为什么后面的网络突然就开始拉垮了呢，如此简单的恒等映射神经网络怎么就训练不出来了

其实原因还是出在激活函数上，我们都知道激活函数的作用是为了升维拟合函数，另一种说法是他降维了特征值（因为经过他映射之后原本非线性的输入数据线性可分了），但是他没有的功能正是不改变数据维度，假如原本的输入数据或者是特征就已经是一维的了，那么你再一降维岂不是降没了？这种现象被称为特征退化，或者说是数据坍塌，正是由于这个原因，网络早期出现的特征虽然我们之前也说过网络的深度会不断强化特征，但是随着网络的越来越深，这些特征会在不断的非线性变化强化到一定程度时到某一层开始又慢慢消失了，但是我们之前也说过，特征值之间的维度可不是一致的，有些特征值在第一二层就已经表示的很明显了，但是有些特征值却要等到网络后期才会出现，而对于维度不一的特征来说，低维度特征在高层次的网络中很难完整留存，当大多数明显的特征都不被保留，高层的网络训练效果低于低层网络也理所当然了

当然这可能还是有点玄乎，但是你只需要记住，引入了激活函数之后在高层神经网络中，原本简单的恒等映射操作会变得十分困难，如在[MobileNetV2](https://arxiv.org/pdf/1801.04381.pdf)的论文中有提到的这张图

![image-20221104142723404](https://s2.loli.net/2022/11/04/UeCFlhPY2Jf7IE6.png)

会发现在经过了随机矩阵变换的特征图，经过了ReLU之后再经过随机矩阵的逆变换，却不能还原回原有的特征图，难以实现恒等变换，于是我们就可以想想，有没有什么办法可以提高神经网络进行恒等变换的能力呢

于是就有了Residual Network（残差神经网络，ResNet），拿上面的例子来说，他在$f_3,f_4$的计算表达式中加入了一项，变成了这样
$$
f_3 = w_5\sigma(w_1x_1+w_3x_2+b_1)+w_7\sigma(w_2x_1+w_4x_2+b_2)+b_3+x_1
$$
![img](https://pic4.zhimg.com/80/v2-252e6d9979a2a91c2d3033b9b73eb69f_720w.webp)

这样就为恒等映射提供了可能，而此时网络输出的含义也就不一样了，之前我们都说每一层的输出是由前面的输入为基础的新特征值，但是在残差网络中，输出的$F(x)$实际上是旧特征值在扩大感受野之后相对于其本身的特征残差值，即相对于原有特征增强或更明显的部分，假如特征完全没有被增强，输出的结果没有向好的方向发展，那么$F(x)$就会在梯度下降中趋向于0，实现了恒等变换

### 迁移学习

以我们目前的知识，我们当然可以手撸一个比如ResNet，VGG，但是面对几十甚至几百层的网络结构，怎么说我们自己搭还是有些够呛，不只是搭的够呛，搭完之后训练的过程也会更加头疼，这时候我们就可以通过评估我们的实现目标，比如说我接下来想要做一个102种花分类的任务，而所谓的VGG也好ResNet也好他们所做的也是1000种物种分类的任务，既然目的相同，我们就可以考虑将他们的网络借过来用用，这也就是迁移学习的根本思想，它既帮我们节约了构建网络的时间，更重要的是由于别人已经训练出来了十分良好的模型，因此我们甚至可以直接套用别人训练好的参数，仅改动输出部分来快速的训练出我们自己的模型

像是ResNet156有156层的参数，要是我们自己训练不知道要训练到什么时候，而且更重要的是还会有过拟合的风险，因此我们可以只改动全连接的部分，冻结之前所有别人训练好的卷积层，这样即使是我们自己的破电脑也可以快速完成训练

这也就可以看作卷积网络的另一个特点，即对于大多数图片来说，其大部分的特征边缘轮廓都是相似的，因此比如我训练不同狗的网络卷积核给训练花的网络用其实效果也不错，虽然狗和花长得不太一样，但是他们的纹理特征大多还是类似的，这使得迁移学习的效果也能比较理想

接下来通过一个102种花分类的案例来尝试一下迁移学习的威力（数据集可以直接百度搜到）

```python
import torchvision.transforms as tsf
import torchvision.datasets as dts
import torchvision.models as presetm
from torch.utils.data import DataLoader
from torch.cuda import is_available
from torch import nn, optim, no_grad, load, save
import os
import torch
import time

TRAIN_DIR = './datasets/flower_data/train'
VALID_DIR = './datasets/flower_data/valid'
SAVE_DIR = './model/flower/'
BATCH_SIZE = 8
USE_TRAIN = False


# 获取数据加载器
def getDataloader(train=True):
    preprocessing = {
        # 数据增强
        't': tsf.Compose([
            # 在45度范围内随机旋转
            tsf.RandomRotation(45),
            # resnet网络输入为224
            ts# 获取模型保存的路径f.CenterCrop(224),
            # 百分之50的概率水平翻转
            tsf.RandomHorizontalFlip(p=0.5),
            # 色彩随机变化，亮度变化0-0.2，对比度0-0.1，饱和度0-0.1，色调-0.1-0.1
            tsf.ColorJitter(brightness=0.2, contrast=0.1,
                            saturation=0.1, hue=0.1),
            # 同以前
            tsf.ToTensor(),
            # resnet网络默认均值和方差
            tsf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        # 测试集不需要数据增强
        'v': tsf.Compose([
            # 重置为256
            tsf.Resize(256),
            # 裁切为224
            tsf.CenterCrop(224),
            # optimizer


# 获取模型保存的路径
def generatePath():
    suffix = ['{}{}.para'.format('model' if i == 0 else 'optim',
                                 'resnet152_fc' if USE_TRAIN else 'googlenet')
              for i in range(2)]
    modelPath = SAVE_DIR+suffix[0]
    optimPath = SAVE_DIR+suffix[1]
    return modelPath, optimPath


# 数据集比较大，因此记得放到gpu上跑
if not is_available():
    print('CUDA is off,train on CPU')
else:
    print('CUDA is on,train on GPU')
device = torch.device('cuda:0' if is_available() else 'cpu')
# 获取模型和优化器
model, optimizer = getMandO()
# 训练，每训练完一轮跑一轮测试集看效果
train()同上
            tsf.ToTensor(),
            tsf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }
    # ImageFolder会识别目录下的所有文件夹，其中每一个文件夹的名字是一个种类的标签，其中是这个种类的图片
    dataset = dts.ImageFolder(root=TRAIN_DIR if train else VALID_DIR,
                              transform=preprocessing['t' if train else 'v'])
    # 获取dataloader
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    return dataloader


# 训练模块
def train(epocoptimizer


# 获取模型保存的路径
def generatePath():
    suffix = ['{}{}.para'.format('model' if i == 0 else 'optim',
                                 'resnet152_fc' if USE_TRAIN else 'googlenet')
              for i in range(2)]
    modelPath = SAVE_DIR+suffix[0]
    optimPath = SAVE_DIR+suffix[1]
    return modelPath, optimPath


# 数据集比较大，因此记得放到gpu上跑
if not is_available():
    print('CUDA is off,train on CPU')
else:
    print('CUDA is on,train on GPU')
device = torch.device('cuda:0' if is_available() else 'cpu')
# 获取模型和优化器
model, optimizer = getMandO()
# 训练，每训练完一轮跑一轮测试集看效果
train()h=25):
    # 训练多少轮，因为每一轮都会对图片进行不同的数据增强，因此可以近似认为每一轮的输入都是不同的一组数据
    for i in range(epoch):
        # 记录时间
        epochStart = time.time()
        print('Epoch {}/{}'.format(i+1, epoch))
        # 将模型调整为训练模式
        model.train()
        # 本轮合计损失值
        curLoss = 0
        testTotalAcc = 0
        # 获取dataloader
        dataloader = getDataloader()
        # 一个batch一个batch循环
        for input, label in dataloader:
            # 注意要将数据交给gpu管理
            input = input.to(device)
            label = label.to(device)
            # 梯度置零
            optimizer.zero_grad()
            # 前向传播，返回的是logsoftmax
            output = model(input)
            # 求损失值
            loss = nn.functional.nll_loss(output, label)
            # 反向传播
            loss.backward()
            # 更新参数
            optimizer.step()
            # 累加损失值
            curLoss += loss.item()
            # 预测结果
            predict = output.max(dim=-1)[-1]
            # 当前batch准确率取平均
            testTotalAcc += predict.eq(label.data).float().mean()
        # 本轮测试平均损失
        epochLoss = curLoss/len(dataloader)
        testAcc = testTotalAcc/len(dataloader)*100
        # 测试时长
        epochDuration = time.time()-epochStart
        print('Time elapsed: {:.0f}m {:.0f}s'.format(
            epochDuration // 60, epochDuration % 60))
        print('Train Loss: {},{}%'.format(epochLoss, testAcc))
        # 测试一下看看效果
        test()
        # 保存模型
        save(model.state_dict(), generatePath()[0])
        save(optimizer.state_dict(), generatePath()[1])


# 测试模块
def test():
    # 获取测试集数据
    dataloader = getDataloader(False)
    # 将模型转换为测试类型
    model.eval()
    testTotalLosoptimizer


# 获取模型保存的路径
def generatePath():
    suffix = ['{}{}.para'.format('model' if i == 0 else 'optim',
                                 'resnet152_fc' if USE_TRAIN else 'googlenet')
              for i in range(2)]
    modelPath = SAVE_DIR+suffix[0]
    optimPath = SAVE_DIR+suffix[1]
    return modelPath, optimPath


# 数据集比较大，因此记得放到gpu上跑
if not is_available():
    print('CUDA is off,train on CPU')
else:
    print('CUDA is on,train on GPU')
device = torch.device('cuda:0' if is_available() else 'cpu')
# 获取模型和优化器
model, optimizer = getMandO()
# 训练，每训练完一轮跑一轮测试集看效果
train()s = 0
    testTotalAcc = 0
    for input, label in dataloader:
        # 测试集没有反向传播
        with no_grad():
            # 但是仍旧要把参数交给gpu管理
            input = input.to(device)
            label = label.to(device)
            # 获取测试输出值
            output = model(input)
            # 累加损失
            testTotalLoss += nn.functional.nll_loss(output, label)
            # 预测结果
            predict = output.max(dim=-1)[-1]
            # 当前batch准确率取平均
            testTotalAcc += predict.eq(label.data).float().mean()
    # 计算平均损失
    testLoss = testTotalLoss/len(dataloader)
    # 计算平均准确率
    testAcc = testTotalAcc/len(dataloader)*100
    print('Test Loss: {},Test Accuracy: {}%'.format(testLoss, testAcc))


# 获取训练模型和优化器
def getMandO(outKind=102):
    # 这里训练模型假如不冻结参数的话选择resnet18层模型，冻结参数的话选择resnet152层模型
    if USE_TRAIN:
        model = presetm.resnet152(
            weights=presetm.ResNet152_Weights.IMAGENET1K_V1)
        # 冻结的话就将所有参数都不求梯度
        for para in model.parameters():
            para.requires_grad = False
    else:
        model = presetm.googlenet(weights=presetm.GoogLeNet_Weights.DEFAULT)
    # 直接替换原有的全连接层为我们自己的，因为resnet输出1000种类，我们只需要输出102个花种类
    model.fc = nn.Sequential(nn.Linear(model.fc.in_features, outKind),
                             nn.LogSoftmax(dim=-1))
    # 模型运算交给gpu
    model = model.to(device)
    # 获取需要求梯度的参数列表，如果是冻结参数的话只需要全连接层参数
    para = []
    for name, param, in model.named_parameters():
        if param.requires_grad is True:
            para.append(param)
    # 初始化优化器类
    optimizer = optim.Adam(para)
    # 加载模型
    if os.path.exists(generatePath()[0]):
        model.load_state_dict(load(generatePath()[0]))
        optimizer.load_state_dict(load(generatePath()[1]))
    return model, optimizer


# 获取模型保存的路径
def generatePath():
    suffix = ['{}{}.para'.format('model' if i == 0 else 'optim',
                                 'resnet152_fc' if USE_TRAIN else 'googlenet')
              for i in range(2)]
    modelPath = SAVE_DIR+suffix[0]
    optimPath = SAVE_DIR+suffix[1]
    return modelPath, optimPath


# 数据集比较大，因此记得放到gpu上跑
if not is_available():
    print('CUDA is off,train on CPU')
else:
    print('CUDA is on,train on GPU')
device = torch.device('cuda:0' if is_available() else 'cpu')
# 获取模型和优化器
model, optimizer = getMandO()
# 训练，每训练完一轮跑一轮测试集看效果
train()
```

这里我使用了resnet152的迁移学习和googlenet不迁移学习做对比，虽然对于我们100多个的分类数据且数据集比较小的情况下来说，他们到最后似乎都过拟合了，但是从训练结果来说，resnet152可以在测试集上达到最高74%的准确率，而googlenet大概只有68%左右，如果用resnet18则效果更差，只有61%不到，也可以进一步说明迁移学习在原本就过拟合的模型上能够表现出更稳定的性质

> 值得注意的是他们在训练集上的数据准确率大多高达96%-98%左右，还是比较恐怖的

### Batch Normalization

在进入下一个章节之前，如果有细心的人可能发现了，当我们想要在pytorch中导入一个resnet模型的时候，如果我们把这个resnet模型结构输出一下，会发现在每一个Conv层到ReLU的中间，好像都多了一个东西，也就是所谓的BatchNorm2d，这究竟是个什么玩意？

![image-20221128221402432](https://s2.loli.net/2022/11/28/GkKIagz357PSnFZ.png)

它的出现甚至比池化还要频繁，那么他究竟是个什么玩意呢

在此之前，我们可以先来思考一个问题，在前面我们也提到了，其实每一个卷积核在作用之后都在原有图像的基础上又生成了一个叫做特征图的东西，既然这个生成的特征图叫做图，那么他必然就是有一些图片的性质了，事实其实也是如此，我们完全可以将其就看成是一张图片，于是我们又想到，既然我们在前面提到了，当我们把一张图片输入到神经网络之前我们需要对其做一下预处理，而预处理中最为重要的便是[图像标准化](#图像标准化)的操作，那么对于这张特征图来说，我们是否也可以对其做一下标准化呢

于是就有了Batch Normalization，中文也可以叫做批量标准化或者直接简写为BN

批量标准化的具体实现其实就是之前说过的Z-Score，但又不完全是，不过在说明他的运算规则时，我们不妨先来了解一下他的作用数据，我们都知道，在做Z-Score标准化的时候我们需要求出一个平均值和方差，而批量标准化正如其名一样，他求的平均即为所有batch中的同一组特征值的平均，例如我的批量大小为5，意味着一次会送入五组数据到神经网络中，理所当然就会输出五组特征值，特征值数量即为这一层的神经元数量，假定为十，而我们要做批量标准化，将会产生十个平均和方差，每一个都是由五个batch对应输出的特征值产生的

当然这是传统的神经网络也就是输出维为二维的情况是这么做批量标准化，我们知道，当网络变成一个卷积神经网络，虽然我们可以硬画把输出看作是一堆一维神经元，但是事实上我们更习惯于将其看作是具有不同通道的特征图，也就是对于任意一层卷积层，他的输出其实是四维的，从高到低分别是batch维，通道维，高和宽，此时的批量标准化有点特别，他将是取所有batch在同一个卷积核下的所有特征图取平均和方差，也就是不管输入进来一个batch多少组数据，也不管你的特征图高宽是多少，标准化的运算次数只跟通道有关，或者说只跟卷积核的数量有关，你一层用了几个卷积核，那接下来就需要几次标准化

搞清楚了批量标准化的作用范围，现在就会有一个问题，由于我们取的是所有batch上的平均值，因此如果batch比较少或者比如当我们训练好一个模型，进行测试的时候根本不存在batch的情况应该怎么办呢，没有了batch维显然没办法自己跟自己做平均，归一化的平均和方差又该怎么求呢

这还不简单，用上一次的均值和方差不就完了，但是问题是上一次的均值和方差又该怎么保存呢，而且我怎么知道什么时候需要计算均值和方差，什么时候需要记录呢

针对第一个问题，批量标准化采用的方式正是我们在不同梯度下降中提到的指数加权平均法进行均值和方差的保存，而对于是否要计算本次均值和方差其实模型也没办法智能识别，因此需要我们在训练和测试两种情况之前手动设置，这也是为什么我们的迁移学习代码在训练和测试的之前需要先将模型调整为训练模式和测试模式的原因了

于是我们终于可以来看看批量标准化的公式，他长这样
$$
\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \\
\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2} \\
\widehat{x}_{i} \leftarrow \frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}} \\
y_{i} \leftarrow \gamma \widehat{x}_{i}+\beta \equiv \operatorname{BN}_{\gamma, \beta}\left(x_{i}\right)
$$
可以发现在$\widehat{x}_{i}$之前其实就是我们之前说过的Z-Score标准化，我们当然可以直接将这个$\widehat{x}_{i}$输出，但是其实批量标准化并不想定死标准化之后的均值和方差（$\widehat{x}_{i}$一定是均值0方差1），于是提供了$\gamma$和$\beta$来对标准化结果进行偏移，增大了神经网络的灵活度，他们可以在神经网络的迭代中被学习

同时每一次运算完之后，我们还需要通过指数加权平均保存住本次的$\mu_{\mathcal{B}}$和$\sigma_{\mathcal{B}}^{2}$到$\bar{\mu} $和$\bar{\sigma}$，当模型模式为测试时直接使用这个保存的$\bar{\mu} $和$\bar{\sigma}$参与运算$\widehat{x}_{i}$

另外，我们应该十分明确的是，批量标准化由于只是单纯的改变了输出的分布，因此他的作用和我们之前讲的普通标准化操作其实是一样的，它只会让梯度下降的速度变得更快，从而使模型更快的到达临界点，而并不会像无脑增加神经网络层数一样让训练结果变好或变坏，因此虽然他被称为批量标准化层，甚至其中还包含了需要训练的参数，但是由于在足够长的时间下，它改变不了模型的最终结果，因此在实际构建神经网络的时候虽然我们频繁的添加批量标准化层，但是我们并不会将其算作一个真正意义上的网络层，他的作用其实和池化层差不多

批量标准化带来的最大的好处就是大大的节约了神经网络训练的成本，神经网络模型产生速度的加快直接为训练更大更复杂的网络提供了可能，同时由于经过标准化之后的超平面下降曲线是平滑的且其产生的结果大概率在0-1的范围内，因此我们甚至可以摆脱各种乱七八糟的梯度下降方法和ReLU的制约，也就是说，我们可以直接暴力调一个很大的学习率，然后暴力使用最简单的SGD和暴力使用Sigmoid作为激活函数，这种在正统训练方法中蠢到不行的选择实际上有了批量标准化就可以化一切不可能为可能，下面就是对比了他对神经网络训练效果的提升

![image-20221129162537819](https://s2.loli.net/2022/11/29/wOzC7ZpqdVExi6y.png)

> 虽然看上去好像也增加了准确率，事实上只是标准化之后避免了下降时的震荡（降不到最低），最低点的值还是不变的

## 循环神经网络（RNN）

除了图片处理之外，深度学习的另外一个特长便是文字处理，比如翻译，语音识别等等，早期，人们都习惯使用RNN来处理文本的语义信息，当时众多科学家们还搞出了比如GRU、LSTM来优化RNN的训练结果，但是由于之后Transformer的强势出现，直接将RNN踢下神坛，直至到现在连CNN的处境也变得越来越危险

但是由于虽然Transformer就是RNN的上位替代，但是通过RNN的案例我们可以了解到文本处理相较于图片处理的不同之处，其中的一些方法并不是RNN独有的，只有掌握了这些方法我们之后才能够无缝衔接到Transformer的学习当中

### 隐变量自回归

还记得一开始的吃饭机器吗，现在我们要重新来看看这个问题，我们当时提到，我们已知了前若干天的吃饭内容，现在需要机器来预测今天应该吃什么东西，但是实际上我们会发现这并不是一个严格的回归问题，因为我们并不知道后天的吃饭内容，如果以日期为横轴，我们永远都会往日期更大的方向预测，这种已知现在去预测未来的行为就可以称作序列模型

而语言模型同样满足序列模型的条件，在卷积神经网络的时候，我们考虑了对于一张图片来说，他的特征可能会产生在什么地方，然后我们发现输入图片的某一个像素点周围的像素点最容易产生特征，于是诞生了卷积核专门用来提取相邻像素的关系，如今来到了语言模型，我们仍旧需要寻找对于一句话来说他的特征可能会产生在什么地方

如果说图片要找的是每个像素周围的像素，那么语言模型要找的就是每个文字周围的文字，俗称为上下文，比如有这么两句话“我好饿，我要\_\_”和“我好渴，我要\_\_“，如果我们现在有一个预测文本的模型，我们需要让他知道第一个应该填面包，第二个应该填水，那么我们就必须让这个模型捕捉到饿和渴这两个字，于是我们也可以看出语言和图片之间的区别，在识别图片的时候我们还特地强调了，一般来说距离较远的像素是不会有什么关系的，就算有什么关系，也一定是两个像素之间所有的像素合在一起描述出了什么东西（比如一大堆像素就可以画出一个狗），但是到了文本，对于两个相距较远的字来说，或许只有这两个字之间存在十分强烈的关系而与中间出现的字关系不大（比如渴和饿不会影响“，我想”这些字的内容，但是会影响面包和水的内容）

这时候就可以提出所谓的隐变量自回归模型，他所描述的是这么一个情况，从句子的开头开始，当读入第一个字$x_1$的时候，他把这个字通过某种手段变成另外一个玩意$h_1$，并且输出一个东西$y_1$，但是先不管这个$y_1$，接着读入第二个字，这时候我们拿着由第一个字产生的$h_1$和读入的第二个字$x_2$一起作用产生一个$h_2$，接着还是输出一个$y_2$，但是同样的我们不管这个$y_2$，接着输入第三个字和$h_2$作用生成$h_3$和$y_3$，就这么一直下去，直到遇到了已知的最后一个字，比如上面的“我好饿，我”都已经被读取完毕，现在我们应该会有一个$h_5$和$y_5$，此时我们读入“要”，然后拿着这个想和$h_5$作用，产生$h_6$和$y_6$，这时候注意了，由于我们已经读入了所有已知文本信息，因此我们希望这个$y_6$应该就是我们需要得到的面包，同时如果我们接下来还想多预测几个字，那么我们就需要拿着这个$y_6$作为新的输入$x_7$，和$h_6$作用产生新的预测字$y_7$

![../_images/sequence-model.svg](https://zh.d2l.ai/_images/sequence-model.svg)

我们会发现，在隐变量自回归模型中，所有已知的字共同维护了一个隐变量$h$，虽然我们不知道这个$h$是怎么生成的，但是由于他不断的接收已知的所有字信息，因此理应对于需要填入字的前面所有字的变化都是敏感的，对于这个$h$来说，饿和渴作为$x_3$是完全不一样的两个输入，因此到了$h_5$两个不同输入产生的$h$也一定是不同的，从另一种角度来说，隐变量自回归模型记录了一个句子当中的所有信息，从而保证语言模型的预测能够足够准确

除了能够对过往的输入进行记录，隐变量自回归模型还有一个十分重要的特性，也是我们之前没有提到的，就是语言模型的顺序性，比如“我爱你”和“你爱我”就是两个十分不同的语境，虽然构成的字都是一样的，但是文本排列的顺序同样会影响到对于结果的判断，而隐变量自回归模型同样可以考虑这个条件，由于每输入一个字产生的$h$都是不一样的，因此即使输入的$x$一样，由于同为输入的$h$不一样因此当然输出的$h$也是不同的

说了那么多，我们会发现这个隐变量的模型似乎就是专门为语言模型量身定制的，正因为如此，其实RNN以及它的各种变种都是在此基础上进行的

### Embedding

但是在正式进入RNN之前，我们还需要考虑一个问题，即如何才能把一个字输入进程序里去呢，有人会说这还不简单，电脑怎么存储字的我们也怎么输，直接输每一个字的Unicode编码不就好了，或者有些人说不是也可以遇事不决one-hot一下，毕竟比如常用的中文字也就几千个，每一个字一个萝卜一个坑看上去也不错

的确，这两种方式其实都可以，但是这种编码方式也忽略了词语之间的相关性，比如我好饿我可以吃面包也可以吃米饭，甚至吃电脑勉强也说的过去，但是我们不希望模型输出一个吃吃吃之类的动词序列，于是我们希望面包和米饭的编码结果可以比较相近一些，而面包和吃两个词则需要通过编码尽可能不类似，这样才能让模型输出的结果朝着我们预期的方向发展

而在此处我们也发现了，在处理图片的时候，我们往往希望可以在不破坏特征的情况下尽可能的压缩图片的尺寸，使输入到神经网络的数据越小越好，这样不仅可以增加训练速度也减少了过拟合风险，但是到了语言模型，我们反而希望能够为每一个文字增加特征，主要的原因还是每一个单独的字从表面上来看所蕴含的特征实在是太少了，这样的特征数量根本不足以支撑训练出一个良好的处理模型，为了能够有一个优秀的编码机制，我们就需要去寻找每一个词语蕴含的特征，我们只需要将这些特征排成一个向量再输入进神经网络，结果符合预期的可期待值也就变高了

这种**将一个词映射到一个向量的技术就叫做Embedding（词嵌入）**

不过我们不希望这个特征是由人找出来的，毕竟我们顶多也就能分分动名词，褒贬义之类十分表面的东西，而且为每一个词都标注一堆特征显然也不是很现实，就拿隔壁图片输入举例子，每一张图片动辄就是几百上千级别的输入量，意味着对于神经网络来说，我们得把一个词根据上千个特征进行标注，想想都能累死人

与此同时，还有一个十分重要的点需要注意，虽然我们需要在输入的时候将一个字或者一个词转换为指定长度的特征向量，但是我们却不希望输出的预测的词也会是一个特征向量，因为我们的分类器说白了就是softmax只能将最后一层的输出转换为one-hot的预测结果，因此所有语言模型的输出一定是整个词典的大小（比如一共有10000个不同中文字词），也就是我们需要解决的是一个词典大小的分类问题

话说回来，我们到底应该如何进行词嵌入呢，其实这也是很长一段时间在自然语言处理方面十分热门的话题，但是所有的这些话题都随着BERT的出现而消失了，在BERT出现之前的比如word2vec，GloVe，ELMo等等词嵌入模型由于在BERT的面前都显得如此不堪，按理说也没有什么学的必要了，但是如果没有这么一个学习的过程，到时候学到BERT一定是一脸懵逼的，所有的事物都有一个发展的过程，只有把简单的内容学懂了，我们才能理解复杂的模型究竟强在了什么地方，因此我们从最简单的word2vec里面的Skip\-Gram入手，浅浅的通过这个例子来说明一下词向量究竟是如何被训练出来的

### Skip-Gram

其实Word2Vec里面包含了两个模型，他们是Skip\-Gram跳元模型和CBOW连续词袋模型，不过其实他们总体的思想都差不多因此其实学一个也差不多了，反正我们的重点也不在这

研究词向量的过程中，我们会发现我们的输入是一个词（或者是一个词的one-hot），输出是一个若干维的向量，这么一个结构是不是很像一个神经网络模型，是的，其实对于简单的词向量模型来说，他就是一个字词通过一个全连接的神经网络的输出，只不过我们得想个办法，让这个全连接网络在输入为相近的词语时输出可以比较相似

我们都知道，在一般的神经网络中在训练的过程中，我们有明确的标签，也就是我们已经拿到了正确答案，之后我们可能用均方误差，可能用交叉熵，总之找到了一种衡量输出和答案差距的方法去计算输出的正确度，从而实现反向传播来优化网络的参数

于是我们知道了，想要某一个网络的参数朝着我们希望的情况发展，第一点我们需要找到标准答案，第二点我们需要找到衡量输出结果和标准答案差距的办法，且这个差距是可导的，以便于我们通过反向传播来更新参数

可惜的是，如果我们稍加思考就会发现我们并不能找到标准答案，因为我们也不知道词向量最终长什么样，不然我们也不会用词向量生成模型了，因此我们可以退而求其次，既然找不到标准答案，我们就去寻找标准答案可能是什么样的

跳元模型假设了一种情况，他认为一个词可以用来在文本序列中生成其周围的单词，因此在这个模型中，标准答案就是一个词周围的单词，比如现在我拿到了一个句子"I love you"，那么我希望其中的"love"和"I"和"you"的词向量相似度比较高，你说这不对啊，你不是之前才说名词应该和名词比较相似，动词应该和动词比较相似吗，怎么现在又变成一个句子的各个单词应该比较相似了，但是你需要注意的是，首先我们没办法通过几个句子具体量化的表示某几个名词他们哪两个更加相似何况在一个句子里单独把动名词抽出来也没什么意义，其次所谓词的特征只是我们假想出来的一个东西，在实际过程中我们不关心其中任何一个值的具体含义，由于词一般都是用于组成句子的，因此我们希望的只是当碰到某两个词向量的时候，机器能够判断这两个词向量应不应该呆在一起，这么一来，跳元模型的思想其实是完全可以被接受的

接受了跳元模型的思想，下一问题是我们如何判断两个词向量的相似度，看上去好像很难，但是如果我们抛开词这个概念，利用我们浅薄的线性代数的知识，想一想两个向量在什么时候是比较相似的，这个答案还是比较容易的，显然是这两个向量共线的时候，也就是我们可以通过夹角的方式来判断向量的类似性，而当向量等长的时候，夹角的大小还可以通过内积的大小判断，与此同时我们又知道对于一层神经网络的输出每一维大概率是在0-1之间的，因此我们完全可以直接用两个向量的内积大小来进行相似性的衡量

有了这些知识储备，我们终于可以来稍微看看跳元模型的损失函数了，他长这样
$$
P\left(w_{o} \mid w_{c}\right)=\frac{e^{\mathbf{u}_{o}^{\top} \mathbf{v}_{c}}}{\sum_{i \in \mathcal{V}} e^{\mathbf{u}_{i}^{\top} \mathbf{v}_{c}}}
$$
其中$w_{o},w_{c}$分别表示周围单词和中心单词，也就是上面的I和love，而$\mathbf{u}_o,\mathbf{v}_c$自然就表示这两个词的词向量，当然在一开始这词向量都是乱七八糟的，然后我们看到了我们熟悉的东西，没错就是这softmax，我们想要让I和love的词向量更相似，就应该让他们的内积跟所有其他单词和love的内积相比更大，可以发现，当我们确定了神经网络的训练标准之后，之后的事情都是异曲同工的，按照流程一套，自然解决方案就出来了

事实上，由于不只是I在love的周围，you也在，包括可能还存在的上下文，我们需要自己定义一个区间m表示在这个区间内的单词都可看作和love有一定的联系，此外我们一般比较喜欢求最小值而不是最大值，因此完整的损失函数是这样的
$$
-\sum_{t=1}^{T} \sum_{-m \leq j \leq m, j \neq 0} \log P\left(w^{(t+j)} \mid w^{(t)}\right)
$$

### 负采样

不过有人可能发现，这玩意不太好求呀，因为如果我们拿着$logP$去对$\mathbf{v}_c$求导，我们会发现结果是这样的
$$
\frac{\partial \log P\left(w_{o} \mid w_{c}\right)}{\partial \mathbf{v}_{c}} =\mathbf{u}_{o}-\sum_{j \in \mathcal{V}} P\left(w_{j} \mid w_{c}\right) \mathbf{u}_{j}
$$
为了求一个单词的梯度，我最后得把这个单词和所有单词的P全部算一遍，要知道一个词库的单词可不是只有几百几千个，面对数以万计的单词每一次梯度下降都要求一次可以说十分的困难

这还不简单，除了相邻的字词之外，其他的字词每次只取一些不就好了吗，是的，这就是负采样，虽然听上去挺简单，但是却是词向量模型训练的过程中极为重要的方法，在这个问题中，我们除了可以在分母位置少取一些样本之外，由于假设已知一个字词的情况下，取得其他字词的事件都是独立的，因此我们还可以用我们浅薄的概率论知识将条件概率转换成联合概率，我们定义
$$
P\left(D=1 \mid w_{c}, w_{o}\right)=\sigma\left(\mathbf{u}_{o}^{\top} \mathbf{v}_{c}\right)\\
P\left(D=0 \mid w_{c}, w_{o}\right)=1-\sigma\left(\mathbf{u}_{o}^{\top} \mathbf{v}_{c}\right)
$$
他表示我们取得了两个字词，这两个字词是否相邻的概率，其中$D=1$即表示字词相邻，$D=0$当然是不相邻，由于此处是一个二分类问题，因此被我们冷落好久的sigmoid函数终于可以一展身手了，有了这两个联合概率公式，我们就可以将原本的条件概率转换为这样
$$
P\left(w^{(t+j)} \mid w^{(t)}\right)\approx P\left(D=1 \mid w^{(t)}, w^{(t+j)}\right) \prod_{k=1, w_{k} \sim P(w)}^{K} P\left(D=0 \mid w^{(t)}, w_{k}\right)
$$
其中$k$就是我们随机取的不属于这个字词上下文的字词，当然这个$k$越大越接近真实的结果，但是和小批量梯度下向和普通梯度下降一样，在训练次数足够多的时候其实由于每一次负采样都是随机进行的，因此整体下降结果和预期状况是一致的，通过这种近似训练的方式使得词向量模型的构建成为了可能

### 朴素RNN结构

有了隐变量自回归模型和词向量模型，我们终于可以看看RNN的结构到底是什么样了，在前面我们唯一没有提到的就是在隐变量自回归模型中，我们的这个$h$究竟是怎么计算的，而这也是各种RNN放飞自我的地方，之后在讨论各种RNN的变种的过程中，我们会发现不同的RNN模型变着法子在这个$h$上做文章

首先当然是最传统的RNN模型，通过了解他的网络结构我们可以进一步的巩固前几节学习的内容，他大体上长这样

![image-20221202223425055](https://s2.loli.net/2022/12/02/B4y3jfcaYMO7gHW.png)

可以发现抛开红线不看，这就是一个十分简单的单隐藏层神经网络，输入$x_1,x_2$即为编码好的单个词向量，注意到一般来说在进行业务流程之前我们都会预先训练好一个词向量模型，在网络传播的过程中冻结其中的参数避免被反向传播更改，我们将词向量输入到网络中，由于是输入的第一个词，因此我们可以先不管红线，直接先通过一个全连接网络，需要注意的是，由于我们之前也提到了整个网络需要共同维护一个隐变量，因此隐藏层的输出$f_1,f_2$除了要参与下一层的网络运算产生$g_1,g_2$之外，我们还需要将其保存起来返回给用户，暂时我们先不管为什么要返回给用户，对于本次前向传播来说，$f_1,f_2$会进入输出层，假设我们要预测下一个词，那么我们的输出就是一个词典大小的超长向量，同样是通过softmax来预测下一个词的编号

然后关键来了，RNN由于使用了隐变量自回归的模型，因此最显著的特点就是能够记住前一次输入的内容，在下一次输入的时候把前几次的输入产生结果同步参与计算，在第二轮我们拿到了第二个词，此时我们不止握有第二个词的词向量，还记得吗，第一轮的时候模型返回给我们当时隐藏层的输出$f_1,f_2$，现在我们将其称作$h_1,h_2$，于是我们拿着第二个词的词向量和前一轮返回的结果$x_1,x_2,h_1,h_2$一起输入进网络，此时是一个四输入到两个输出的全连接网络，网络会用他们同样产生一个$f_1,f_2$，同样，这个$f_1,f_2$需要备份一份进行返回给用户，另一份继续参与网络的运算，而由于此时的$f_1,f_2$是由上一次的$h_1,h_2$和本次的词向量共同作用的结果，因此从抽象的角度来看一定是本次输入和以往所有的字词信息共同的运算结果，这也符合了隐变量自回归的模型定义

> 注意$h_1,h_2$指的是上一次产生的隐变量，而$h_1',h_2'$指的是本次产生的隐变量，只不过在朴素RNN中$h_1',h_2'$和$f_1,f_2$实际上一样的

### BPTT

看了那么久神经网络一定很累了，不如我们一起来复习一下反向传播最重要的求导法则，看下面这个式子，我们应该怎么通过求导的方法获取$f_3$的导数呢
$$
f_3(x) = ((x+1)x+1)x+1
$$
我们当然可以把他展开，那么就变成了$f_3(x) = x^3+x^2+x+1$，易得$f_3'(x) = 3x^2+2x+1$，但是计算机可不会乘法分配率，何况这还只是一个最简单的复合函数求导，如果算式复杂一些可不是我们化简一下就可以求导的

你可能发现了，$f$就$f$，你干嘛要搞个$f_3$，这不很简单，有3意味着有2和1呗，我们把他变一变，你是否可以把$f_3$看成这么一个递推关系式呢
$$
f_1 = x+1\\
f_2 = f_1x+1\\
f_3 = f_2x+1
$$

好像确实是这么一回事，如果我们把$f_3$看作是由$f_2$和$x$共同组成的复合函数，然后我们用我们小学就学过的复合函数求导法则（或者其实就是链式求导法则），当然如果你忘了复合函数怎么求导，我可以把公式放在下面，其中$u,v$都是可以由$x$表示的函数
$$
f = uv\\
\frac {df}{dx} =\frac{\partial f}{\partial u}\frac{du}{dx}+\frac{\partial f}{\partial v}\frac{dv}{dx}=u'v+uv'
$$
可能我们记住的都是等号的最后一条式子，但是实际上他的完全式是中间这样的，有了这条式子，我们就可以尝试通过这种方式求导$f_3$了
$$
\begin{align*}
\frac {df_3}{dx}&=\frac{\partial f_3}{\partial x}\frac{dx}{dx}+\frac{\partial f_3}{\partial f_2}\frac{df_2}{dx}\\
&=\frac{\partial f_3}{\partial x}+\frac{\partial f_3}{\partial f_2}(\frac{\partial f_2}{\partial x}\frac{dx}{dx}+\frac{\partial f_2}{\partial f_1}\frac{df_1}{dx})\\
&=\frac{\partial f_3}{\partial x}+\frac{\partial f_3}{\partial f_2}\frac{\partial f_2}{\partial x}+\frac{\partial f_3}{\partial f_2}\frac{\partial f_2}{\partial f_1}\frac{df_1}{dx}\\
&=\sum_{k=1}^{3} \prod_{j=k+1}^{3} (\frac{\partial f_j}{\partial f_{j-1}})\frac{\partial f_k}{\partial x}\\
&=f_2+xf_1+x*x*1=3x^2+2x+1
\end{align*}
$$

> 我们假设底层$f_1$不包含复合$x$项，因此$\frac{df_1}{dx}=\frac{\partial f_1}{\partial x}$成立

结果一定是对的因为我们只是采用了不一样的方法求解导数，重点是我们在倒数第二行似乎推出来了一条怪怪的公式，拿着这条公式，我们似乎可以方便的求解任意t次递推关系后的$f_t$的导数，即有以下式子
$$
\frac {df_t}{dx}=\sum_{k=1}^{t} \prod_{j=k+1}^{t} (\frac{\partial f_j}{\partial f_{j-1}})\frac{\partial f_k}{\partial x}
$$
这种原函数是由递推关系生成的，需要我们递归求导之后产生的就是著名的BPTT（随时间反向传播）的式子，他和反向传播还有和时间到底有什么关系我们可以先放一放，你可以先把他记住，之后我们会详细说明，但是现在我们先来看最后我们得出的求导结果即$f_2+xf_1+x*x*1$这个结果

### 梯度爆炸

相信上过幼儿园的同学应该都知道对于结果来说，这个递归了三次的$f_3$的导数是一个二次函数，而且我们也会发现随着递推次数增加到$t$，$f_t$的求导结果的最高项一定是$x^{t-1}$，想象一种情况，现在我在$f_{101}$上取了一个点$x=2$，假设我现在不知道$f_{101}$长什么样，但是我希望能够找到$f_{101}$的最低点，婷婷，这问题怎么这么熟悉，接下来的环节是不是就要引出梯度下降了，没错，现在我正打算使用梯度下降，于是我对$f_{101}$求了个导，然后把$x=2$代入导数，这可不得了，我们发现这个导数的结果居然大的无与伦比，达到了$2^{100}$的量级，对于这个量级来说，大多数的电脑肯定已经罢工不干了，但是假如真有一个电脑能存储这么大的数，下一步就是根据这个梯度朝梯度方向挪一小步，大概率我们要把$2^{100}$乘上一个学习率，然后让$x$减去这个数重新赋回自己，但是我们站在上帝视角来看，假如我们考虑最简单情况$f_{101}=x^{101}$，那么我们只需要让$x$向左移动两个单位让$x’=x-2$即可到达最低点，但是由于梯度实在是太大了，如果要让$\eta 2^{100}=2$这个学习率也得是$2^{-100}$这个量级的，显然这是不可能的，出现了这种违背常理的梯度，我们就说这个神经网络有梯度爆炸的风险

![image-20221204172226280](https://s2.loli.net/2022/12/04/BGaxCpXkdRQAjVf.png)

我们发现造成梯度呈指数上升的罪魁祸首就是这个递推式，而我们会发现我们推导的过程同样也使用了链式求导的法则，这是不是意味着正常的神经网络也会有梯度爆炸的可能性呢

这个可能性当然是有的，但是在一般的神经网络中，由于反向传播每一层梯度的大小都是有大有小的，虽然网络的层次变深确实会让多个梯度进行相乘，但是他们相乘的结果也不会至于太离谱，而当求导的目标是这么一个递推式的时候就不一样了，他在反向传播的过程中只涉及单变量递归求导，因此只要一出现$x>1$且递归的层数比较深的时候，就非常容易出现梯度爆炸

我们来看RNN的网络结构，在前前一节中，我们计算到了第二轮产生隐变量并且返回的过程，我们不妨再多算一轮，我们用$h_t$表示第$t$轮产生的隐变量，易得如下式子
$$
h_1 = w_xx+b_h\\
h_2 = w_xx+h_1w_h+b_h\\
h_3 = w_xx+h_2w_h+b_h
$$
注意为了表示方便，其中的所有变量都是矩阵形式，而且没有考虑激活函数，乘法运算均为叉乘，当我们需要求解$w_h$的梯度的时候，你会发现这不是和前面我们讲的情况一模一样吗，没错，所谓BPTT正是RNN的隐变量更新时的反向传播方式，而其中的时间当然指的是每输入一轮，就会使所谓的递归层次多加一层，意味着假如一个句子里面有一百个字，那么当你读到第一百个字的时候，每一次反向传播的过程就需要递归一百次，梯度同样也会上升到一百次方这个量级

这就是RNN为什么非常容易产生梯度爆炸的原因，那么我们应该如何解决它呢？

#### 梯度裁切

不就是因为递归次数变多了嘛，那我让他少一点不就完了

确实如此，这就是所谓的截断时间步，我们可以让反向传播的时候递归到某一层的时候手动跳出就行了，不过由于在pytorch中的反向传播只能设置是否计算梯度，并没有提供反向传播详细的实现过程，因此这个方法其实实现起来并不是很容易，于是就有了梯度裁切的方法，他会将过大的梯度直接裁切到我们指定的阈值$\theta$，从而暴力的控制不出现梯度爆炸，详细的算式是这样的，其中的$\mathbf{g}$就是当前的梯度
$$
\mathbf{g} \leftarrow \min \left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}
$$

> 有些人可能会说RNN当$x<1$的时候也会出现梯度消失，但是稍加分析就可以发现由于递归求导过程中还会产生次方更小的项，而当$x<1$的时候确实高次项会产生一个特别小的数，但是最终决定权实际是在低次项上和常数项上，与此同时梯度消失的定义其实是函数在无穷远处的导数为0（比如sigmoid），因此RNN从架构上来看其实并不会有梯度消失的问题

### GRU

通过学习朴素RNN，我们发现RNN比起普通的全连接网络只是多了一个隐变量需要维护，同时对于朴素RNN模型，由于在隐变量的产生过程中我们只是单纯的使用了隐藏层的输出作为结果，因此没有参数可以学习，这种方法虽然效率比较高，但是每一个词的输出都使其强制的考虑了前面所有词的信息，而在实际情况中，语句中的词并不是由前面所有的词决定的，比如我们可以把一开始我们的例子拓展一下，现在他是这样的“我好饿，我要面包。今天天气不错”

同样我们一个一个字往下读入，我们会发现比如要产生面包这两个字的时候，我们需要格外注意“饿”和“要”这两个字，而对于其他的比如“好”，“我”甚至逗号其实都不是特别重要，他们在这个句子里甚至可以不存在或者被替换为其他同义词，因此我们希望这个句子所维护的隐变量应该注重于记录“要”和“饿”，而对于其他的词可以选择性的记录，也就是当读入到比如”我”的时候，我们希望这个“我”被赋予一个较小的权重，再加入前一层的隐变量，而读入到“要”的时候，我们则希望这个“要”能够被赋予一个较大的权重再加入隐变量

然后我们再来看后半段句子，我们会发现其实后面的句子和前面的句子可以说是一点关系都没有，我们完全可以把他看作是另外一个句子，当然这个例子比较极端，在正常情况下距离当前词越远的词自然和当前词的关联度越小，这并不是由前面的某个词决定的，更大的可能是我可能都已经开始讲另外一件事情了，因此面对这种情况，我希望训练出来的模型可以选择性的忘记前几轮隐变量的内容，也就是比如读到这个句号的时候，我希望可以赋予整个隐变量一个较小的权重后再加入句号的输出结果，为了之后我们读到“今”的时候的输出可以不考虑以往的隐变量影响，尽可能接近单独一个“今“的输出结果，不再考虑前面的什么“饿”什么“面包”了

这就是GRU在干的事情，对于第一个要求，他定义了一个更新门，对于第二个要求，他定义了一个重置门，从宏观角度来看，更新门是用来处理本次输入经过一个全连接之后产生的输出结果的，而重置门则是用来处理共同维护的隐变量的，与传统神经网络不同的是，这两个门都是又以一个全连接网络为架构的，意味着他们对词的处理是可学的，我们希望通过这两个结构可以实现前面我们对语句分析后的处理结果

我们来看看加了这两个门结构之后的GRU神经网络会变成什么样子，如果我们把他画出来，大概是这样

![image-20221202213901793](https://s2.loli.net/2022/12/02/AODjGf1FcqPSEKr.png)

此处彩色的线均表示一层神经网络，对比上一张朴素RNN的图，我们可以发现除了绿色的线是朴素RNN也存在的，GRU又多加了两组神经网络，他们分别就代表着重置门$r$和更新门$z$的生成过程，为了进一步理解其中的细节，我们可以把他和下面这张细致一些的图一起看

![../_images/gru-3.svg](https://zh.d2l.ai/_images/gru-3.svg)

对照着这两张图，我们不妨来走一遍整个流程

首先我们拿着上一轮的$h_{t-1}$（$h_1,h_2$）和本轮输入的词向量$x_t$（$x_1,x_2$），首先我们就得对$h_{t-1}$和$x_t$做两次全连接（红线和蓝线），注意此处输出维度应和$h_{t-1}$保持一致（因为词向量和$h_{t-1}$的维数往往是不一致的），且全连接的激活函数是一个sigmoid函数，因为他的输出是重置门和更新门，意味着需要重置和更新多少内容，首先我们拿到了重置门$r$，他是一个概率向量（$r_1,r_2...$），表示我需要重置多少上一次的$h_{t-1}$，因此我们把他和$h_{t-1}$进行点乘得到了被重置了一定程度的$h_{t-1}$，我们拿着这个被处理过的$h_{t-1}$和输入$x_t$进行一轮全连接获得了$\tilde{h_t}$（$f_1,f_2$），这个过程就可以看作是朴素RNN中的全连接部分，拿到了本轮的输出结果，我们接下来要确定这个更新结果是否存入最终的$h_t$，因此我们需要使用更新门$z$，与此同时我们还需要注意为了防止一直累加导致输出的结果$h_t$越来越大，我们采取了两个措施，第一个是让产生$\tilde{h_t}$的激活函数变成tanh，控制输出在$(-1,1)$之间，另外我们还让更新门同时控制$h_{t-1}$和$\tilde{h_t}$，即$h_t = zh_{t-1}+(1-z)\tilde{h_t}$，使得结果可以被控制在一定范围内

极端情况下，当重置门的输出为0，更新门的输出为0时则直接抛弃原有的$h_{t-1}$，本轮输出的$h_t$仅由词向量$x_t$控制，而当更新门的输出为1时则直接抛弃本轮输入$x_t$，直接复用上一轮的$h_{t-1}$作为本轮$h_t$，而对于额外情况重置门输出为1，而更新门输出为0的时候GRU坍缩成朴素RNN，由此也可以更加明显的看出，GRU其实就是朴素RNN的一个超集，他给予了原本普通的RNN选择的权力，让网络可以有选择的考虑每一个字的重要性，使其最终获取的语义信息是有重点的而不是和朴素RNN一样一锅乱炖

最后，我们重新总结一下更新门和重置门的作用

- 重置门专门服务于本轮的输入值，代表以往内容中有多少内容是本次信息产生输出所需要的
- 而更新门则服务于上一轮输出值，代表本轮的输入值对于以往信息来说有多重要

有了两个门控开关控制隐藏层的输出，整个RNN的架构也变得更加智能了

### 文本情感分析

接下来用一个文本情感分析的案例来整合上面提到的知识，其中词向量直接采用别人训练好的GloVe模型，网络架构就使用刚刚学习的GRU，数据集就用imdb这个比较著名的数据集，刚好，对于这些十分著名的数据集，在kaggle上也有专门的竞赛项目用以测试和提交，于是就可以顺便参加一下传说中的[kaggle的竞赛](https://www.kaggle.com/competitions/word2vec-nlp-tutorial)，也算是纪念第一次成系统的完成一整个网络和流程的代码编写

首先我们可以先了解一下kaggle的提交方式和其提供的数据集，其文件结构大概是这样的

![image-20230113170840175](https://s2.loli.net/2023/01/13/2xoZeuQ3ijBTA7z.png)

其中，`labeledTrainData.tsv.zip`这个文件，他是我们的训练数据，包含了25000条电影评论和标签（积极/消极，用0/1表示），而`testData.tsv.zip`这个文件则是25000条测试数据，只包含电影评论而没有标签，我们需要用训练数据训练一个模型，接着用这个模型在测试数据上预测出所有的标签，和评论的id一起打包成文件`submission.csv`作为提交文件，系统会通过比对`submission.csv`来得出我们的正确率和得分

接着我们看看训练数据的格式

![image-20230113172706782](https://s2.loli.net/2023/01/13/ZtdneT7xbOvcsVl.png)

其中第一列是评论编号，第二列是标签，第三列是评论内容，可以发现一些评论当中包含着html标签，反斜杠等非法符号，在数据处理的过程中应当注意特别处理，接下来我们看看测试数据的格式

![image-20230113185037117](https://s2.loli.net/2023/01/13/kFDebjVq73GN9Lx.png)

可以发现除了缺少标签列之外，和训练数据的内容别无二致，之后我们只需要读入评论内容输出标签并携带数据编号写入到输出文件即可，也就是以下的输出文件的样例

![image-20230113185436783](https://s2.loli.net/2023/01/13/nvCu7UeDI8gb6wj.png)

知道了程序的运行目标，接下来我们就可以开始编写程序了，以下是程序的内容

```python
import os
import torch
from torch import nn
from torch.utils.data import Dataset
import pandas as pd
import re
from torchtext.vocab import GloVe
from collections import Counter
from util import Util
import pickle

# 全局常量
data_path = './datasets/imdb'
model_path = './model/imdb'
train_path = os.path.join(data_path, 'labeledTrainData.tsv')
test_path = os.path.join(data_path, 'testData.tsv')
vocab_path = os.path.join(data_path, 'GloVeVocab.pkl')
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
max_sentense_len = 500
batch_size = 64
h1 = 300
h2 = 200
epoch = 1


# 加载到数据集
class imdbDataset(Dataset):

    def __init__(self) -> None:
        self.data = pd.read_csv(train_path, header=0, delimiter='\t')

    def __getitem__(self, index):
        label = self.data['sentiment'][index]
        content = self.data['review'][index]
        return content, label

    def __len__(self):
        return len(self.data)


def get_input(sentences: list):
    return nn.utils.rnn.pad_sequence([
        torch.stack([vocab[j] for j in tokenizer(i)[:max_sentense_len]])
        for i in sentences
    ]).to(device)


dataset = imdbDataset()


# 词向量生成
class Vocab():

    def __init__(self, data, min_freq=5):
        print('Tokenizing...')
        tokens = sum([tokenizer(i) for i in data], [])
        print('Sorting...')
        self.freq = sorted(Counter(tokens).items(),
                           key=lambda x: x[1],
                           reverse=True)
        print('Building...')
        self.tokens = [
            self.freq[i][0]
            for i in range(len(self.freq)) if self.freq[i][1] > min_freq
        ] + ['<unk>']
        glove = GloVe('6B')
        self.size = glove.dim
        print('Embedding...')
        self.vocab = {i: glove.get_vecs_by_tokens(i) for i in self.tokens}

    def __getitem__(self, index):
        return self.vocab.get(index, self.vocab['<unk>'])


def tokenizer(line):
    _patterns = [
        r"\'", r"\"", r"\.", r"<br \/>", r",", r"\(", r"\)", r"\!", r"\?",
        r"\;", r"\:", r"\s+", r"\\"
    ]
    _replacements = [
        " '  ", "", " . ", " ", " , ", " ( ", " ) ", " ! ", " ? ", " ", " ",
        " ", " "
    ]
    _patterns_dict = list(
        (re.compile(p), r) for p, r in zip(_patterns, _replacements))
    line = line.lower()
    for pattern_re, replaced_str in _patterns_dict:
        line = pattern_re.sub(replaced_str, line)
    return line.split()


if os.path.isfile(vocab_path):
    vocab = pickle.load(open(vocab_path, 'rb'))
else:
    print('Local vocabulary not found. Building... This process may be slow')
    vocab = Vocab(dataset.data['review'])
    print('Success!')
    pickle.dump(vocab, open(vocab_path, 'wb'))
print('Vocabulary is ready!')


# GRU实现
class GRU(nn.Module):

    def __init__(self, input_size, output_size):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size

        def init_para():
            return (
                nn.Parameter(
                    torch.randn(
                        (self.input_size, self.output_size), device=device) *
                    0.01),
                nn.Parameter(
                    torch.randn(
                        (self.output_size, self.output_size), device=device) *
                    0.01),
                nn.Parameter(torch.zeros(self.output_size, device=device)))

        self.wx_z, self.wh_z, self.b_z = init_para()
        self.wx_r, self.wh_r, self.b_r = init_para()
        self.wx_h, self.wh_h, self.b_h = init_para()

    def forward(self, X):
        H = torch.zeros(self.output_size, device=device, requires_grad=True)
        batch_output = []
        for x in X:
            # z:更新门,r:重置门,h:候选隐状态
            z = torch.sigmoid((x @ self.wx_z) + (H @ self.wh_z) + self.b_z)
            r = torch.sigmoid((x @ self.wx_r) + (H @ self.wh_r) + self.b_r)
            h = torch.tanh((x @ self.wx_h) + ((r * H) @ self.wh_h) + self.b_h)
            H = z * H + (1 - z) * h
            batch_output.append(H)
        return torch.stack(batch_output).requires_grad_()


# 网络实现
class IMDB(nn.Module):

    def __init__(self, input_size):
        super().__init__()
        self.rnn_layer = nn.Sequential(GRU(input_size, h1), nn.ReLU())
        self.output_layer = nn.Sequential(nn.Dropout(0.2), nn.Linear(h1, h2),
                                          nn.ReLU(), nn.Dropout(0.3),
                                          nn.Linear(h2, 2))

    def forward(self, inputs):
        x = self.rnn_layer(inputs)
        return self.output_layer(x[-1])


# 训练
util = Util(model_path, '')
util.train(epoch, IMDB(vocab.size).to(device), dataset, batch_size, get_input)
# 测试和提交
model, _ = util.load(IMDB(vocab.size))
submission_data = pd.read_csv(test_path, sep='\t')
raw_data = list(submission_data['review'])
predict = torch.cat([
    torch.argmax(model(get_input(raw_data[i * 100:(i + 1) * 100])), -1)
    for i in range(len(raw_data) // 100)
])
submission_data['sentiment'] = predict.cpu()
submission_data.to_csv(os.path.join(model_path, 'submission.csv'),
                       index=False,
                       columns=['id', 'sentiment'])
```

其中由于大多数神经网络的训练代码都相差不大，因此我单独写了一个Util类用以专门处理网络的训练过程，代码如下

```python
import os
from typing import Callable
from torch import load, nn, optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch


class Util():

    def __init__(self, dir: str, filename: str):
        if not os.path.exists(dir):
            os.mkdir(dir)
        path = os.path.join(dir, filename) + '{}.para'
        self.model_path = path.format('model')
        self.optim_path = path.format('optim')
        self.device = torch.device(
            'cuda:0' if torch.cuda.is_available() else 'cpu')

    def load(self, model: nn.Module):
        optimizer = optim.Adam(model.parameters())
        if os.path.exists(self.model_path):
            model.load_state_dict(load(self.model_path))
        if os.path.exists(self.optim_path):
            optimizer.load_state_dict(load(self.optim_path))
        return model.to(self.device), optimizer

    def train(self,
              epoch: int,
              model: nn.Module,
              dataset: Dataset,
              batch_size: int,
              f: Callable = lambda x: x):
        model, optimizer = self.load(model)
        model.train()
        loss = nn.CrossEntropyLoss()
        for e in range(epoch):
            ave_los = []
            ave_acc = []
            dataloader = DataLoader(dataset, batch_size, True)
            for i, (x, y) in enumerate(dataloader):
                x = f(x).to(self.device)
                y = y.to(self.device)
                optimizer.zero_grad()
                y_pre = model(x)
                lo = loss(y_pre, y)
                lo.backward()
                optimizer.step()
                acc = torch.argmax(y_pre, -1).eq(y.data).float().mean()
                ave_los.append(lo.item())
                ave_acc.append(acc.data.cpu())
                print(
                    f'train:{i+1}/{len(dataloader)},loss:{lo.data},acc:{acc.data}\r',
                    end='')
                if i % 100 == 0:
                    torch.save(model.state_dict(), self.model_path)
                    torch.save(optimizer.state_dict(), self.optim_path)
            print(
                f'\nepoch:{e+1},average loss:{np.mean(ave_los)},acc:{np.mean(ave_acc)}'
            )

    def test(self,
             model: nn.Module,
             dataloader: DataLoader,
             f: Callable = lambda x: x):
        ave_los = []
        ave_acc = []
        loss = nn.CrossEntropyLoss()
        model.eval()
        for x, y in dataloader:
            with torch.no_grad():
                y_pre = model(f(x))
                ave_los.append(loss(y_pre, y).cpu())
                ave_acc.append(
                    torch.argmax(y_pre, -1).eq(y).float().mean().cpu())
        print(f'test average loss:{np.mean(ave_los)},acc:{np.mean(ave_acc)}')
```

注意到当将代码移植到kaggle上时应该注意需要更改全局常量中的各path值，在kaggle中，input目录只允许我们上传数据文件而不能在程序中写入到这个目录中，相反，output目录不允许我们从本机上传文件到其中，而允许程序运行过程中对其进行更改和写入（我们的提交文件也应该被提交至output目录）

### Weight Decay

在之前我们曾经提到过，在深度学习的过程中，一个非常严重的问题就是过拟合，同时我们也提出了两个有效的降低过拟合风险的方法，就是增加训练样本数量和减少网络复杂度，在卷积网络中，我们对图片进行预处理（放大、旋转、灰度等）有效的提高了训练样本的数量，而使用卷积核来减少网络的复杂度，但是面对文本信息时，我们发现这些方法似乎并没有办法复用，文本的翻转和改变都在一定程度上会改变语义的理解，而RNN从网络架构的角度天生就比卷积网络要复杂，我们必须用一些其他的方法来降低网络的过拟合风险

你如果仔细观察上面的网络结构，就会发现和当时学习卷积网络时发现的BN层类似，你在其中也发现了一个不认识的叫做Dropout的东西，而这个东西，就是循环神经网络能够降低过拟合风险的关键

不过在了解Dropout之前，我们先要学习一下正则化这个知识，而在这个领域最传统的方法，就不得不提到weight decay（又称权重衰减或L2正则化）了

在[前一节](#过拟合)中，我们知道了过拟合最直观的理解就是在大量参数的情况下最终所得到的曲线过度区分了不同的分类点，而如果我们在此基础上继续挖掘下去，似乎造成这样问题的关键是这条曲线扭曲程度过高了，要是它能够平滑一些，比如在参数不变的情况下，如果能够把下图的第三种曲线用什么办法改成接近于第二种这样的曲线，也不至于会出现过拟合的问题了

![image-20221024203438615](https://s2.loli.net/2022/10/24/kHzRVfXEQ1PbID3.png)

于是就有人开始思考，究竟是什么导致了曲线的扭曲，没错，就是斜率，你可能说这不对呀，如果曲线的斜率是恒定的，我仍然会觉得这条曲线是平滑的吧，比如在第二张图中，我觉得随着$x$的增大曲线斜率不是挺大的吗，但是实际上他的确看上去是平滑的，这么一来，是不是斜率的变化率，也就是曲线二阶导才是控制曲线扭曲程度的关键呢？

其实并不是如此，我们都知道，对于神经网络来说，对于前后两层网络的两个连接的神经元来说，在不考虑激活函数的前提下，他们都是线性相关的，而在之前我们也[曾经](#非线性拟合)详细推理过了在多层网络当中是如何将线性相关的神经元在网络的叠加过程中扭曲化的，而对于ReLU控制下的多层拟合函数，我们会发现在某段范围内直线的斜率都是恒定的，正是多条斜率恒定的函数的叠加控制了复杂化的拟合结果

![image-20230113213115596](https://s2.loli.net/2023/01/13/ACRTsdeSH6Zr314.png)

于是我们终于可以揭示在这张图中我为什么取的$w$值都是1的原因了，你会发现，如果$w$的值取的过大，尤其是对于符号相反的两条函数来说，最后结合后的拟合函数的扭曲程度也会显而易见的大，一旦网络结构愈加复杂，拟合结果的图像也会如上面第三张图一般混乱，即产生了过拟合的结果

而我们经过分析也明白了，为了减小过拟合风险，我们需要让损失函数求导获取的下一轮$w$不至于太大，这将如何做到呢，这就不得不提传说中的惩罚项了，而损失函数的作用本身就是用来评判函数的好坏的，现在我们在原有的损失函数的基础上新加一惩罚项$w^2$，新损失函数为$L'(f)=L(f)+\lambda w^2$（其中$\lambda$是我们自己定的超参数）

从最直观的角度来看，我们将$w^2$作为了损失函数的一项，由于我们的目的是尽可能最小化这个损失函数，显然的，如果$w$的值过大，我们损失函数的值也会跟着增大，这样就算$L(f)$达到了最小值，由于$w^2$项的加入，如果$w$此时比较大，那么$L'(f)$其实并未达到最小值，使得梯度下降的过程并不会就此停止，最终，梯度下降的结果会使得$w$达到一个相对权衡的值

这种**通过加入平方项的方式限制权重不至于过大的方法就是权重衰减，而控制模型尽量避免过拟合的方法就是正则化**

你可能还有一些疑问，比如为什么$w$要取到平方项而不是三次四次甚至是指数，毕竟我们已经知道惩罚项就是当$w$大的时候能够使损失变大的，既然如此难道不是通过控制$w$的次数来控制惩罚的力度，而在上面的公式中我们也看到了$\lambda$项并不在$w$的指数而是系数上，这又是什么原因呢

其实这个问题并不复杂，我们在研究神经网络的结构的时候就曾经提到过在选择激活函数的时候应该满足的[条件](#激活函数)，那时候我们就明确强调了，任何对网络结构的改变都不能违背两条铁律，他们是单调性和连续性，而这样约束的目的就是为了使损失函数的最小值唯一，然而我们如果提高了衰减项的次数，损失函数将有不唯一的最小值甚至不存在最小值，这不让我们前面好不容易约束的结果直接成为一纸空谈了吗

### Dropout

说完了weight decay究竟是怎么一回事，接下来才是我们的重头戏，我们说过，在没有办法增加训练样本的情况下，我们只能硬着头皮从优化参数入手解决过拟合的问题，对于权重衰减来说，他从网络的数学结构入手，通过给网络参数的变化加上人为的限制来减少过拟合的风险，而Dropout（暂退法）则显得更为暴力，既然你说我网络结构太复杂了，那我就把网络的复杂程度降降不就行了吗

当然，这个降可不是直接把网络砍掉多少那么简单，那可不配用这么高大上的名字，但是其实也大差不差，我们都知道，训练过程中，我们把数据集分成不同的批次，然后各个批次先后进行训练，Dropout的思想就是在每个批次的训练过程中，我们会随机的抽取一部分神经元让他们失效，仅通过剩余神经元的相互作用产生输出，并仅在剩余神经元的参数上进行梯度下降

至于如何让神经元失效，这可太简单了，由于我们可以获取每一层的输出，因此我们只需要在输出基础上随机令其中的几个神经元的输出为0，由于下一层的前向传播的计算方式为$\sigma(\sum_{i=1}^n w_ix_i+b)$，由于我们令不少神经元的输出为0，因此在这一层计算的过程中会存在不少$x_i$等于0的情况，此时无论是前向传播还是反向传播，其系数$w_i$既不会作用于结果也不会进行更新，也就等于前一层的该神经元无效了

同时不止如此，在令神经元输出置零之后，我们还需要让未置零的输出变得大一些，保证单层输出的结果总的期望值不变，使得梯度下降的结果不至于因为置零之后产生偏向，既某层输出$h$经过Dropout之后，输出$h'$如下
$$
h'=
\begin{cases}
0  & \text{概率为p} \\
\frac{h}{1-p}   & 其他
\end{cases}
$$
我们会发现Dropout从实现的方法和理由来说比起权重衰减都更易理解，更可气的是，在实际测试的过程中，Dropout的效果相较于权重衰减还要更好，意味着大多数情况下我们更愿意使用权重衰减来减少过拟合的风险，同时需要注意的是这两个方法并不建议一起使用，有论文已经指出两个方法的不兼容性，同时使用反而会起反效果

另外以上我们都是在训练环境下的操作，对于测试模型来说，我们同样要令Dropout失效来追求更高的正确率，这也是为什么在上面的代码的测试之前我使用了`model.eval()`将模型调为测试模式，在pytorch中经过了`eval`的声明，程序就会自动使模型中已存在的Dropout层失效

### Encoder&Decoder

至此其实循环神经网络的主要内容就告一段落了，毕竟已经算是一个过时了的网络结构，我们也不深究一些更为复杂的内容，点到即止，但是在进入下一章之前，我们仍然要借助RNN的结构引入一些相对重要的知识内容，使得在下一章的学习中能够更快的理解新内容

在之前，我们使用了RNN结构完成了文本情感分析的功能，但是在NLP（自然语言识别）领域，文本情感分析仅是其中的一个小小部分，语言模型有着各种各样的应用，比如现在我们需要实现一个文本翻译功能，我们试试用传统的RNN网络是否可以实现

我们首先明确，在这个应用中，我们的输入是一种语言的句子，输出是另一种语言的句子，我们知道，在RNN中，我们输入一个单词，模型就会输出一个one\-hot结果，在文本情感分析过程中，我们只取最后一位输出的结果用以判断情感类型，而如果你了解过文本续写任务的话，在那个任务中，我们同样输入一个单词，然而输出的是另一个单词，之后我们使用输出的单词作为下一个输入，再产生下一个单词，如此循环往复形成了最终的续写句子

但是在翻译任务中，我们遇到了问题，就是输入和输出的句子单词长度并不一致，而且不同语言之间的语法和结构也大不相同，比如某些倒装句在翻译的过程中我们就需要将其倒转回来，如果我们同样是输入一种语言的一个单词，然后输出另一种语言的一个单词，无论网络结构多么精妙，最后的结果仍然是逐词翻译，不仅无法保留翻译语言结果的语法信息，而且输出结果永远和输入内容长度相同，这种程度的翻译结果甚至还不如自己买本词典，神经网络优越性完全没有体现出来

于是研究人员引入了Encoder\-Decoder的模型结构，在这个模型中存在两个输入通道，前一个输入内容为一个句子，在经过一个网络结构之后输出为一条浓缩后的语义信息，你可以把他理解为RNN的最后一层输出结果，这一块网络被称为Encoder（编码器），而另一个输入内容则老老实实运行文本续写的网络，只不过在输入单词之前，我们要将这个单词的词向量结果连接上我们编码器输出的结果作为新的词向量，这一块被称为Decoder（解码器），之后，我们继续将解码器的输出结果拼接上同样的编码器输出结果作为新的解码器输入

![../_images/seq2seq-details.svg](https://zh.d2l.ai/_images/seq2seq-details.svg)

Encoder\-Decoder的结构十分适合处理输入输出不等长的文本信息，最著名的应用就是文本翻译的任务，当然，广义上的Encoder\-Decoder结构指一切多层网络结构，我们都可以将其中的几层当作Encoder层，几层当作Decoder层，在学习卷积网络的过程中我也曾经[提到过](#再谈激活函数)在卷积层当中前几层负责抽取局部特征，后几层负责将特征进行非线性映射提高区分度，其实Encoder\-Decoder正是这个思想的具体体现，在简单的文本处理任务中，我们同样可以将Embedding层称为网络的Encoder，后面的RNN网络作为Decoder理解，只不过对于这些广义的Encoder\-Decoder模型来说，其编码器的输出结果直接作为解码器的输入而已

### seq2seq

对于Encoder\-Decoder模型来说，运用其思想的开山鼻祖正是seq2seq，他是早期翻译任务的扛把子架构，也为后来的Transformer结构铺平了道路，对于seq2seq来说，他的输入是一个序列内容（往往是一个句子），输出是另一个序列内容（往往也是一个句子），且输出的长度仅受专门的句子结束符号控制，由于解码器的输出作为自己的输入，因此网络可以自己决定何时停止输出（在下面的例子中`<eos>`符号表示句子中止）

![../_images/seq2seq.svg](https://zh.d2l.ai/_images/seq2seq.svg)

接下来我们来实现一下seq2seq，定义一下所谓的Encoder\-Decoder结构，也为了可以与下一章的内容进行比较

```python
from torch import Tensor, nn, tensor, cat, float32, squeeze


class Encoder(nn.Module):

    def __init__(self, input_size, output_size):
        super().__init__()
        self.layer = nn.GRU(input_size, output_size)

    def forward(self, X):
        return self.layer(X)


class Decoder(nn.Module):

    def __init__(self, input_size, hidden_size, vocab_size):
        super().__init__()
        self.rnn = nn.GRU(input_size + hidden_size, hidden_size)
        self.dense = nn.Linear(hidden_size, vocab_size)

    def forward(self, X: Tensor, encoder_output: Tensor):
        state = encoder_output.repeat(X.shape[0], 1, 1)
        # 拼接编码器输出和解码器输入
        input = cat((state, X), 2)
        _, output = self.rnn(input)
        return self.dense(squeeze(output))


class Seq2Seq(nn.Module):

    def __init__(self, embed_size, encoder_output_size, vocab_size):
        super().__init__()
        self.encoder = Encoder(embed_size, encoder_output_size)
        self.decoder = Decoder(embed_size, encoder_output_size, vocab_size)

    def forward(self, X_o, X_p):
        output = self.encoder(X_o)[1][-1]
        return self.decoder(X_p, output)


# 第一个是词向量长度，第二个是编码器输出长度，第三个是所有词的onehot长度，输出可通过交叉熵计算损失
s2s = Seq2Seq(2, 2, 3)
print(
    s2s(tensor([[[1, 2], [5, 6]], [[3, 4], [7, 8]]], dtype=float32),
        tensor([[[1, 3], [5, 7]], [[2, 4], [6, 8]]], dtype=float32)))
```

## 自注意力网络（Transformer）

到现在为止，我们一步步了解了两种非常传统的神经网络，他们大多都是上个时代的产物，也正在逐渐变成时代的眼泪，但是在这一章中我们需要介绍的，是目前为止最为火热的网络架构，比起前两种具有特殊功能的神经网络，这种网络的泛用性更强，效果还更好，于是被广泛运用在了几乎所有可以以深度学习完成的任务上，可见其效果之好，它就是大名鼎鼎的Transformer

而从这一章开始，将会把编写的重点放在网络的结构和实现上，而对网络的原理一笔带过甚至不提及，因为目前为止深度学习已经逐渐变成了一个玄学炼丹的过程，越来越多的网络结构能够有良好的效果靠的全是各种研究人员不断调参最后试出来的结果，至于背后为什么会起作用甚至连提出这个方法的论文中也没有解释，深度学习的现状就是实践超前于理论太多，对于一些原理性的问题我也不在此处班门弄斧了

同时在接下来几章中并不会考虑手动实现模型并以某个任务为要求实现正式网络的训练，一方面是接下来需要学习的网络往往比较大，可学习的参数也是爆炸性增长，不仅如此这种大型网络模型需要的训练样本量也非常巨大，动辄就是亿为级别的，以我们的垃圾电脑不仅运算能力不够存储能力也不够，训练模型的难度非常大，另一方面是大部分网络结构在pytorch中都有十分易使用的实现类，和ResNet一样，真的大型的网络模型也有可直接使用的模型可供下载，自己重新造轮子重新训练并不是一个好主意，最重要的是在前几章的学习中如果你跟着代码编写的思路走下来，其实pytorch的内容也已经会的七七八八了，我们要时刻牢记pytorch只是实现的工具而不是深度学习本身，对于深度学习来说，有工具的辅助下掌握了理论之后的实现其实都并不困难，把学习的中心放在理论的理解上才是有效率的学习方式

### RNN的缺陷

在学习注意力网络之前，我们还需要回顾一下RNN这个看似完美的网络究竟存在什么问题，促使着研究人员去研究不一样的网络结构妄图替代他

1. 我们曾经也提到过，由于梯度爆炸的存在，RNN对于长句的处理效果非常一般，即使我们利用各种方法试图缓解梯度爆炸的效果，但是随着读入序列的不断增长，网络的更新效果也会越来越差，同时由于递归参数的存在，不只是效果，每一次反向传播的计算开销也将不断增大，尤其是面对如句子续写一类的任务时，随着句子的增长网络将越来越不稳定，最终生成的结果错误率也将不断上升
2. 我们曾经也说过，为了缓解梯度爆炸问题，RNN也有不一样的优化架构比如GRU，但是GRU能够缓解问题的关键是引入了一个门的概念，有得必有失，虽然更新门和重置门的加入让RNN有了一定读入较长文本的能力，但是却弱化了其对句子中较远部分的语义理解能力，对于任一过程读入的单词来说，距离其越远的单词越不容易享受到其提供的语义信息，因为他为了把自己的信息传给后面的单词，每一轮的传播过程都得保证重置门不重置，更新门不更新，这样的条件是苛刻的，因此就算我们给GRU读入了长句，其最后一个单词的输出结果从直观上理解大概率也只考虑了末尾若干单词的语义信息，过早读入的单词遗忘的概率非常高
3. 与此同时，RNN的效率也是极低的，对于一个句子来说，由于RNN的顺序性，所以虽然我们提交给RNN的也是一个句子，但是在这个句子却在网络中被拆分成了一个个单词逐个处理，其中一个单词被读入网络的过程中，我们必须等待其产生输出才能输入下一个单词，这其实极大的浪费了GPU资源，这也是为什么对于RNN网络来说，即使是用CPU跑似乎也不会比GPU慢到哪里去，相较于卷积网络每一张图片和其中任何两个像素的独立性，给程序的效率优化提供了极大的空间

通过几个问题的描述，我们其实可以总结出RNN问题的根源，其实就是它过度以单词及其关系为主体而缺少把整个句子看作是一个完整元素的能力，我们希望网络可以像CNN一样同时对一整个句子进行处理，并且保证其中存在相关联的单词能够明确起作用，即句子长度只作为句子的一个无关属性存在，而不严重影响网络的传播效率

### Self-Attention

假如你是一个厨师，此时你接到了一个订单，打开发现是一幅意义不明的图画，你一边骂着顾客为什么会发给你这么抽象的订单，一边开始仔细的分析起这幅画来，终于，你大概认出来了这玩意可能是一盘鱼香肉丝（~~为什么是鱼香肉丝，因为我想吃了~~），同时，你的脑海中浮现出了鱼香肉丝的食谱：肉丝，胡萝卜丝，木耳等等

你对照着脑海中的食谱，来到冰箱里开始挑选食材，首先你看到了一个袋子上大大的写着肉丝的标签，于是你将袋子拿出，根据今天的菜谱所有菜的分量分配打算称350g肉丝出来，之后循环这个步骤，获得了这份菜所有的食材包括其分量，接着你把这些食材加到一起，炒一炒，一盘新鲜的鱼香肉丝就出锅了

通过上面这个情景，其实你已经把自注意力机制会的七七八八了，其实我们也会发现，深度学习这一块，虽然高大上的名字是层出不穷，但是其实所谓的新东西的理论理解起来并不困难，好用的方法往往都没那么多数学的弯弯绕绕，反而是那些洋洋洒洒公式一大堆，推导过程让人晕头转向的方法，理解了半天结果却发现效果并没有这些简单的方法来的好（没错鞭尸的就是word2vec和RNN）

当然为了让自己的方法显得高大上一点，我们还是需要把这个情景转换成数学语言，首先我们先定义一些符号，我们令顾客发来的这个图片为$x_1$，我经过分析，脑海中浮现出的菜谱为$q_1$，接着，我们拿着这个菜谱来到冰箱，此时冰箱里各种各样的袋子上写的各种标签分别是$k_2...k_m$，其中$m$表示冰箱里菜的总数，袋子里面的东西是$v_2...v_m$，对于我们取材烧菜的操作，用数学的语言来描述一下，就是我们拿着$q_1$，逐个比对$k$看看$k$是否出现在菜谱中，并根据食材和菜谱结合获得了这个食材所需要的分量，如果把这个过程描述成一个函数，假设$k_2$是肉丝这个标签，那么就可以写作$\alpha (q_1,k_2)=150\text{g}$，然后我们从袋子里面取出了所需要的食材实物$v_2$，然后，我们把取出的这一堆食材加在一起，获得了最终的输出鱼香肉丝这道菜，如果把输出看作$f$，那么整个过程可以描述成一个函数写作
$$
f(x_j)=\sum_{i=1}^{m}\alpha(q_j,k_i)v_i
$$
这个函数正是大名鼎鼎的注意力汇聚函数，其中的$\alpha$可以叫做注意力评分函数，这个读入输入获取输出的过程也就可以被称为Self-Attention（自注意力），对于我们的这个情景来说，他的输入是一个照片和各种各样的食材，输出是一道真真实实的菜，而在NLP中，输入是一个句子，其中的$x$表示其中的单词，对于任意一个单词$x_j$的输出$f(x_j)$，我们要先根据$x_j$产生一个他的$q_j$，接着用这个$q_j$分别与所有的单词（*包括自己*）$x_i$产生的$k_i$做一些操作$\alpha$之后产生了一个类似分量的注意力权重，表明之后的$v_i$我要取多少，然后和同样是该$x_i$产生的$v_i$相乘，再把这些所有的相乘结果加起来作为$x_j$的输出，整个过程用图像来描述如下（其中$a$和$b$分别是上面的$x$和$f$）

![image-20230123223453868](https://s2.loli.net/2023/01/23/Y9itrkNwP8dh3Il.png)

这个结果和我们的情景唯一不同的地方就是其中的$q$会和自己的$k$也做一次$\alpha$操作，这其实也是为了程序编写的方便，实践证明不会对网络的精度产生影响

从这个网络结构中我们会发现之前我们提到的RNN的问题好像都被完美的解决了，首先就是网络不再会被句子的长度所束缚，关你是多长的句子，我反正一个个遍历你们的$k$，而我是否取你只取决于你产生的$k$符不符合我的要求，并且由于已经产生的$q,k,v$​​，因此所有的单词进行的操作都是互不干扰的，再也不用强制顺序遍历每一个单词了，他在砍掉了递归参数这个弊端的同时保留了加权获取词与词关系的途径，不可谓不妙

了解了网络的大体结构之后，我们就会开始思考一些细节问题了，对于我当时学习的过程来说，最令我纳闷的无非是两个问题，第一个是这个$x$是怎么变成$q,k,v$的，以及汇聚函数中的$\alpha$究竟是什么东西

对于第一个问题来说其实是比较简单的，在深度学习中有一句老话，叫做遇事不决就用FC，没错，实际情况中这里就是一个全连接层而已，也就是说，当我们输入一个句子，第一步没什么区别还是词元化和Embedding，把一个句子变成一堆词向量之后，下一步我们会先把每一个词向量$x_i$都扔到三个全连接层输出$q_i,k_i,v_i$，只是需要注意的是由于我们并不知道一个句子里有几个单词，因此每一个词向量经过的全连接网络都是一样的，所有的词共同维护三个全连接层，三个全连接层的输入维度都是词向量的长度，输出维度可以看你喜好，但根据之后提到的$\alpha$选择的不同，有时候$q$和$k$需要强制输出维度相同

### 缩放点积评分

对于第二个问题来说，我们首先需要搞清楚这个$\alpha$的作用是什么，对于我们提出的这个情景来说，其实$\alpha$就是我们需要取得的食材分量，从名字上来说我们也可以很容易意识到他其实是给另一个参量$v$评分用的，由$\alpha$的输出控制我们要取得的$v$的多少，而控制的手段则是通过输入进的$q$和$k$共同作用决定，需要注意的是，既然说的都是评分了，分数一就是一，二就是二，但是可不会是一个向量，因此无论你用什么函数，必须保证这个$\alpha$函数的输入是两个向量，而输出一定是一个值

那么我们如何通过$q,k$来评价$v$的好坏的，一个直觉的想法就是比较$q,k$的相似度$q,k$之间的相似度越高，我就给$v$越大的分数，而说到相似度的评价，我们在学习word2vec的时候也曾经[提到过](#Skip-Gram)，两个向量的点乘（内积）就是一个不错的主意，于是便有了最简单的一种评分函数，点积评分模型

由于自注意力模型所有词向量的$q,k,v$都是同时生成的，因此对于某个词的$q$，我们会将其和所有的$k$做点积$q\cdot k_i$，由于我们知道，点积的输出天然就是一个值而不是一个向量，正好什么都不需要做了，直接将这个结果作为$v$的系数就好了，于是点积评分模型的汇聚函数就为
$$
f(x_j)=\sum_{i=1}^{m}(\frac{q_j\cdot k_i}{\sqrt{d_k}})v_i
$$
其中$d_k$其实是$q$或者$k$的长度，由于点积要求左右两侧的向量必须等长，在构建网络的时候应该注意全连接层的$q,k$两层输出维度相同，至于为什么要除$d_k$，说深奥一点就是控制点积后方差为1，说通俗一点就是为了控制点乘后的值范围，使其在接下来的前向和反向传播过程中数值更稳定，减小出现诸如梯度爆炸和消失的问题，这也是这个评分方法中缩放一词的含义

我们刚刚说过，越简单的方法并不代表他的效果越差，对于这个注意力评分函数来说，同样也并没有比其他方法弱到哪里去，在大部分场景下，我们使用点积作为我们自己网络的评分函数完全够用了

### 并行计算

不过我们其实会发现一个小问题，就是我们一直使用注意力汇聚函数，但是这个函数只能算出来一个词向量的输出而已，况且这个函数里面还有一个连加操作，在正常情况下我们都会面对的是一个句子，到时候进行代码实现的时候难不成要搞一个二重循环吗

其实并不然，提出注意力汇聚函数的目的还是为了便于理解，实际上我们可以发现我们完全可以把所有的$q,k,v$向量分别拼在一起变成$Q,K,V$，如果不考虑batch的影响，正常$q,k,v$应该是一个一维向量，假设长度为$n$，如果句子长度为$m$，那么我们把$m$个$q,k,v$拼在一起$Q,K,V$的大小就是$m\times n$，此时根据矩阵的乘法特性，其实在使用点积评分的时候完整的注意力传播函数长这样
$$
\text{Attention} = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$
这个softmax进一步控制了评分的范围，不过其实实践证明只要是激活函数放在这个位置效果都不算差，因此这个softmax更重要的作用应该还是增加模型的非线性关系而已，$QK^T$其实就是把每一条$q_j$和所有$k_i$做点乘之后再转置（因为$A\cdot B=A^TB,(A^TB)^T=AB^T$），这在之后会详细说明

由于可以发现这个函数和之前我们提出的注意力汇聚函数还是挺不一样的，因此在接下来的内容中，这一个特例化的注意力汇聚函数一律称为Attention模型或Attention块，意味着其有着可以直接加入神经网络的能力了（类比于单个卷积核操作和卷积层的概念），并且由于其内部有不止一层网络连接，因此不好作为一层网络看待，块结构能更直观描述其传播的结构特性

为了加深对于模型实际运行过程中的概念，接下来将会从词向量开始从张量维度变化的过程来更加直观的理解一遍这个模型的传播过程，从中我们也可以进一步看出注意力模型在并行计算上的优越性

假设我们有一个句子"I love you"，它由三个词组成，于是我用我的词向量模型将每个词都变成了长为10的向量，此时我的句子就可以表示为一个$3\times10$的句子张量$X$，如果默认第一维为行向量，则这个张量每一行代表一个单词，每一列是每个单词在这一维的分量，接着我们假设我们希望的$q,k$长度为5，$v$长度为8，则我们将初始化参数矩阵$W_q,W_k,W_v$，他们的大小是$10\times 5,10\times 5,10\times 8$，然后我们让$X$分别乘$W_q,W_k,W_v$模拟全连接传播，前两个结果大小为$3\times 5$，分别是$Q$和$K$，而$V$结果大小为$3\times 8$，这时我们就产生了$Q,K,V$，他们每一行就是每一个单词的$q,k,v$，接着我们让$Q^T$和$K$相乘，得到大小是$3\times 3$的张量，这个张量的每一行代表一个单词的$q$和$k$点乘的结果，由于每一个$q$都要和三个$k$点乘，因此列维度也是3，得到的结果全部除$\sqrt 5$之后以行为单位做softmax得到$\alpha$，使每一个$q$和三个$k$点乘结果在处理后加在一起为1，之后由于评分函数的输出和$V$其实也是一个点乘关系，因此我们将$\alpha^T$和$V$相乘得到最终大小为$3\times 8$的张量，其中每一行正是每一个词向量的最终输出结果

可以发现，比起RNN实现过程中必须的循环结构，在Attention模型中所有的循环结构都可以用矩阵乘法代替，更重要的是，无论句子有多长，对于使用缩放点积评分函数来说，进入了Attention块之后都可以在五个矩阵乘法之内搞定输出，这既给我们编写程序带来方便，又可以更加充分使用GPU，提高传播效率

如果这还看不懂，那么你就记住对于任何一个Attention块来说，他的输入是多少个词向量，输出仍然是多少个向量即可，对于输出的向量的长度则是你自己定的

> 虽然至此位置我们都在以词向量也就是NLP任务作为范例，但实际上并没有谁规定Attention块只能用来处理NLP任务，说到底他只是提供了一种多向量输入和多向量输出的方式而已，而正是这个泛化的模型可用于做各种各样类型的任务才显出Transformer的强大，下一节就会分析Attention块在用来处理NLP任务中需要特化的内容

### 位置编码

对于某个句子来说，句子中单词的位置信息势必是十分重要的，比如“算盘”和“盘算”传达的就是完全不同的意思，甚至连词性都变了，然而如果我们使用Attention块来处理这两个词语，我们会发现他们输出的两个向量虽然先后顺序不一样，但是向量的内容一定是一样的，因为注意力机制只关心两个词语之间的关联程度，却并不关心他们之间的顺序，这也是Attention块在处理NLP任务中十分致命的问题

于是便提出了位置编码的方法，句子在进入网络之前众所周知是一堆排列好的词向量，此时我们只需要在每个词向量上都加一个额外的向量，而这个额外的向量只跟词语位置相关而和词向量究竟长什么样无关，两个向量相加之后结果作为新的词向量输入进Attention块，即可保证在网络的传播过程中考虑词与词的位置信息

现在的问题就是如何设置位置向量的编码规则了，首先我们应该想到这个位置向量并不能和句子的长度绑定的太死，毕竟每一个句子的长短都是不可控的，如果我们只编码了十个向量，输入的句子有十一个词就不好办了，同时还应该注意这个向量的长度也不应该定死，因为我们不知道原始词向量的长度是多少，于是我们便可以分析出这个编码规则应该是一个输入为词向量长度和词位置，输出为和词向量等长长度的位置向量

一个传统的编码方式是`sin/cos`编码，已知每一个词向量的长度为$d$，对于一个句子中的第$i$个词向量中的第$j$维，他的值有以下公式
$$
p=
\begin{cases}
\text{sin}(\frac{i}{10000^{j/d}})  & j为偶数 \\
\text{cos}(\frac{i}{10000^{j-1/d}})   & j为奇数
\end{cases}
$$
单看公式其实并不是特别容易理解，但如果我们换个角度，假设我们现在拿到了一个词向量长度为4的句子，然后我们想知道第十个词向量的位置向量是什么，那么接下来我们可以在一个坐标系下画出两条正弦和两条余弦函数，假设画出来长这样

![../_images/output_self-attention-and-positional-encoding_d76d5a_52_0.svg](https://zh.d2l.ai/_images/output_self-attention-and-positional-encoding_d76d5a_52_0.svg)

然后我们画出$x=10$这条线和所有函数线的交点，这些交点的排列向量就是这个词向量的位置向量

至于为什么用这么复杂的位置编码方式，最主要的原因还是这种编码方式允许模型学习相对位置的信息，即对任何两个位置向量来说都可以通过线性投影和平移相互转换（其实就是二倍角公式），不过此处不详细说明，事实上也并不是只有这一种位置编码的方式，我们也发现了这种编码方式其实并不够优雅（指有数学公式），对于力大飞砖的深度学习来说，我们完全可以什么都不管额外训练一个用于位置编码的网络来处理位置信息，事实证明效果也不错，不过此处也不做说明

### Multi-Head Attention

Attention到此其实已经趋近于大成了，而距离Transformer的最后一块拼图就是所谓的Multi-Head Attention，这玩意是干什么的呢，其实就是把原本的Attention块中的$q,k,v$通过不同的全连接网络进一步衍生出$q_i,k_i,v_i$，接着每一组$q_i,k_i,v_i$自己做完Attention运算后再拼在一起，用一张图可以非常直观的描述

![../_images/multi-head-attention.svg](https://d2l.ai/_images/multi-head-attention.svg)

而这就是Multi-Head Attention的全部内容，但是我相信在刚理解这个步骤之后你一定会和我一样十分奇怪，原本的Attention块不是挺好的吗，莫名其妙搞这么一出是想干什么，一个最明显的问题就是加了那么多连接层难道不怕过拟合吗，另外还要个问题就是既然需要多个$q,k,v$那为什么不从词向量开始就生成多个$q,k,v$，像图中这样在原有$q,k,v$基础上又分出来多个$q,k,v$岂不是多此一举

这还得说到这个模型的中文名字，我一开始一直没有提这个Multi-head到底应该怎么翻的问题，从直翻角度来说应该翻作多头注意力，但是从原理上来说我更愿意翻作分头注意力，为什么呢这就得从处理$q,k,v$的全连接的输出长度说起了，你可能会说全连接的输出不是随便定的吗，正常来说是这样，但是在这里的全连接层实际上起到的是一个分块的作用，比如我现在有一个长度为三百的$q,k,v$，我把它丢到两个输入为300输出为150的全连接层输出$q_1,k_1,v_1$和$q_2,k_2,v_2$，这就是分块，至于为什么要分块，这不是把本来就运算复杂的网络搞的更复杂了吗，的确是，它却带来了更好的模型泛化能力

说到这，你是不是想到了什么，没错，这种分块的思想好像和多个小卷积核处理整张图片的过程有异曲同工之妙，事实也的确如此，在经过了分块之后，每一块单独学习，就像是用小的卷积核提取一小部分的局部性信息一样，这种Multi-Head Attention网络明显更容易学习到混在一起的Attention不容易提取的局部特征。或者用学术一点的话来说，Multi-Head的结构允许每个注意力头只关注最终输出序列中一个子空间，使模型能够在不同的表示子空间里学习到相关的信息并在最后无损失的拼接到结果向量中

### 效率问题

不过在进入正式的Transformer介绍之前，我们先来看看Attention块是否存在一些别的问题

之前我们一直说Attention is all your need，提到的这个结构不考虑位置信息也是建立在NLP的任务上的，并不能说这是Attention自己本身的问题，但是Attention真的就那么完美吗，它从结构上来说就一点问题都没有吗

还真不是，Attention的一个非常重要的问题就是他的传播效率，尤其是面对长句子的时候每次传播都要耗大量的运算资源，我们可以试想一个一千个词向量的句子，那么他就需要产生三千个$q,k,v$，且产生每个输出都需要运算若干次一千维的矩阵乘法，这相当于一个Attention块里面的运算复杂度就和几个一千个神经元的全连接层一样，其原因我们也知道是因为矩阵乘法的运算复杂度都是三次方级别的，不过由于这个乘法中另一维（词向量长度）并没有等量增长，但是就算这样也是次方级别的复杂度，对于大部分GPU来说几千维的矩阵运算同样是吃力的，此外Attention块本身就已经比较复杂了，何况还有更为复杂的Multi-head Attention，我们这些模型里面看到了常规CNN，RNN网络结构中都没有的多个全连接层，模型的复杂意味着过拟合，过拟合意味着要么减少参数要么增加样本，但是减少参数就不是Attention了（当然除去了Attention中使用Dropout的情况），于是我们只能多找一些样本数据，这导致Attention网络不仅模型传播运算的慢，往往还有大量的样本需要训练，训练出一个成熟模型的成本大大增加

但是和RNN不同的是RNN面对长句子时属于是空耗运算资源没把资源用在刀刃上（递归参数导致的梯度问题），导致效率又差效果又差，而Attention虽然同样耗资源，但是他至少能出成果，这也是他比起RNN更受吹捧的原因

但是当我们面对小体量训练样本或者是任务要求比较简单的时候时，有时我们也并不是一定无脑使用Attention，尤其是在自身设备和时间有限的情况下，使用Attention不仅容易过拟合，而且费时费力，可谓是吃力不讨好，这时候使用RNN或许是一个更好的选择，比如之前做过的文本情感分析任务来说，使用双向的GRU已经能够达到95以上的正确率了，Attention的需求也不会特别强烈了

Attention和存在时间比较长的神经网络比较，就像是深度学习和普通机器学习的比较，在处理相对简单的问题时大材小用反而难获得好的结果，因此方法本身并无优劣之分，只有分析对了问题的要求，才能更好选择适合的方法

### Transformer结构

我们终于可以开始学习Transformer的结构了，先给一张直观的结构图，他长这样，之后我们一个个来分解解读每一层的作用

![论文解读:Attention is All you need](https://picx.zhimg.com/v2-0af88beb0cc280317e06b24c2582eb60_720w.jpg?source=172ae18b)

乍一眼看过去，我们明显可以发现Transformer同样是一个Encoder&Decoder结构，在RNN中我们也提到过，在这个结构中Encoder只负责处理若干组向量的非线性映射，因此Encoder的输出并不会改变输入给定的向量组数量，但是可能改变向量长度，而Decoder则负责将Encoder映射完成的向量参与到另一组的输入中，最终输出指定组数的向量（判断任务）抑或是一组one-hot向量（生成任务），典型的运用任务就是翻译任务

#### Transformer Encoder

Encoder的结构是相对简单的，大部分都是之前已经详细说过的内容，其每一个有颜色的方框框所代表的含义如下

- Positional Encoding这就是之前说的位置编码，完全没有变化，由于位置编码只对词向量做加法操作，因此词向量经过该结构之后长度和数量也完全不变
- 之后这个句子词向量将会进入Encoder结构，图中框框框起来的地方称为一个Block，编码器由若干个Block组成，在Transformer原始论文中这个N为6，意味着相同结构的Block将会重复六遍
- Multi-Head Attention同样和之前说的没有区别，需要注意的是无论头有多少个Attention块的特点就是输入和输出在向量个数上完全一致，而在向量长度上由于其取决于$v$因此可能会不同，但是注意在Transformer中，严格要求多头注意力中的$k,q,v$完全一致，也就是保证进入Attention块的内容和从Attention块出来的内容完全一致，至于原因就在下一个步骤
- Add & Norm这其实是两个步骤，我们先来讲Add这是我们在卷积网络的[ResNet](#ResNet)中讲到过的伟大发明，已经成为了如今大型网络中必不可少的一部分，我们可以叫他Residual Connection（残差连接）结构，其操作正是将输入向量和输出向量加起来作为新的输出向量，由于这个加法的存在因此必须保证向量长度相等，这也是为什么如果你用pytorch的`TransformerEncoderLayer`类无法设置$v$大小的原因
- Norm步骤全称为Layer Normalization，和我们之前讲过的Batch Normalization非常相似，唯一的区别就是它以词向量为单位做标准化，对于每个从Attention块中出来的向量我都对其单独做一次标准化，标准化的方式同样是[Z-Score](#图像标准化)
- Feed Forward看上去好像是一个没见过的单词，实际上就是两层全连接层，至于为什么名字不一样其实还是因为这个全连接层处理的是一排向量而不再是单独一个向量，而且注意是一排向量中所有的向量都经过同样的全连接层，全连接处理的还是其中的一个向量，并不是什么把一排向量拼一起进入全连接层之类的，其中由于接下来还要做一次Add & Norm，因此这两层全连接层的第一层输出也就是第二层输入的长度是可以自定的，但是第二层全连接的输出必须和第一层输入一致，同样是为了保证整个Feed Forward的输出和输入内容一致

以上的所有内容共同组成了一个Transformer Encoder Block，正如我们预料的一样，Encoder部分并没有改变输入向量的维度和长度信息，输入是什么样的输出也是什么样的，经过了若干个Encoder Block之后，输出的一大堆向量就要开始进入Decoder了

#### Transformer Decoder

Decoder的大部分内容看上去和Encoder都没什么区别，不过所改变的地方还是需要根据实际的情景帮助理解一下，我们假设现在就在处理中文到英文翻译的任务，已知一个中文句子“我爱你”进入了Encoder，他是由三个词组成的句子，因此不管Encoder做了什么，输出一定是三个向量，当我们拿到Encoder的输出之后，我们就要开始生成英文句子了

由于我们也不知道第一个单词是什么，因此我们会先给Decoder一个开始的标志，比如就叫做`<begin>`，把这个标志名变成向量之后，此时注意Decoder的输入只有一个向量，进入了名为Masked Multi-Head Attention的结构，它和普通的Multi-Head Attention唯一的不同就是其中的每一个词向量的$q$只考虑其左边的$k,v$进行运算，忽略其右边的内容，大概是这样

![image-20230130162217303](https://s2.loli.net/2023/01/30/Q2OSfsoLWUZHd9q.png)

我们说过，只要是Attention结构，那么一律输入和输出内容一致，既然此时输入只是一个标识开始的向量，因此输出同样也是一个向量，于是这个孤零零的向量来到了第二层Multi-Head Attention，此时我们发现这个Attention块的输入箭头变成了两条来自Encoder的输出，一条来自前一层的输出，这又是什么东西呢

如果在Self-Attention你理解了当时我举例的情景，那么此处的操作其实是十分容易理解的，这个网络会拿着上一层输出的那单个向量经过全连接生成一个$q$，接着拿Encoder输出的一排向量经过全连接生成$k,v$，然后用这一个$q$和一堆$k,v$做Attention操作，而这完全符合Attention在实际情景中的含义，不过由于这个计算方法并没有把自己的$k,v$考虑在内，所获取的$k,v$也不是由自己的句子产生的，因此我们更乐意叫他Cross-Attention而不是Self-Attention（~~所以其实我给出的情景也只是一个Cross-Attention而已啦~~）

![img](https://s2.loli.net/2023/01/30/XehNzHIb97WtRJB.png)

除开这两不一样的部分之外，其余的内容Decoder和Encoder都是一样的，意味着对于Decoder来说，其实他输出的内容同样也是和输入内容完全一样的，对于这个中翻英的任务来说，这个起始标志向量输入进Decoder中，输出就是一个向量，然后我们再把这个向量扔到一个全连接层输出是所有英文单词的one-hot，得到翻译出的第一个单词`I`

于是我们拿到了第一个单词，同样的，我把这个`I`扔到Embedding变成一个词向量，然后再和最初的`<begin>`的词向量一起进入Decoder，输出是两个向量，此时要注意，由于接下来我们输出目标是下一个词的one-hot，因此此处其实并不需要由`<begin>`产生的那个向量，因此我们只取最后一个向量进入全连接层输出下一个词的one-hot

就这样一个词一个词的生成，直到Decoder网络输出的下一个词为预先规定好的结束标志，比如说是`<end>`，整个翻译的任务才算结束

### Teacher Forcing

你可能说不对啊，说好的Transformer的长项就是并行计算呢，你这么分析下来，Encoder确实是并行了，但是Decoder不是还是一个字一个字生成句子吗，这和RNN又有什么区别呢，我知道你很急，但是先别急，我可没说上面介绍的是我们实际训练的时候要做的事情，一个词一个词的产生结果仅限于我们在测试训练好的Transformer的时候使用而已，至于训练的过程可还没提到呢

训练的过程其实也是十分直觉的，其实我们想一想也就明白，在训练的时候，我们是拿着标准答案的，答案都有了再把Decoder的输出作为输入岂不是不把答案放在眼里，因此训练时Decoder的输出只作为计算损失的条件，而不将其作为下一轮产生下一个词的输入，也就是在Decoder中，我们永远都使用正确的句子词汇，而不管输出到底是什么玩意

这么一来，由于输出并不会影响输入，再由于Attention的特性，并行计算可谓是小菜一碟，唯一需要顺序执行的地方就只有先执行Encoder后执行Decoder这一点了，这一个也是模型需要无法改变的，于是训练过程中，无论是Encoder还是Decoder，我们都直接传进去一排词向量给他跑就完了，**这种不管输出强制使用标准答案作为训练输入的方法叫做Teacher Forcing**，这么一来在预测过程中看上去没什么卵用的Masked Multi-Head Attention也有实际作用了

## 自监督式网络（BERT）

当网络的模型越来越大，参数越来越多，为了保证网络不过拟合，我们训练的样本也必须同步增大，尤其是对于语言文字类，且任务目标有关创造性的模型来说，想要让一个模型说出一个完整的句子，其实最起码也得有百万上亿数量级的句子作为训练样本才行

在这一章中，学习BERT模型会帮助我们解决几个问题，一是如何弥补传统的Embedding模型（例如Word2Vec，GloVe）存在的问题，二是如何处理当训练样本过多人工标注标签实现难度较大时的情况，三是如何在文本任务上进行如同我们以往在图像分类上实现的迁移学习任务

### 传统Embedding的缺陷

当时我们在学习RNN的第一步就是了解了词向量的生成方式，同时我们以Skip-gram为例子介绍了如何自己训练一个词向量模型，使得在之后的任务中可以将每一个词编成不同的向量之后再加入网络中进行任务的学习，不知不觉之间词向量模型已经成为了每一个NLP任务不可或缺的一部分之一，但是以往的所有词向量模型是否存在什么问题呢

想想当时我们是怎么评价RNN网络的，我们说他过于以单词为主体而弱化了句子在文本分析任务中的分量，而词向量就是问题的关键，其原因是单个词是有多义的，我们都知道，在以往，我们都把一个词和其对应的向量看作是一个唯一映射关系，甚至在大部分词向量模型里我们都是拿一个字典数据结构来承载所有的词和他的词向量，但是“一个苹果”和“苹果手机”同样是苹果两个字却有着截然不同的含义，他们若是输出同一个向量那么无论之后网络结构如何复杂，在词编码这一块输出的内容一定是一样的

因此词的正确含义应该是包含在句子当中的，我们希望的词向量模型应该是我们给他一个句子，他可以返回一排词向量，而这一排词向量正是这个句子当中所有词的词向量集合，你可能会说不对啊，这样一来岂不是所有的词必须依附着句子才能获得自己的含义，万一我就是想给他一个词输出一个向量呢，说得好真正的词向量模型就应该是这么一个效果，可以这么说，没有一项NLP任务内是只有词没有句子的，不依附于句子的词向量就是耍流氓

如果你接受了词向量模型应该的样子，那么我们来分析一下，这个模型输入是一个句子，输出是一排向量，诶这玩意好像有点眼熟，这不就是Transformer Encoder做的事情吗，这下你说对了，传说当中的BERT，其实一点也不神秘，它就是一个Transformer Encoder

### Self-Supervised Learning

了解了词向量模型就是一个Transformer Encoder，那我们是不是就可以开始训练了呢，话是这样说没错，但是以往我们学习深度学习，训练一个模型都有三要素，分别是样本，模型和目标，但是我们现在充其量就是只有一个模型，样本和目标应该长什么样我们完全不知道，与此同时，我们也说过了，对于Transformer来说需要训练的样本数都是极其庞大的，以往训练样本比如进行线性拟合，我们找到了一堆样本，都是需要雇一大堆人来帮我们标注样本目标，然而过于多的样本（比如几个亿）全部人工标注显然不是很现实，是否有一种方法可以免去标注的手段直接把样本喂给模型，然后模型就可以自己去学习样本中的信息呢

听起来很玄幻，但是这并不是一个新的概念，在机器学习中他被称为Non-Supervised Learning，意思就是机器不需要人类去监督的学习样本中的特征，来到了深度学习中，尤其是BERT中，他被改名作Self-Supervised Learning，也叫做自监督学习

传统意义上的Non-Supervised Learning指的是根本没有任何标签只是一堆样本扔给机器他就可以学到东西，但是Self-Supervised Learnin虽然属于Non-Supervised Learning的一类但是实际上它还是有标签的，而他的标签就是全体初始样本，而输入进训练网络的是经过处理后被破坏的样本，这也是“自监督”的含义，比起文字处理方面的实践，其实自监督学习在图像方面用的更加广泛，比如我现在想做一个图像修补的任务，那么我先拿到了一堆图片，接着在图片预处理的时候将其中间抠掉一块然后扔进模型，接着把输出模型的图片和原始图片进行比对，计算损失，这样一来其实人并不需要介入标注每个图片，模型自动就可以学习到图片修复的任务

除了处理这种生成类任务，自监督模型还可以处理某些简单的标注任务或是排列任务，比如同样是图片，机器自动把他切成几份打乱作为输入，输出结果可以是正确的排列顺序也可以是输入内容是否为正确排列

我们会发现，自监督学习往往处理的都是通过破坏样本的方式构建输入，此类问题不需要人工标注，其实人工也不知道怎么标注，机器自动训练，自动构造标签和样本，自动迭代，于是我们就会想，如何能把构建词向量模型的任务也转换成一个破坏并重建的任务呢

### BERT Pretrain

在思考这个问题之前，我们不妨先思考一下在我们上学以来，学校是怎么检测我们是不是掌握某一个语言的，想想你的英语试卷上都有什么题目，我相信大部分人应该都做过诸如完形填空，阅读理解，词汇填空，病句判断，翻译，作文等等，而只有我们知道了每一个英文单词是什么意思之后，做这些题目才能够游刃有余，而我们知道英文单词的意思这个过程，是不是就可以理解成词向量的编码过程呢？

反过来想，我们如果也给机器做这些题目，他是不是也可以理解每一个词的含义，并学会怎么给每一个词编码呢，于是我们就可以分析一下以上这些题目哪些可以不需要人工介入机器也能够自己构造并学习，没错就是词汇填空和病句判断，对于其他问题，要么需要人工构造题干，要么需要人工评价答案，而对于词汇填空来说，我们只需要让机器随机给某一个句子挖掉几个空，然后控制机器输出原始句子就行了，而病句判断我们也可以让机器随机破坏或改变句子的一些结构，然后控制机器判断句子的完整性即可

#### 词语填空

BERT正是通过完成这两个任务来进行训练，对于第一个任务其实非常简单，我们只需要输入一个少了一个单词的句子，少掉的地方我们用一排等长的随机向量替换，我们知道，BERT就是一个Transformer Encoder，意味着我们输入的句子经过模型之后会吐出一排和句子等长的词向量，接着我们把这个挖了空的词对应的词向量输出加上一个比较小的全连接网络输出one-hot编码，然后和原始句子比对，对其求交叉熵确定词向量模型的下降方向

![img](https://s2.loli.net/2023/01/31/o7GylWdJZXps3tb.png)

但是你可能会有一个疑问，怎么感觉和我想的不太一样啊，我们说好是训练一个词的词向量，但是我现在在挖空的部分填入的只是一个随机的向量，这样一来训练出来的结果在这一个挖空的词上输出的词向量会不会往奇怪的地方下降，正常的情况不应该是输入正常的句子，然后在输出结果上取一个向量经过全连接后输出结果输入内容比较吗，这种想法并没有错，但是相比之下，这个填空模型会把除了空之外的其他所有词向量都往正确的方向训练，而对于单个词向量单独训练，一个句子就需要训练一个句子等长度的次数，这对本来就负担比较大的模型来说是得不偿失的

#### 病句判断

其实第一个训练的方式对于词向量的产生已经够用了，然而BERT为了更好的泛用性，在训练的时候还增加了一项训练内容，就是病句判断，但是在介绍这个方法前还是得说一下在BERT模型中几个特殊的标志词语，可以把他理解成之前我们提到过的`<begin>`，`<end>`等特殊符号，他们为了告诉模型句子的起始和结束，在病句判断的任务中，我们输入给模型的是两个句子，起始用一个名为`<cls>`的特殊符号标注，而两个句子的中间用`<sep>`标注，比如我现在输给BERT的是“我爱你”和“今天天气不错”两个句子，那么经过处理之后就是`<cls>我爱你<sep>今天天气不错`输入进BERT中，按照Transformer Encoder的尿性，输出的就是十一个词向量，接着BERT会只取`<cls>`这个词输出的词向量进行全连接，然后输出的结果是判断这两个句子应不应该接在一起

![img](https://s2.loli.net/2023/01/31/qiHfgxtS6VDJpvM.png)

对于现在的情况来说，显然是不应该接在一起的，所以输出结果将会和`No`进行比对并下降，至于选择两个句子的方式，则是通过随机抽取不连接的句子和连接的句子来实现，这对于有大量文本样本的机器来说也是十分容易做到的

于是对于BERT的训练，我们在搭建好网络模型之后，只需要从网络上随便爬一大堆文字扔给模型，让模型慢慢去跑就行了，因此对于BERT模型来说，百万级甚至几亿级的训练样本都是十分常见的，当然实验证明训练出这么一个强大的BERT也的确需要那么多样本的支持，在样本较少的情况下，训练的结果十分差强人意

**从一个什么都没有的Transformer Encoder中训练出一个可以进行词语填空和病句判断的模型这个过程就叫做BERT pretrain**（预训练），只不过由于其训练好的参数已经开源，因此对于一般人来说并没有什么重新自己训练一个BERT的必要，我们用他完全可以像用ResNet一样冻结其参数再进行我们个性化的训练，那么如何把训练好的BERT运用于我们自己的任务当中呢？

### BERT Fine-Tune

在了解了BERT的训练方式之后，接下来就是我们应该如何使用BERT了，根据训练方式我们可以知道，BERT可以接收三种类型组合输入，分别是一个单词，`<cls>`和`<sep>`，不过当然这些输入都必须经过多个组合才是有效输入，比如说现在我有一个训练好的BERT模型，那么我们可以丢给他一个句子“我爱你”那么按理说他就会给你输出三个向量，分别是“我爱你”这三个字的词向量

而BERT强就强在虽然他是用词语填空和病句判断训练出来的，但是对于大多数除此之外的NLP任务居然也是十分有效的，甚至于在他刚出来的时候在十几个NLP任务中就爆杀了当时被认可的最好的神经网络模型，接下来我会举例几个NLP领域的应用看看BERT如何处理这些下游任务

- 文本情感分析，拿我们做过的文本情感分析来说，只需要输入一个句子，在开头加上`<cls>`，接着把`<cls>`输出的向量接入全连接网络输出正面或负面标签即可
- 词性分析，同样是读入一个句子，把每一个词输出的向量分别接入同一个全连接网络输出词性即可
- 阅读理解，此处的阅读理解规定答案可以从文章中找到原句回答，则我们只需要读入文章和题目，中间用`<sep>`分开，然后拿到文章输出的所有向量，然后把这些向量和某一个随机向量做点积，取输出的值最大的位置作为答案的起始，同样的方式获得另一个位置作为结束，文章中起始和结束位置中间的部分即是题目的答案

会发现本来应该只会做填空和判断的模型在其他方面依旧有很好的表现，甚至于你都不知道为什么他会有好的表现，与此同时，由于BERT是一个Encoder，面对诸如翻译，问答等需要创造性的输出任务时，我们同样可以给他接一个正常的Decoder，使其有更加多样的功能表现

**这种在已经训练好的模型的基础上只添加少量网络结构便实现实际需求的过程成为Fine-Tune**（微调）
