# Machine Learning

学深度学习学上瘾了，于是打算直接梭哈机器学习

> 虽然顺序有点不对，但是在看这个之前最好看过深度学习那个笔记不然可能看不懂

## 线性回归

一个严肃的问题是，机器学习应该从哪里开始呢，如果你拿着这个问题去问老师或者是比较牛逼的同学，我相信只要这个人对机器学习有那么一丢丢了解，那么他就一定会建议你，从线性回归开始吧

由于线性回归的内容在深度学习中已经详细的从实用角度介绍过了，但是在介绍的过程中，我们并没有解释为什么通过这样的方式就能够达到我们的目的，究其原因其实也是为了希望当时的你可以在学习深度学习的过程中逐渐适应这种不需要解释，拿过来就可以用的思想，如果当时你咬着“为什么”不放，那么面对不可解释的深度学习来说，你迟早会在学习中因为“为什么”这个问题而崩溃

但是如今不一样了，机器学习的一大特点就是所有内容都可以通过优美的数学语言进行推导解释，妈妈再也不用担心我学不会了，只不过相对应的，文章中含公式量也会飙升，相较于可以囫囵吞枣只掌握网络结构的深度学习来说，机器学习的部分更适合你静下心来慢慢阅读，我相信我这种脑子不太好的人都能够解释清楚的内容，只要你不带着排斥的心态慢慢的尝试理解文中的公式，那么也一定能够理解每一个算法中的理论依据的

事实上，在本节中，我们只是使用线性回归的这个问题，来引出可以说是占领了机器学习半壁江山的估计方法——极大似然估计，至于他们之间存在什么关系，请听我细细道来

### 简单的回顾

那么我们先来简单的回顾一下线性回归到底干了一件什么事，我们用一个简单的流程来过一遍

1. 首先，线性回归需要解决的问题是，我们有一堆共$n$个样本，简单起见每一个样本都只有一维，他们分别为$x_1,x_2\dots x_n$，并且我们有这堆样本的$n$个标签$y_1,y_2\dots y_n$，这些样本和标签值都是连续的，比如，我们可以把$x$看作是身高，$y$则是体重，那么我们的目标就是，在给定了这一堆$x,y$的前提下，现在我拿到了一个没有$y$的$x$，即现在我只知道一个人的身高，现在我需要通过给定的样本信息去预测这个人的体重$y$是多少
2. 线性回归预想了一种十分简单的情况，即身高$x$和体重$y$满足线性关系，也就是说如果以$x$为横坐标，$y$为纵坐标，我们把这些样本点画在坐标系中，则我们现在假设有一条隐藏的直线可以拟合这些样本点，我们的目标就是求解出这条最优的直线
3. 接下来，我们需要先随便假设出一条直线$f(x)=kx+b$，接下来我们需要引入一个名叫均方误差的损失函数，具体来说，我们会计算每一个样本点$x$在$f$上的值$f(x)$，接着用$x$的真实标签$y$与之相减累和，即$L=\sum_{i=1}^{n}(y_i-f(x_i))^2$，我们将其叫做**评价函数**（或者损失函数）
4. 之后我们的目的就是找到一条直线能使评价函数值最小，即求解出参数$k,b$使$L$取得最小，这个过程，我们既可以使用梯度下降方法，也可以直接通过对$k,b$求偏导求出解析解，总之，在经过了一系列的方法之后，我们求解出了最终的结果$\hat k,\hat b$
5. 接着，我们就可以拿着这个$\hat k,\hat b$所定义的直线$f$去预测任意的$x$了，至此我们完成了线性回归的整体流程

### 模型 评价 求解

你说这个过程不是很简单吗，每一步看上去好像并不是那么难解释，你凭什么说我只会用却不懂原理呢？是的，线性回归问题的确是最简单的一类机器学习方法之一，但是通过这个问题，你仍可以思考下面三个问题

1. 为什么选择直线**模型**来拟合所有的样本点？
2. 为什么可以使用均方误差作为**评价**函数？
3. 在确定了均方误差之后为什么可以通过求偏导的方式**求解**解析解？

这三个问题看似简单，但是它们构成了机器学习方法的三大要素——**拟合模型，评价指标和求解方法**，对后面对任何机器学习方法，你都可以在其中抽离出有关这三个问题的讨论，正是以这三个问题为基础，才能构建出了整个机器学习的体系论

而对于线性回归问题来说，上面的第一和第三个问题其实是容易的

对于第一个问题来说，其实答案就是因为它的名字叫线性回归而不是抛物线回归，线性回归能叫出这个名字就已经规定了必须使用线性模型

而第三个问题的原因则是它的评价函数是可导且导数的零点是可求解的，注意此处的两个条件，在之后我们可能遇到某些时候，它构建出的评价函数不可导，或者求导之后无法求解出导数为零时的参数的，这时候通过求导这种方式进行的求解过程显然就不可行了

于是所有的问题都到了第二个问题上，为什么我可以使用均方误差来作为评价函数呢？

均方误差的这个过程看似也是很显然的，但是如果深究一下你就会发现很多问题你并没有办法解答，比如

- 从公式角度，为什么样本点和函数值相减取平方作为公式的主体，就算是拿距离作为度量指标，也可以构造出无数种公式，比如说我还可以把两者相除后减一再平方，甚至我不想平方而是取绝对值或者取四次方难道不行吗，这个公式是如何蹦出来的呢，总不会是只是觉得它简单吧
- 从结果角度，你怎么知道这个直线就一定是最佳的拟合直线的，我把这条直线往上挪一点点或者改一丢丢斜率难道就一定错了吗，万一背后的直线模型就刚好是往上挪了一点点的直线呢，这种情况在之后的线性分类问题中更容易被提出来，面对线性可分的两堆样本点来说，其实存在无数条直线可以区分这些点，那么凭什么我求出来的这条直线是最优直线呢
- 从预测角度，你会发现即使你求出了所谓的解析直线，但是其实几乎所有的点都无法真正百分百的落到这条直线上，而是均匀的分布在直线的上下边，但是在进行预测的时候，直线却只能输出一个固定的值，这显然也是不合理的，从直觉角度，我们似乎也应该给预测值加上那么一些些的随机偏移量，那么这些偏移量该如何取值呢

在思考这些问题的过程中，我们似乎发现了，即使是简单的线性回归的背后，似乎也有某些不为人知的神秘力量在支持着它能够起作用，不要着急，在接下来几节中，随着学习的深入，上面的问题都会被逐步解答

### 高斯分布

相信大家在概率论这门课中都学过正态分布，当时老师们或许会举一个例子，说现实中很多数据的分布其实都是满足正态分布的，比如世界上所有男生的身高很可能就是一个正态分布，本节就让我们从公式的角度深入了解一下正态分布也就是高斯分布到底是什么东西，同时基于我们学过的低维高斯分布，引出在高维情况下的高斯分布公式，需要注意的是本节我们不讨论公式的来由，所有的推导过程都是在已知公式的基础上进行的，如果想知道为什么会产生这么一条复杂但神奇的公式请不要着急，我会在之后的章节中提到

#### 一维高斯分布

首先我们来看一下最简单的也是我们概率论中提到过的一维高斯分布的密度函数
$$
f(x) = \frac{1}{\sqrt{2\pi }\sigma }e^{(-\frac{(x-\mu)^{2}}{2\sigma^{2}})}
$$
这条公式告诉我们，想要确定一个一维高斯模型，我们必须确定两个非常重要的参数$\mu,\sigma$，如果你有印象，你的老师一定说过，$\mu$控制了高斯分布密度函数均值的位置，而$\sigma$则控制了高斯分布样本在均值两侧的离散程度（或者说方差），如果直观的展示在图上，高斯模型的密度函数的图像像一个竖起的驼峰，当$\mu$变化时，函数图像会随着坐标轴左右平移，而当$\sigma$变化时，函数图像的峰值位置则会高低大小起伏

![image-20230529185649387](https://s2.loli.net/2023/05/29/6inT7LzOHqawcvl.png)

> 如图红线为标准正态分布，紫线为固定$\mu$后将$\sigma$在$[0.3,1)$范围变化，而绿线则是固定$\sigma$后$\mu$在$[-3,3]$变化

#### 概率密度函数

在进行接下来的内容之前，我们需要先明确一个概念，那就是密度函数到底是什么，请千万不要将它和所谓的分布函数或者是概率函数搞混了，尤其是之后当我们不区分离散变量和连续变量的时候，我们往往会在推导的时候逐渐的把离散变量的概率函数和连续变量的密度函数混为一谈，但是请在所有计算过程中时刻牢记，**密度函数的函数值不是这个点的概率值！**

正因为这个函数值不是概率值，因此没人规定密度函数一定小于1，即使出现存在正无穷值的密度函数也没什么好奇怪，这个道理其实一说就懂，比如你只要有足够精度的计量机器，你找不出世界上身高完全一样的两个人，甚至一个人一分钟前和一分钟后的身高都不一样，因此对于像身高这种连续变量来说，所有的变量值单个取值的概率值都是无限小的

那么密度函数的函数值代表的含义是什么呢，他其实是概率的变化率，如果觉得有点抽象，我们可以将其类比为速度，而输入的变量则是时间，那么密度函数就可以看作是一条速度\-时间曲线，这时候概率值就变成了位移量，我们只知道在给定的时间中通过了1单位的位移（定义域上的概率密度函数积分为1），但是我们却没必要讨论在某一个时刻通过了多少位移

由于概率的变化率这个概念本身并不像速度那样有具体的意义，围绕着概率进行的讨论才是机器学习方法的目的，因此抛开积分不谈的概率密度函数就是耍流氓，而当存在积分号时，在概率密度函数上某一段的定积分确实就是概率值，正是由于他的这个性质，我们有时会将密度函数的符号与离散变量的概率函数一致化，都写作$p(x)$

#### 多维高斯分布

如果说上面那么多内容只是放在开头骗你看下去的话术，或者说只是帮你回忆一下你在概率论里面学了什么东西，那么接下来就要到比较超纲的部分了，之后的内容也将逐渐脱离专业课的范畴，先在此处提前预个警

现在，想象如果我们收集了不止一个身高信息，还有体重信息，假设他们都满足高斯分布，那么概率论会告诉我们两个高斯分布的联合概率分布也是高斯分布，但是书上不会告诉你这个联合概率分布的高斯模型长什么样，由于此时求解的高斯分布也不再是一个维度，因此我们可以推广出多维高斯分布模型，他的密度函数如下
$$
x\sim\mathcal{N}(\mu, \Sigma)=\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)}
$$

> 此处就不花大篇幅介绍有关边缘概率，条件概率，联合概率，协方差，条件独立等基础的概率论知识以及像是向量，矩阵，转置，逆，行列式等基础的线性代数的知识了，如果对这些概念及含义（主要是含义）不了解可以查阅教材或者善用搜索引擎

同样的，我们先不关心公式的来由，仅从解读符号意义的角度来说，在这个公式中，$x,\mu$都是向量，$\Sigma$是一个协方差矩阵（也就是半正定矩阵），$D$是$x$的维度是一个值，同时分母的那个$|\Sigma|$其实是求解$\Sigma$的行列式

符号的意义解释完了，不过相信你还是晕晕的，其中最主要的晕点其实还是发现怎么莫名其妙的多了些线性代数的知识，刚刚不还在学概率论吗，实际上，如果说概率论是技巧，那么线性代数就是达到这些技巧必不可少的工具，他们并没有你想象中两门学科分隔的那么大，或者说，当你把概率论的知识拓展到高维的过程中，线性代数绝对是你必须要迈过的坎

话说回来，如果对比着一维高斯分布的密度函数公式来看，我们发现这两个公式之间好像差别并不是很大，想要确定一个多维高斯分布，我们同样需要先确定两个参数$\mu,\Sigma$，只不过这两个参数不再是一个值而是一个包含多个值的向量和方阵，输入的$x$也变成了一个包含多个值的向量，仅此而已

需要稍微注意一下的是这个公式的指数部分$\Delta=(x-\mu)^{T} \Sigma^{-1}(x-\mu)$，我们假设输入维度为二维，我就直接说结论了，在$x$是二维向量的情况下，当确定了一组$\mu,\Sigma$的时候，实际上你就确定了一组椭圆，$\Delta$变化的时候，他会产生一组同心椭圆或者是等高线，此外还有另一组解读方式，你可以把这个公式看作是一个距离表达式，只不过此时的距离相等量不再是一个圆到圆心的距离而是椭圆，这被称为马氏距离，也就是这组椭圆上的任意点带入到这个公式里都会产生相同的$\Delta$

当$\Sigma$为单位矩阵的时候$\Delta=(x-\mu)^2$也就是我们熟知的欧氏距离或者圆的公式，$\Sigma$控制了同心椭圆的原点位置和旋转角度

有了这个理解之后，我们就会发现我们好像能够画出至少二维高斯模型的图像了，因为当我们已知一个模型参数并且确定了一个模型输出之后，会发现$(x-\mu)^{T} \Sigma^{-1}(x-\mu)$将等于一个确定的值，沿用上面的结论，即所有的$x$都将落在一个椭圆上，用人话来说，如果我将一个二维高斯分布画在三维坐标系上，那么我沿着函数值方向上横劈一刀，所获得的切面将是一个椭圆，如图

![image-20230529212809421](https://s2.loli.net/2023/05/29/aE5dpqFvxQ9ywuf.png)

另外还需要注意的是，当我们希望人们知道我们正在采用高斯模型作为我们的推导模型的时候，我们会使用上述那样的$x\sim\mathcal{N}(\mu, \Sigma)$来表示我们是从概率密度函数满足高斯分布的模型中抽取出来的$x$，由于高斯分布只需要确定两个参数，因此比如标准正态分布就会写成$\mathcal{N}(0, 1)$这样

#### 高斯分布概率定理

概率论告诉我们，在已知联合概率分布的时候是可以求解出边缘概率和条件概率的，但是如果只知道边缘概率或者只知道条件概率却求不出联合概率，这也就是为什么书本上会提到两个正态分布的联合概率一定是高斯分布，但就是不告诉你这个高斯分布是怎么样的，不是他不想说，是他也不知道，其中的一个重要因素就是变量之间的独立关系，比如身高和体重就肯定是两个不相互独立的事件，身高高的人普遍也比矮的人体重更重，因此我们除了需要知道身高和体重单独的分布关系，还需要知道他们之间的相互关系，如果用数学语言表达，就是两个事件的协方差，协方差描述了事件与事件之间的独立关系，这也是为什么在多维的高斯分布公式中，我们见到了那个协方差矩阵的原因

在这一节中，我们就要详细推导一下如何通过多维高斯分布的概率密度函数，同时也就是所谓的联合概率，去推导出这个模型里的边缘概率和条件概率，以及当我们同时拥有边缘概率和条件概率的时候，他们的联合分布又该如何求解，之后我们可以把这些转换公式当作结论记住，在之后有关高斯分布的概念转换的时候可以拿来就用

首先我们先要声明一个定理，就像是相对论规定了光速不变一样，这个公式的证明非常容易，因此详细的证明过程就不展开了，感兴趣的可以搜一搜，他表示所有的高斯分布经过了线性变换之后一定还是一个高斯分布
$$
x \sim \mathcal{N}(\mu, \Sigma), y \sim A x+b \Rightarrow y \sim \mathcal{N}\left(A \mu+b, A \Sigma A^{T}\right)
$$
其中$A$是一个任意与$x$行相等的矩阵，需要注意在没有特别说明的情况下，我们都会默认所有的向量都是列向量而不是我们可能刚学向量的时候喜欢写的行向量，列向量的维度是$n\times 1$

之后我们如果令
$$
x=\begin{pmatrix}
x_a \\
x_b
\end{pmatrix},\mu=\begin{pmatrix}
\mu_a \\
\mu_b
\end{pmatrix}, \Sigma=\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}
$$
其中$x_a,x_b$是对向量$x$进行分块，原本$x,\mu$长度为$p$则$a+b=p$，则我们可以将$x$看作是$x_a$和$x_b$的联合分布概率（比如在$x$的维度为2，那么$x_a,x_b$就是两个值，用上面的例子就分别代表身高和体重）现在我们的需求是根据联合分布概率去求解边缘概率分布和条件概率分布，在此之前我们引入一条式子

首先我们需要搞清楚，我们求解的是分布情况，而我们又知道对于高斯分布的线性变换一定是高斯分布，因此我们的最终目标一定是求解边缘概率分布和条件概率分布的$\mu$和$\Sigma$两个值即可

1. 边缘概率分布$p(x_a)$其实非常好求，因为我们可以将$x_a$写作$x_a=\begin{pmatrix}\mathbb{I}&\mathbb{O}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}+0=Ax+b$，接着照着上面的式子，容易得到$x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$，这个结果也是符合我们直觉的

2. 条件概率分布$p(x_b|x_a)$则需要花一点功夫，我们需要先引入一条式子
   $$
   x'=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a
   $$
   需要注意这条式子可以说没有任何意义，他只是为了辅助得到条件概率的工具罢了，同时之后如果要求解$p(x_a|x_b)$则需要把这几条式子的$a,b$都进行对换，然后我们可以得到
   $$
   x'=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbb{I}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}=Ax+b
   \\ \mu'=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\\Sigma'=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
   $$
   然后我们如果把第一条式子变一变，即可得到$x_b$的求解式
   $$
   x_b=\mathbb{I}\times x'+\Sigma_{ba}\Sigma_{aa}^{-1}x_a=Ax+b\\
   \mathbb{E}[x_b]=\mu'+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
   \mathbb{D}[x_b]=\Sigma'\\
   x_b\sim\mathcal{N}(\mathbb{E}[x_b],\mathbb{D}[x_b])
   $$
   由于这个$x_b$的$\mu$和$\Sigma$是由$x_a$控制的，因此其实这个$p(x_b)$正是等于$p(x_b|x_a)$，于是我们得到了
   $$
   x_b|x_a\sim\mathcal{N}(\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a+\Sigma_{ba}\Sigma_{aa}^{-1}x_a,\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab})
   $$

知道了如何用联合概率求解边缘概率和条件概率，我们不妨反过来再看看如何用条件概率和边缘概率求解联合概率

现在我们已知$x\sim\mathcal{N}(\mu,\Lambda^{-1}),y|x\sim\mathcal{N}(Ax+b,L^{-1}),y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})$，需要求解$p(y),p(\begin{pmatrix}x\\y\end{pmatrix})$

1. $p(y)$根据公式可以直接算出$y\sim \mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)$，计算过程中注意$\epsilon$和$x$相互独立因此均值和方差加法计算均可以分解后独立计算

2. 根据上面结论其实易得
   $$
   \mathbb{E}[\begin{pmatrix}x\\y\end{pmatrix}]=\begin{pmatrix}\mu\\A\mu+b\end{pmatrix}\\
   \mathbb{D}[\begin{pmatrix}x\\y\end{pmatrix}]=\begin{pmatrix}\Lambda^{-1} & Cov(x,y)\\Cov(y,x) &A\mu+b\end{pmatrix}
   $$
   由于$Cov(x,y)=Cov(y,x)$因此我们只需要求一个即可，根据协方差公式$Cov(x,y)=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])]$易得$Cov(x,y)=\Lambda^{-1}A^T=A\Lambda^{-1}$带入即可得$\begin{pmatrix}x\\y\end{pmatrix}\sim(\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1} & \Lambda^{-1}A^T\\A\Lambda^{-1} &A\mu+b\end{pmatrix})$

之后如果想求$p(x|y)$则可以根据上面联合概率求条件概率的方式求解，此处不提

### 极大似然估计（MLE）

通过一堆乱七八糟的公式，我们稍微深入的了解了一丢丢高斯分布的情况，同时我们也反复提到，如果我们收集了足够多的身高样本，那么（其实是我们假定）他是满足高斯分布的，那么问题来了，我们该如何确定高斯分布的那两个参数呢

你说这还不简单，你前面都说了这两个参数一个表示均值一个表示方差，我拿着那么多样本求出他们的均值和方差，高斯分布不就确定了，这有什么讨论的必要吗？

那么恭喜你，你陷入了一个大家都非常容易陷入的误区中，那就是把所谓的统计量和模型的参数混为一谈了，这和之前讲过的概率密度函数和概率函数一样，他们都是在我们使用得多了之后容易无意识的混淆的概念，无论是高斯模型还是我们之前通过线性回归求解的线性模型，其中的参数本身都是不具有任何意义的，我们更不是因为我想要规定这些参数而定义的模型，而是在定义一个模型（比如一条直线）的时候自然而然产生的这些参数

其实无论是高斯模型还是线性模型，所有的参数都是需要通过求解过程求出来的，我们将**通过一堆从某个模型中抽取的样本去还原原始模型（往往是求解模型参数）的过程叫做似然**，这时候就不得不再次提到上上小节我们强调过的机器学习三要素，**现在我们可以进一步叫他似然估计三步骤，他们分别是模型种类、评价函数和求解方法**

> 在很多时候，我们经常会把似然和概率这两个概念进行比较，你也可以将其作为辅助理解的一种方式
>
> **概率**就是抛硬币之前我们知道加入我们抛出去一次有50%概率为正面，也就是我们知道一个模型的所有信息去预测可能的结果，而**似然**则是我们拿到了一万次抛硬币五千次正面的结果得到正面出现的可能是50%，也就是根据结果去重构模型的参数
>
> 他们的关系大概是已知样本和模型种类可以通过似然推导出模型参数，已知参数的模型可以通过概率预测出未知样本

首先我们需要明确目前我们的问题，我们希望通过手里的一堆真实的身高样本，找到背后产生这些身高样本的模型，并且我们假设这个背后的模型是高斯模型，这么一来，上面的三步骤的第一步确定模型种类我们就解决了，顺序往下，下一步就是如何确定评价函数的问题

这时候我们这一节或者说是本章最重要的知识点就要出现了，他就是极大似然估计，英文全称Maximum Likelihood Estimation，简称MLE，简单来说，极大似然估计就是在这个问题中我们需要找的评价函数，不过使用极大似然估计不是没有代价的，在此处我们暂时先仅写出极大似然估计的公式，关于公式的讨论留在之后进行，他的公式是这样的

> 事实上，关于到底是极大似然估计还是最大似然估计这两个翻译其实都有，到底是极大还是最大取决于的是你求解方法的选择到底能不能达到最大值，因此没必要过多在意翻译的问题，在本文中我们统一使用极大似然估计称呼MLE

$$
\hat\theta =\arg\max_\theta\prod_{i=1}^Np(x_i)= \arg\max_\theta\sum_{i=1}^N\log(p(x_i))
$$

> 可以发现在存在着$\arg\max$的情况下，只要我们不改变其中函数极值点的位置，且我们保证$p(x)$一定是大于0的，那么我们可以给内部的函数加上像$\log$这种单调增并且复合函数不影响内部极值的函数，这样的变化并不会改变求解结果，还能方便我们运算

这个公式看起来乱七八糟的符号怎么那么多，没关系，接下来我就以上面的问题为例，使用极大似然估计的方法一步步带你过一遍求解的前向过程，在这个过程中，你可以对照着看上面的公式进一步理解

1. 首先写出带未知参数的概率密度函数，对于上面描述的问题也就是一维高斯分布来说，就是$\mu,\sigma$两个参数，即 $p(x) = \frac{1}{\sqrt{2\pi }\sigma }e^{(-\frac{(x-\mu)^{2}}{2\sigma^{2}})}$

2. 接着把已知的所有$n$个样本$x$代入$p(x)$，注意此时$p$中的$x$都被确定，未知的只有参数部分

3. 然后把和样本数一样多也就是$n$个带参数的$p$取一个$\log$再加在一起产生$\sum_{i=1}^N\log(p(x))$，于是我们构建出了极大似然估计中的似然函数，如果对似然函数没什么概念，那么如果在这个函数前面加一个负号，那么他就还有一个名字，叫做损失函数，到此为止，我们完成了三步骤中的第二步，确定评价函数，这个评价函数就是我们构建出的这个似然函数

4. 你说损失函数我熟啊，没错，接下来我们就需要去求解这个似然函数的最大值（损失函数的最小值），在求解方法的选择上，我们仍然可以使用最简单的偏导方法，具体来说，我们只需要对损失函数关于$\mu$和$\sigma^2$求偏导之后，求解导数为零时的极大值$\hat{\mu},\hat{\sigma}^2$即可

由于求解的过程用到的都是高数里面最基础的偏导知识，因此这里就不过多的一步步带你推导了，需要稍微注意的一点是在求解的过程中，我们需要先对$\mu$偏导求解$\mu$的解析解$\hat\mu$，再拿着这个$\hat\mu$去求解$\hat\sigma$，总之，在经过这么四步之后，我们可以得到通解$\hat\mu=\bar x=\frac{1}{N}\sum^{N}_{i=1}x_i,\hat\sigma^2=\frac{1}{N}\sum^{N}_{i=1}(x_i-\hat\mu)^2$，诶你发现了，统计量这不就来了，没错，**通过极大似然估计这种方法，对样本进行高斯模型拟合，求解出来的参数的最优解析解正是样本的均值和方差**

到此为止，如果有人问你，凭什么要用样本的均值和方差来作为拟合高斯模型的参数，这样子拟合出来的高斯模型凭什么是最优的呢？你终于至少可以自信的回答——我不对这种操作负责，是极大似然估计这么说的！

于是，接下来的问题就来到了极大似然估计为什么能够那么狂妄的说他求出来的就一定是最优拟合模型呢，实际上，对于上面那个问题来说，我其实可以告诉你极大似然估计求解出来的还真就不是最优解析解，也就是说，假如你真的无聊自己先定义了一个身高高斯模型，然后模拟从这个高斯模型中抽取出来若干个样本进行极大似然估计，假设你重复了足够多遍，这时候把你所有求解过的方差取个平均，你会发现这个每次都用极大似然估计求解出来的方差的平均值实际是偏小于你定义的模型的那个所谓的参数$\sigma$的，正是有这样的偏差，因此我们把通过极大似然估计求解出来的高斯模型的方差参数叫做有偏估计，真实的无偏估计方差应该是$\sigma^2=\frac{1}{N-1}\sum^{N}_{i=1}(x_i-\mu)^2$

### 最小二乘估计（LSE）

这么说好像极大似然估计也没什么厉害的，怎么搞了半天求出来的结果还是错的，我知道你很急但是先别急，我可以先把结论告诉你，极大似然估计一定是没问题的，你觉得有问题，那其实是你有问题，究其原因，是因为我在介绍的时候就是直接把公式抛给你，却没有跟你说明白这条公式产生的条件，没错，不只是模型的参数选取的背后隐藏着评价函数的这个条件，选取评价函数的过程同样是需要满足一些隐含条件的，什么时候可以用极大似然估计，什么时候不能用，都是需要根据样本的实际情况进行判断的

不过现在，我们刚了解的这个极大似然估计，趁这个知识还热乎的，我们先把一开始留下的那个问题解决掉，也就是——为什么线性回归会使用均方误差作为损失函数呢？

其实，把均方误差作为损失函数，并且求解这个损失函数最小值的过程其实是有一个洋气的名字的，我们可以叫他最小二乘估计（Least Squares Estimation），其中最小指的就是最小化损失函数，二乘指的就是均方误差，求解的这个过程叫做估计

#### LSE的高维形式

在解释这个方法的来由之前，请允许我先把这个过程拓展到高维度，由于估计的过程我们在之前已经回顾过了，接下来我只提一些拓展过程中可以遇到的问题和一些技巧

首先均方误差的高维公式长这样$L(W)=\sum_{i=1}^{N}\left\|f(x_i)-y_{i}\right\|^{2}$，看上去没什么区别，但是需要注意这里的$x,y$都是一个向量，但是他们不一定是同维的（比如我想通过身高和体重预测年龄，这样$x$就是二维$y$就是一维），假如我们希望似然的是线性模型，那么其中的$f(x)=W^Tx$，其中$W$是一个矩阵，其长宽与$x,y$维度有关，你说怎么感觉少了点什么，没错，其实是我在$x$中多增加了第零维，即$x_{0}=1,W_0=b$实现线性关系中的$b$偏置，这种小技巧在很多深度学习的框架中都可以见到，可以有效的简便运算

进一步的，假如我们有$N$个样本点，那么我们可以直接拼接它们令$X=(x_1,x_2\dots x_N)^T,Y=(y_1,y_2\dots y_N)^T$，则均方误差可以改写成$L(W)=(W^TX^T-Y^T)(XW-Y)$，接下来我们对$W$求偏导即可求得LSE在线性问题上的解析解，即$\frac{\partial L(W)}{\partial W}=2 X^{T} X W-2 X^{T} Y=0$，可解得$\hat{W}=(X^TX)^{-1}X^TY$，其中$(X^TX)^{-1}X^T$是$X$的左逆（或者是伪逆），这是用求导方式最小化损失函数

> 矩阵的求导在此处我们暂时不提，之后我们会统一花一节的内容介绍矩阵求导的过程，目前来说你只需要理解这个过程，并且知道他的求导结果即可

如果我们只考虑输出一维内容，即$y$为一个值，此时$Y$的维度是$N\times 1$，也是一个向量，这时候从另一种角度看$L(W)$还可以看作是在高维空间中找出一个可以垂直于指定向量$Y$的超平面，即最小化向量$Y$在$X$向量组构成超平面中的投影（反映到一维其实就是我们所谓的求解$y$到$f$直线的距离），从这个角度来说，为了让模型仅有唯一解，我们应该让样本的数量大于等于每一个样本的维度，保证在高维中可以形成平面

更通俗的说，比如你想考察身高和体重两个维度，你总不可能想着是只拿自己一个人的身高和体重建立模型，从几何上来讲，其实就是我们无法拿一条二维向量在三维空间构成一个平面，至少得有两条线才能形成平面

#### LSE的正确性

搞了那么久，终于到了重点，我们应该如何说明这个在线性回归中我们使用的评价函数也就是LSE的正确性呢，实际上，他的证明过程就是使用我们刚刚学过的极大似然估计

其实需要证明的最大原因是还是直线的拟合问题，因为我们发现，假如最终的模型是完全符合我们预测出的那条直线的，那么所有的样本点应该一点都没有偏差的全部落在直线上，但是事实是样本点虽然均匀分布在直线的上下两侧，但是就是不落在直线上，也就是这一点，让我们所拟合出的直线看上去好像没那么有说服力

但是现在我要告诉你，**线性回归背后所使用的模型其实根本就不是绝对的线性模型**，我们在那三步骤的第一步假设模型的时候其实就是有问题的，只不过是均方误差在为我们负重前行，他帮助我们纠正了我们假设当中的错误

事实是，我们真正假设的模型是一个带有噪声的线性模型，我们可以将他写作$y=f(x)+\epsilon,\epsilon\sim \mathcal{N}(0,\sigma^2)$，正是这个模型让产生的数据在高斯噪声的作用下落在实际直线两侧，这么一说是不是恍然大悟，那么有了这个模型，进一步的，我们还可以套用一下曾经说过的[高斯模型线性变换公式](#高斯分布概率定理)，这样易得$y\sim\mathcal N(w^Tx,\sigma^2)$（此处为了方便仅假设一维直线），此时我们的$y$由于也满足高斯模型，因此，此时的问题又演变成了如何找到一个最佳拟合样本标签点的高斯模型，咦，这描述怎么那么熟悉，没错，下一步我们就要使用极大似然估计来求解$f(x)$中的参数$w$，这次我会写出其中的部分推导过程，顺便也弥补了之前高斯模型省略的推理过程
$$
\begin{align*}
\hat w&=\arg\max_w\sum_{i=1}^N\log p(y)\\
&=\arg\max_w\sum_{i=1}^N\log(\frac{1}{\sqrt{2\pi }\sigma }e^{(-\frac{(y_i-w^Tx_i)^{2}}{2\sigma^{2}})})\\
&=\arg\max_w\sum_{i=1}^N\left(\log\frac{1}{\sqrt{2\pi }\sigma }-\frac{(y_i-w^Tx_i)^{2}}{2\sigma^{2}}\right)\\
&=\arg{\color{red}\min_w}\sum_{i=1}^N(y_i-w^Tx_i)^{2}
\end{align*}
$$

> 最后一步的变换其实很简单，因为我们需要求解的是$w$的最大值，因此任何和$w$无关的变量都可以看作是常量，而对一个函数的加减或者乘除一个正常数（如果是负的就得像我上面一样把$\max$改成$\min$）都不会对函数极值点所对应的$w$产生影响，于是我们就可以约去所有的无关项，只留下我们需要的部分，在之前求解高斯分布的过程中用的也是这个技巧，这都得得益于极大似然估计的$\arg\max$操作

可怕的事情发生了，我们发现当我们的目标是求解这条直线的参数（也就是$w$）的时候，我们最后的求解目标居然变成了求解出$\sum_{i=1}^N(y_i-w^Tx_i)^{2}$的最小值，这求解的玩意可不就是均方误差吗，最小化均方误差，那和最小二乘估计有什么区别？终于，我们把这两个知识串联起来了

总的来说，**最小二乘法的公式是建立在极大似然估计之上的，它隐含了样本噪声服从均值为零的任意高斯分布的假设**，之后如果别人问你凭什么线性回归求解出来的直线是最佳拟合直线的时候，你同样可以自信的回答他们——是极大似然估计这么说的！

### 最大后验估计（MAP）

没错，极大似然估计已经两次成为了你的挡箭牌，但是万一别人还是不服气，更进一步问你凭什么极大似然估计就是对的？那么目前的你就只能用笑容来掩饰自己的尴尬了，不过不要担心，在本节中，我们会从概率学的角度入手，帮助你理解极大似然估计到底假设了一件什么事

在这之前，不知道你有没有想过，所谓的似然估计三步骤或者说似然的过程，能不能用一条公式表达呢，不然我们每次学机器学习的内容都要念一边十二字真诀好像有点中二，事实上从数学角度根本没有那么多花里胡哨的东西，由于我们就是有一堆样本想要求解出背后的模型参数，因此公式其实就是一行
$$
\hat{\theta} = \arg\underset{\theta}{\max} P(\theta\mid x_1,x_2\dots x_n)
$$
这个公式解释起来更加简单，就是我们在已知一堆样本$x_1,x_2\dots x_n$的条件下，想在所有的模型参数里找到一个概率最大的模型参数$\theta$而已，没错就是这么简单，这也是所有似然问题的最本质原理，同样的，我们发现这种带$\arg\max$的公式好像都有一个什么什么估计结尾的名字，没错，这条公式的名字叫做最大后验估计（Maximum a posteriori）

不过问题就在于这个公式看着挺好看就是显得有些抽象，在大多数情况下我们都没办法具体写出公式的表达式并解，面对条件概率不好解的时候应该干什么，没错，接下来就需要祭出伟大的贝叶斯公式了，我们将公式转变为
$$
\begin{align*}
\hat{\theta} &= \arg\underset{\theta}{\max} P(\theta\mid x_1,x_2\dots x_n)\\
&=\arg\underset{\theta}{\max}\frac{P(x_1,x_2\dots x_n\mid\theta)P(\theta)}{P(x_1,x_2\dots x_n)}\\
&=\arg\underset{\theta}{\max}P(x_1,x_2\dots x_n\mid\theta)P(\theta)
\end{align*}
$$

> 由于分母部分不包含$\theta$因此对我们求解不造成影响可以划去

#### 似然概率

最后化简出来的式子也是我们一般见到的最大后验估计的样子，我们发现它包含两项的乘积，我们暂时不管后一项，只关注$P(x_1,x_2\dots x_n\mid\theta)$这一项，这一项的概念十分容易理解，即在模型参数为$\theta$的情况下，同时出现$x_1,x_2\dots x_n$的概率，与此同时，比如我们拿身高的分布举例，由于我们获取到的样本$x$之间都是独立的，因此他们的联合概率其实可以转换为边缘概率的乘积，即
$$
P(x_1,x_2\dots x_n\mid\theta)=P(x_1\mid\theta)P(x_2\mid\theta)\dots P(x_n\mid\theta)
$$
诶，你发现化简到这种程度好像你也会求了，这个$P(x_1\mid\theta)$，不就是在已知模型参数为$\theta$的情况下求解$x_1$这个事件发生的概率吗，而我们又假设了身高是满足高斯分布的，那么这个概率不就是$P(x_1\mid\theta)=\int_{x_1-\Delta x}^{x_1}p(x)\mathrm{d}x$吗，其中$p$是$x$的概率密度函数即高斯分布满足$x\sim\mathcal{N}(\mu,\sigma)$

> 因为我们说过了$p(x)$表示的不是概率，因此我们不能直接使用$p(x_1)$表示出现$x_1$的概率

你说这个概率不是无限小吗，他都求不出来我写这有什么意义，这说明你一看就不太了解积分的实际含义，我们在刚刚学习积分的时候老师一定说过，积分的本质实际上就是把一个函数切分成一块块非常微小的矩阵块，然后计算他们的面积之和，因此由于我们求解的概率就是函数上无限小的一点的积分，因此上面的式子可以直接去掉所谓的积分号，写作$P(x_1\mid\theta)=p(x_1)\Delta x$，其中$\Delta x$表示$x$轴上的足够小的一段（其实你要直接写成$\mathrm{d}x$也完全没问题，本身$\mathrm{d}x$就有包含足够小量的意思），他们之积就是这足够小一段的面积，也就是我们希望求解的概率，于是，我们可以将每一项都带入最大后验估计的式子中
$$
\begin{align*}
\hat{\theta} &= \arg\underset{\theta}{\max}P(x_1,x_2\dots x_n\mid\theta)P(\theta)\\
&=\arg\underset{\theta}{\max}\prod_{i=1}^Np(x_i)\Delta xP(\theta)\\
&=\arg\underset{\theta}{\max}\left(\sum_{i=1}^N\log p(x_i)+\log P(\theta)\right)
\end{align*}
$$

> 由于$\Delta x$与求解$\theta$无关，因此可以略去

看到这条式子，你想到了什么？如果还没什么感觉，那么你试着把$\log P(\theta)$遮起来，然后你发现，蛙趣，剩下的部分怎么就是我们说过的极大似然估计的公式！没错，所谓最大后验估计的第一项$P(x_1,x_2\dots x_n\mid\theta)$，他有一个好听的名字——似然概率

#### 先验概率

听到现在你可能晕晕的，我们从尝试写出求解问题的公式表示这公式有个名字叫做最大后验估计，到去解释其中第一项的含义，然后莫名其妙怎么又跑到极大似然估计去了？先别着急，我们不是还有一项没解释呢，剩下的这项$P(\theta)$又是什么含义呢

先说结论，这个$P(\theta)$叫做先验概率，从直观视角来看，你会发现这玩意是不含$x$的，意味着这东西至少和我们目前拿到的样本没什么关系，为了通俗理解先验概率是个什么东西，我们同样拿似然一个身高模型举例

问题还是一样的，你现在有一堆身高样本想要拟合出一个高斯模型，你确实拿到了一堆身高样本，不过由于这堆身高样本数量可能实在过于庞大了，你没时间也没精力过一遍其中的数据到底有没有问题，你更不知道这堆身高样本是从哪里取出来的，于是自然的，你会担心这堆样本的质量，你会想万一这些身高全部来自全中国的排球篮球运动员，那么自然的这些身高拟合出来的高斯模型至少均值就会偏大，这显然是不符合全中国男人真实的身高高斯模型的，这可怎么办呢

不过你并不是没有办法，虽然你不知道这堆样本数据质量，但是你发现你旁边的朋友，家人都有高有矮，我们假设你可以确定你身边的人都是完全随机的，意味着假设所有的身高是一个高斯模型，至少你身边的这些人是从这个高斯模型中无偏差的随机抽样出来的，不过坏就坏在它们的数量又不够，你同样不能根据它们完全拟合出一个准确的高斯模型，你只能通过他们大概估计一下最后拟合出来的高斯模型的各个参数大概会在什么样的范围，以及落在什么值的概率比较大，仅此而已

没错，你通过过往的经验或者是对身边人的抽样调查，对于将要处理的数据有一个预先的印象，这个印象就可以叫做先验知识，将这个先验知识抽象成为限制模型参数的概率模型，我们就叫他先验概率

由于我们需要拟合的是一个高斯模型，因此我们需要知道$\mu,\sigma$两个参数，通过我们的调查，发现$\mu,\sigma$分别满足模型$q_1(\mu),q_2(\sigma)$（同样$q_1,q_2$也是两个概率密度函数，你当然可以规定他们也满足高斯分布，但是需要注意的是这两个模型的参数一定是确定的，因为这是你已知的先验信息），于是有如下式子
$$
\hat{\theta} = \arg\underset{\theta}{\max}\left(\sum_{i=1}^N\log p(x_i)+\log q(\theta)\right)
$$
这就是用最大后验概率求解身高问题的最终版本答案，现在，我们终于可以回过头来，问问自己，极大似然估计究竟是什么东西

我们发现极大似然估计其实就是最大后验估计去掉了先验概率，那么什么时候我们不需要考虑先验概率呢？一个明显的答案是我们没有任何先验信息的时候，没错，但更确切的是，我们并不是没有先验概率，而是先验概率的概率密度公式为均匀分布，即所有参数的取值概率相同，此时$q(\theta)=N$为一个常数，自然可以在求解$\arg\max$的时候去除掉了

> 至于为什么在没有先验信息的情况下默认使用均匀分布的公式，在此处先不做说明，之后会从熵的角度证明其正确性

结合我们在身高问题中假设的独立条件（即取第一个人的身高不会对第二个人的身高产生影响），终于，我们知道了，想要使用极大似然估计，必须满足两个条件——**样本相互独立和不存在先验信息**

#### 正则化

接下来我们回归本心，回到本章的最初的线性回归的问题上来，既然我们知道了线性回归的本质是极大似然估计，然而此时我们拥有了看上去比极大似然估计更强大的最大后验估计，是不是可以尝试使用最大后验估计再推导一遍损失函数呢？

我们预想一种最简单的情况，即求解的直线中的参数$W\sim\mathcal{N}(0,\sigma^2)$，他表示我们认为直线的斜率最可能是在零周围的一个数，即斜率的绝对值不会太大，你说凭什么呢我就想让斜率为一万有问题吗，当然没问题，只不过我之前没有提到的是，线性回归其实说到底没有任何卵用，因为你几乎找不出现实中有任何可以直接用一条直线就能预测的模型，所谓线性回归的最终形态要么是多项式回归，要么就直接是神经网络，而在这些模型当中，相信如果你学过的话，一个最终求解结果中的一个较大的斜率要么会导致过拟合，要么会产生恐怖的梯度爆炸

![欠拟合、过拟合及如何防止过拟合 - 知乎](https://pic1.zhimg.com/v2-117601802669511b2e77abcbd78ce414_r.jpg)

> 可以发现过拟合的拟合线斜率往往存在某些极端值，越平滑的曲线越不容易产生过拟合

不过要是你不知道神经网络是什么也没关系，总之我们就先假设我们拿到的先验概率就是$W\sim\mathcal{N}(0,\sigma_0^2)$即可，方便起见，我们还是采用一般的线性模型进行似然，那么接下来就是推导过程
$$
\begin{aligned}
\hat{W} & =\underset{W}{\operatorname{argmax}} \log (P(y \mid W) P(W)) \\
& =\underset{W}{\operatorname{argmax}} \log \left(\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(y-W^{T} X\right)^{2}}{2 \sigma^{2}}} \frac{1}{\sqrt{2 \pi} \sigma_{0}} e^{-\frac{\|W\|^{2}}{2 \sigma_{0}^{2}}} \right)\\
& =\underset{W}{\operatorname{argmax}}\left(-\frac{\left(y-W^{T} X\right)^{2}}{2 \sigma^{2}}-\frac{\|W\|^{2}}{2 \sigma_{0}^{2}}\right) \\
& =\underset{W}{\operatorname{argmin}}\left( \frac{\left(y-W^{T} X\right)^{2}}{2 \sigma^{2}}+\frac{\|W\|^{2}}{2 \sigma_{0}^{2}}\right) \\
& =\underset{W}{\operatorname{argmin}}\left( \sum_{i=1}^{N}\left(y_{i}-W^{T} x_{i}\right)^{2}+\frac{\sigma^{2}}{\sigma_{0}^{2}}\|W\|^{2}\right)
\end{aligned}
$$
会发现，我们推导出来的损失函数变成了一个均方误差加上一个带系数的参数$W$的平方项，需要注意这里的系数不能被消除，因为系数的大小会影响函数最大值时$W$的取值，如果你有印象，加的这一项其实就是大名鼎鼎的Weight Decay正则化项

> 关于这种方法的名字实在太多了，包括但不限于权重衰减，L2正则化，岭回归，Ridge正则化等等，不过总的来说，他作为一种著名的正则化方法一定是没有问题的

如果你感兴趣，完全可以假设一个更高维的多项式模型比如$W_1X^2+W_2X+b$，由于更高维的多项式影响其弯曲程度的往往是最高项次的系数，因此你可以假设$W_1\sim\mathcal{N}(0,\sigma_0^2)$，求出来的结果的正则化项和上面一定是一样的，而从结论上来说，我们在之前学习正则化的时候的目的，也正是为了缓解模型的过拟合问题，这与我们对参数的先验假设不谋而合

----

说了那么多，我们总结一下本章那么多内容所抽象出来的结论，他们每一个都十分重要

- **任何似然问题的求解过程都可以用三步概括——确定模型种类，确定评价函数，确定求解方法**
- **MLE其实就是样本取值独立且先验概率服从均匀分布的MAP**
- **LSE除了要满足MLE的条件之外，还额外假设了样本的取值存在一个围绕着求解函数的高斯噪声**
- **MAP是所有似然问题的最高层次抽象，它的函数主体可以化简成一个似然概率乘上一个先验概率**
- **Weight Decay方法是先验概率满足零均值的高斯分布的MAP，一般用以缓解模型可能产生的过拟合问题**

## 线性分类

在线性回归问题中，我们画了一条线用于拟合一堆样本点从而达到预测未知样本的目标，线性分类则是要画一条线去分割两堆不同的样本点，因此在这两个问题中，我们的模型种类都是直线模型，只不过在线性分类的过程中，我们显然不能通过假设样本点噪声呈高斯分布在直线两侧来构造评价函数，由于每个点都有自己的标签，分类直线的目的是为了尽可能明显的分割两堆样本点，因此我们需要寻找一个专属于线性分类的评价函数

如果说我们在上一章线性回归的问题中是为了寻找到评价函数为均方误差函数的合理性，那么在本章线性分类中就是需要寻找到sigmoid模型和交叉熵评价函数的合理性，不过现在先不用着急，同样的，我们还是得从线性分类的演化史开始讲起

首先我们得先定义几个分类问题中用得上的参数，通过几个参数也能够更直观的理解分类问题的解决内容

有$N$个在$p$维空间内的点集合$X=\{x_1,x_2\dots x_n\}$，其中$x_i$表示第$i$个数据点，是一个$p$长度的列向量，与每一个$x_i$匹配的是$N$个$y_i$集合$Y$，由于此处我们只考虑二分类问题，因此$y_i$不仅是一个标量而且仅能取到$0$或$1$，即$y_i \in \{0,1\}$，$(x_i,y_i)$表示第$i$个$x$和$y$的对应关系，同时由于我们定死了用一条线来分类的要求，因此只需要求解一个参数$w$，令某函数模型$f(w^TX)=Y$即可实现分类目标，其中$w$维度为$p\times 1$，也是一个向量，为了找到这个$w$，还需要有一个评价函数或者说是损失函数$L(f)$，通过最小化损失函数来找到最优参数$w$

一下子看到那么乱七八糟的参数一定十分摸不着头脑，接下来不妨用一个最傻瓜的解决分类问题的思路来看看这些参数在实际问题中大概能怎么用

### 感知机算法（PLA）

虽然我们之前一直说画线什么的，但是那只是在二维平面上便于理解的说辞罢了，实际上由于样本点的特征是多维的，因此无论是线性回归还是分类，所得到的所谓直线其实是高维空间下的一个超平面，想要用一个超平面分割两组数据点，其实就是让平面上方的点赋值$1$，下方的赋值$0$，用数学语言就是$f(x)=\mathrm{sign}(w^Tx)$，其中$\mathrm{sign}$叫符号函数同时也叫激活函数，当$w^Tx>0$的时候输出$1$，不然输出$0$，我们假设现在已经找到了这么一条完美的平面$w^Tx$，这时候无论遇到什么未知样本点$x_i$，我们只需要丢给$f$，他的输出就可以直接作为我们的$y_i$，这个分类模型是非常完美的

你说现在我们找到了一个完美的模型，为了获得这个模型的参数，下一步显然就是构造评价函数了，然而，以我们现在粗浅的知识角度，已知模型构造评价函数有且只有唯一一种方法——极大似然估计（此处就不考虑线性模型\+最小二乘估计这种情况了，毕竟LSE说到底也是MLE），然而极大似然估计的运行条件首先你就得确定出一个概率密度函数或者概率函数，至少也得把问题抽象成一个概率问题，而上面的符号函数显然不是一个什么和概率有关的东西

在做似然问题的过程中，我们应该时刻提醒自己所有的$x$都是已知的，我们的目标不是拿到了一个$x$如何获得$y$，而是有了那么多$x,y$要如何获得参数$\theta$，我们构造的评价函数的目标是为了评价参数$\theta$而不是评价样本$x$，首先应该满足的条件就是在带入样本之后输出值是一个参数（因此评价函数往往都会存在$\arg\max$之类的求解参数的过程）

不过也不是说没有极大似然估计我们就可以开摆了，我们可以从定义的角度强行构造一个评价函数，通过最小化函数结果寻找最佳值$w$
$$
\hat{w} = \arg\min_w\sum_{(x_i,y_i)}-(y_i-0.5)w^Tx_i
$$
这玩意的含义可比什么极大似然估计简单多了，它只是为了秉承着符号函数的意志，这个评价函数做的就是想尽可能让$y_i-0.5$和$w^Tx_i$同号，这样从另一个角度同样也满足了类似符号函数的定义，同时该函数可导意味着我们理论上同样可以求出它的解，但是它仍然还有问题，如果你真的打算做一做，会发现这玩意求导完是一个不含$w$的东西，说明在一组样本下他是没有最小值的，我们想一下也知道只要我$w$​向梯度方向增的越多，当然损失函数也会越小，为了能够获得一个较好的预期值，就需要我们多找几组样本进行重复实验，然后每组实验过后都根据结果稍微变一变$w$，这个过程其实非常类似梯度下降的思想

显然，这种所谓的感知机算法是有局限性的，不过他的局限性不在于预测函数$f$构造的不好，事实上这个$f$正是绝对理想情况下的理想结果，他的问题在于评价函数的构造有些过于刻意了以至于效果也肉眼可见的不佳，但是他提供的梯度下降方法为包括深度学习在内的复杂模型不易求参数解析解的情况下逼近极值提供了解决思路，而其理想情况下的符号函数作为区分函数则带来了另一个重量级的非线性分类器——SVM

### 矩阵求导

上一节说是说让你试试求导，但是相信大多数人对于矩阵的求导还是一脸懵逼的，于是我们得花一点时间来说一说矩阵求导法则

首先我们得知道什么矩阵可以被求导，我们知道一个求导首先需要一个函数和自变量，大概写成这个样子$\frac{\partial f}{\partial x}$

$x$是一个矩阵相信应该没什么问题，主要就是$f$会长什么样，同样是传入一个矩阵，常见的$f$可以分为两个大种类，分别是全元素标量函数和逐元素标量函数，其中全元素标量函数传入一个矩阵，函数会提取出其中的所有元素进行计算得到一个标量值，比如$f(X)=a^TXb$，逐元素标量函数同样是传入一个矩阵，函数会对其中每一个元素进行同样的操作得到一个矩阵，比如$f(X)=\sin(X)=[\sin (X_{ij})]$

接下来就可以看一些直接可用的求导公式了，我们令$x_i$为标量，$x$为列向量，$X$为矩阵，$\mathrm {tr}  (X)$表示计算$X$的迹（矩阵对角元素相加和），$f$为全元素标量函数，$\sigma$表示逐元素标量函数，$\mathrm dX$表示$X$的全微分，$\odot$表示逐元素相乘，和$X$形状一致，基于这几个元素，有以下几个恒成立公式
$$
\begin{align}
x_i &= \mathrm {tr}(x_i)\tag1\\
\mathrm {tr}(x_1A+x_2B) &= x_1\mathrm {tr}(A)+x_2\mathrm {tr}(B)\tag2\\
\mathrm {tr}(X)&=\mathrm {tr}(X^T)\tag3\\
\mathrm {tr}(ABC)&=\mathrm {tr}(CAB)=\mathrm {tr}(BCA)\tag4\\
\mathrm {d}(AB)&=\mathrm {d}AB+A\mathrm {d}B\tag5\\
\mathrm {d}(X^T)&=(\mathrm {d}X)^T\tag6\\
\mathrm {d}f(X)&=\mathrm {tr}(\frac{\partial f(X)}{\partial X^T}\mathrm {d}X)\tag7\\
\frac{\partial f(X)}{\partial X^T}&=(\frac{\partial f(X)}{\partial X})^T\tag8\\
\mathrm {d}(AXB)&=A\mathrm {d}XB\tag9\\
\mathrm {d}|X|&=|X|\mathrm {tr}(X^{-1}\mathrm {d}X)\tag{10}\\
\mathrm {d}X^{-1}&=-X^{-1}\mathrm {d}XX^{-1}\tag{11}\\
\mathrm {d}(A\odot B)&=\mathrm {d}A\odot B+A\odot \mathrm {d}B\tag{12}\\
\mathrm {d}\sigma(X)&=\sigma'(X)\odot \mathrm {d}X\tag{13}\\
A^T(B\odot C)&=(A\odot B)^TC\tag{14}
\end{align}
$$

根据这些公式，我们不妨来求一求$f=a^TXX^Tb$关于$X$的导数

首先确认$f$是一个全元素标量函数，因为如果$X$的形状为$m\times m$则$a,b$形状一定是$m\times 1$的，最后结果是$1\times 1$的矩阵，即为标量，然后确认我们需要求解的结果是$\frac{\partial f(X)}{\partial X}$也是一个标量，之后开始求解
$$
\begin{align*}
(1)&\Rightarrow& \mathrm{d} f(X)&=\mathrm {tr}(\mathrm {d}f(X))=\mathrm {tr}(\mathrm {d}(a^TXX^Tb))\\
(9)&\Rightarrow&&=\mathrm {tr}(a^T\mathrm {d}(XX^T)b)\\
(5)&\Rightarrow&&=\mathrm {tr}(a^T(\mathrm {d}XX^T+X\mathrm {d}X^T)b)\\
(2)&\Rightarrow&&=\mathrm {tr}(a^T\mathrm {d}(X)X^Tb)+\mathrm {tr}(a^TX\mathrm {d}(X^T)b)\\
(6)&\Rightarrow&&=\mathrm {tr}(a^T\mathrm {d}(X)X^Tb)+\mathrm {tr}(a^TX(\mathrm {d}X)^Tb)\\
(4)&\Rightarrow&&=\mathrm {tr}(X^Tba^T\mathrm {d}X)+\mathrm {tr}((\mathrm {d}X)^Tba^TX)\\
(3)&\Rightarrow&&=\mathrm {tr}(X^Tba^T\mathrm {d}X)+\mathrm {tr}(X^Tab^T\mathrm {d}X)\\
(2)&\Rightarrow&&=\mathrm {tr}((X^Tba^T+X^Tab^T)\mathrm {d}X)\\
(7)&\Rightarrow& \frac{\partial f(X)}{\partial X^T}&=X^Tba^T+X^Tab^T\\
(8)&\Rightarrow&\frac{\partial f(X)}{\partial X}&=(\frac{\partial f(X)}{\partial X^T})^T=ab^TX+ba^TX
\end{align*}
$$
我们发现了，利用迹求解矩阵导数的要点就是通过取标量函数全微分的迹，通过分离$\mathrm dX$，构造出形如$(7)$一致的全微分和导数转换的形式，便可容易求得矩阵的导数结果

### 线性判别分析（LDA）

既然直接用符号函数区分样本无法求解超平面，研究者们开始了新一轮的尝试，这次他们决定直接跳过构造激活函数，用一种名为降维的方法，直接从损失函数的角度下手解决分类问题

这种方法的思想也十分简单，在上一节中我们是要找一个超平面（在样本点为二维时则为直线）分割两组数据点，这次我们不再拘泥于这个平面，而是实实在在的构造一条新的过原点的直线（注意这回无论是高维还是低维都是直线），然后我们让所有的数据点全部投影到这条直线上，接着我们只关注两组数据点在这条直线上的投影点，然后改变直线的参数$w$即斜率，看看哪条直线对应的投影点能够将两组数据点分得更开，我们就使用这条直线

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201121151154839.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0NzY2ODgz,size_16,color_FFFFFF,t_70#pic_center)

> 空间内相同的两组红蓝数据点，明显第二条的投影点更能够区分数据，此处只画了二维情况，高维同理

这条过原点的直线当然不是为了分割数据点，但是这种另辟蹊径的方法居然看上去还挺有效果（至少在这个二维图上），而想要求解这条投影直线，我们只需要解决两个问题，一是如何找到投影点，二是如何评价投影点的分布

对于投影点实际上非常简单，我们用幼儿园就学过的向量点积即可解决，我们令所求投影直线方向上的单位向量为$w$，接着取每一个样本点向量$x_i$与其做点乘$w^Tx_i$，向量的点乘结果即为样本点向量在这条直线上的投影距离，假如我们取这条投影直线为坐标轴，则投影距离即可转换为投影坐标，$w^TX$将会产生一条向量，其中每一维度的值就是一个样本点在直线上的投影，于是我们可以构造出一个映射$(w^Tx_i,y_i)$，表示这条向量的每一个值对应的类别$y_i$

![img](https://s2.loli.net/2023/02/15/5BZFm2QlXT1Dn4b.png)

如何评价这条直线的好坏呢，我们挑出映射当中所有$y_i=0$的点$w^Tx_i$（例如上图的红点）组成一个向量，记为$u_0$，同时记其中有$N_0$个元素，同理挑出另一个向量（蓝色的点）组成$u_1$，其中有$N_1$个元素，接着我们构造损失函数如下
$$
L(w)=\frac{(\bar{u}_0-\bar{u}_1)^2}{\Sigma_{u_0}+\Sigma_{u_1}}\\
\bar{u}_0=\frac{1}{N_0}\sum^{N_0}_{i=1}u_0,\bar{u}_1=\frac{1}{N_1}\sum^{N_1}_{i=1}u_1\\
\Sigma_{u_0}=\frac{1}{N_0}\sum^{N_0}_{i=1}(u_{0_i}-\bar{u}_0)(u_{0_i}-\bar{u}_0)^T\\
\Sigma_{u_1}=\frac{1}{N_1}\sum^{N_1}_{i=1}(u_{1_i}-\bar{u}_1)(u_{1_i}-\bar{u}_1)^T
$$
这个损失函数的意义就是让同类点之间的方差$\Sigma$尽量小，令异类点之间的均值距离$|\bar{u}_0-\bar{u}_1|$尽量大，这也是符合我们直觉的，更满意的是这个函数是可导的，意味着我们可以通过求导的方式获得这条投影直线的最优解，将参数和$u$都用$w^Tx$代换，可以化简出以下式子
$$
L(w)=\frac{w^T(\bar{x}_0-\bar{x}_1)(\bar{x}_0-\bar{x}_1)^Tw}{w^T(\Sigma_{x_0}+\Sigma_{x_1})w}
$$
其中$\bar{x}_0$指$X$里的所有输出值$y_i=0$的样本点（向量）的均值，$\Sigma_{x_0}$则为方差，$\bar{x}_1,\Sigma_{x_1}$同理，这个式子分子分母计算结果都是标量，因此可以写成分数形式，但是千万不要把上下的$w^T,w$约掉了，因为其虽然写成这样但是其本质还是分子矩阵乘分母矩阵的逆

由于分子和分母的中间项和$w$都没什么关系，我们令$(\bar{x}_0-\bar{x}_1)(\bar{x}_0-\bar{x}_1)^T=\Sigma_b,(\Sigma_{x_0}+\Sigma_{x_1})=\Sigma_w$，然后我们可以对其进行求导，尝试求解他的解析解，同时我们应该注意我们希望求解$w$的方向而非大小，因此对于其中的常系数部分我们可以用$C$代替
$$
\begin{align*}
\frac{\partial L(w)}{\partial w}=&(w^T\Sigma_bw)(w^T\Sigma_ww)^{-1}=0\\
&2\Sigma_bw\cdot(w^T\Sigma_ww)^{-1}+(w^T\Sigma_bw)\cdot(-1)(w^T\Sigma_ww)^{-2}\cdot2\Sigma_ww=0\\
&\Sigma_bw-(w^T\Sigma_bw)\cdot(w^T\Sigma_ww)^{-1}\cdot\Sigma_ww=0\\
&(\bar{x}_0-\bar{x}_1)(\bar{x}_0-\bar{x}_1)^Tw-C(\Sigma_{x_0}+\Sigma_{x_1})w=0\\
&(\bar{x}_0-\bar{x}_1)C=C(\Sigma_{x_0}+\Sigma_{x_1})w\\
&w=C(\Sigma_{x_0}+\Sigma_{x_1})^{-1}(\bar{x}_0-\bar{x}_1)
\end{align*}
$$

其中我们遇到了对于$w^T\Sigma_bw$的求导，我们令$g=w^T\Sigma_bw$，可得$g$是一个全元素标量函数，则他的详细求导过程如下
$$
\begin{align*}
\mathrm{d}g&=\mathrm{tr}(\mathrm{d}(w^T\Sigma_bw))\\
&=\mathrm{tr}(\mathrm{d}(w^T)\Sigma_bw)+\mathrm{tr}(w^T\Sigma_b\mathrm{d}w)\\
&=\mathrm{tr}(w^T(\Sigma_b)^T\mathrm{d}w)+\mathrm{tr}(w^T\Sigma_b\mathrm{d}w)\\
\Rightarrow \frac{\partial g}{\partial w}&=(\Sigma_b+(\Sigma_b)^T)w^T
\end{align*}
$$
又因为$\Sigma_b$是协方差矩阵，对角对称因此$\Sigma_b=(\Sigma_b)^T$，$\frac{\partial g}{\partial w}=2\Sigma_bw^T$得到推导的结果，于是，我们得到了只需要让$w$和$(\Sigma_{x_0}+\Sigma_{x_1})^{-1}(\bar{x}_0-\bar{x}_1)$同方向，就可以得到我们理想当中的投影向量

传统意义上的LDA到这就结束了，我们确实找到了一条向量$w$可以满足理论上的降维和分类要求，但是感觉好像缺了什么东西，没错，LDA只提供了求解损失函数的思路，但是没告诉我们分类的界限在哪里，也就是激活函数是什么，我们的确拿到了一个向量和在这个向量上的一堆投影点，但是此时如果再来一个点我们并没有一个判断标准去衡量这个点的输出分类结果

不过这并不是什么难事，我们不妨把投影向量和投影点看作是另一个样本模型，这回我们从需要在高维寻找一个超平面变成了在一条直线上寻找分离两组数据的分界点，这也是降维操作的核心

综上所述，**LDA就是让若干组高维的样本点能够在最有区分度的前提下投射到一条直线上的过程**

### 高斯判别分析（GDA）

那么如何找到这个分界点呢，聪明的人可能会有一个想法，我是否可以把一条直线上的两组数据看作是由两个一维高斯生成的点集呢，接着遇到一个新的点，我们只需要把这个点带入两个解出的高斯模型，看看它更可能由哪个高斯模型生成我们便判断他的类别

![img](https://s2.loli.net/2023/02/16/5IwNtoxkKpChqHQ.png)

例如上图我们根据橙蓝两组样本求解出了两个高斯分布模型（蓝线和橙线），取其交点的$x$坐标即为分界点，之后再来一个样本点，若落在绿线的左边，我们则认为他是蓝色这一组，落在绿线右边则认为是橙色一组，没错，这就是著名的高斯混合模型（GMM）的雏形了，不过GDA中并没有完全使用GMM的假设，其中有一个根本原因在之后会提到，另外一个原因其实是源于对于上面图像的理解所建立的假设

这个假设的内容也十分容易理解，我们发现上述的两个高斯模型似乎蓝色的模型好像明显肥一点，GDA认为所有样本都是地位相等的，那么肥一点的高斯模型在之后的预测任务中也理所当然会占优一点，而控制胖瘦的主要原因还是高斯模型中的$\Sigma$方差一项，为了保证两个求解模型的公平，这个假设条件就是令两个模型的方差相等

那么接下来的问题就在于怎么求解这两个模型了，我们发现，这个评价函数的主体正是一个概率密度函数，再一看，对于分类模型来说，样本点肯定也是保证独立分布的，而我们同样没有什么有效的先验信息，极大似然估计的所有条件都满足了，此时不用更待何时！

同时既然我们也学过了多维高斯分布了，那又为什么非要降维了以后求一维的高斯呢，我们不妨把原有的样本点集合$X$就看作是两个高维高斯模型抽取出来点集合，直接求解这两个高维高斯模型不就好了

这种思想就是GDA，他假设空间内的每一组样本点均服从同方差的高斯分布，求解出高斯模型之后根据未知点在哪个高斯模型被筛选出来的概率高来判断未知点的分类结果

我们不妨复习一下MLE的公式，他往往长这样$\hat{\theta} = \arg\underset{\theta}{\max} P(x\mid \theta)=\arg\underset{\theta}{\max}\log\left(\prod_{i=1}^{N}P(x_i\mid \theta)\right)$，唯一需要动点脑子的就是其中的似然概率$P(x\mid \theta)$也就是如何用一条式子表示他的模型，在GDA中，我们令他为$P((x_i,y_i)\mid (\mu_0,\mu_1,\Sigma))=\mathcal{N}(\mu_0,\Sigma)^{1-y_i}\cdot\mathcal{N}(\mu_1,\Sigma)^{y_i}$，注意到由于存在两个模型，因此在已知参数的情况下为了获取某一样本点的概率，我们不止要传给似然函数样本点的数据，还需要给他的种类信息，他才能给我们返回正确的概率值，这也是符合常理的（如果你了解过GMM，会发现在模型的表达式上GDA和GMM是不一样的）

接下来就是对其进行求导了，我们从简单的入手，先看看$\mu_0,\mu_1$怎么求解，其实我们不求也能大致猜到他们的最优值应该就是每一组分布点的均值向量，求解只是进一步证实我们的假设，和上一节类似，其中的$N_0,N_1$分别代表两种类别点的个数$N_0+N_1=N$，$C$表示和所求内容无关的常量，接下来以求解$\hat\mu_0$为例
$$
\begin{align*}
\arg\underset{\theta}{\max}\log\left(\prod_{i=1}^{N}P(x_i\mid \theta)\right)&=\arg\underset{\theta}{\max}\sum_{i=1}^{N}\left(\log \mathcal{N}\left(\mu_{0}, \Sigma\right)^{1-y_{i}}+\log \mathcal{N}\left(\mu_{1}, \Sigma\right)^{y_{i}}\right)\\
&=\arg\underset{\theta}{\max}\sum_{i=1}^{N}(1-y_i)\log\left(C e^{-\frac{1}{2}(x_i-\mu_0)^{T} \Sigma^{-1}(x_i-\mu_0)}+C\right)\\
&=\arg\underset{\theta}{\max}\sum_{i=1}^{N_0}(x_i-\mu_0)^{T} \Sigma^{-1}(x_i-\mu_0)
\end{align*}
$$
将$(x_i-\mu_0)^{T} \Sigma^{-1}(x_i-\mu_0)$求导后易得$\hat\mu_0=\sum_{i=1}^{N_0}x_i$的时候取极值，同理可得$\hat\mu_1=\sum_{i=1}^{N_1}x_i$，代表的正是两组样本的统计均值，符合我们猜测，然后我们来求一下难一些的$\Sigma$，已知化简后$\sum_{i=1}^{N_0}\log \mathcal{N}(\mu_{0}, \Sigma)+\sum_{i=1}^{N_1}\log \mathcal{N}\left(\mu_{1}, \Sigma\right)$，我们令$g=\sum_{i=1}^{N}\log \mathcal{N}(\mu, \Sigma)$，求其关于$\Sigma$的导数
$$
\begin{align*}
\mathrm{d}g&=\mathrm{d}\left(\sum_{i=1}^{N}\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)}\right)\\
&=\mathrm{d}\left(C+\sum_{i=1}^{N}\log|\Sigma|^{-\frac{1}{2}}-\sum_{i=1}^{N}\frac{1}{2}(x_i-\mu)^{T} \Sigma^{-1}(x_i-\mu)\right)\\
&=-\frac{1}{2}\mathrm{d}(N\log|\Sigma|)-\frac{1}{2}\mathrm{d}\left(\mathrm{tr}\left(\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^{T} \Sigma^{-1}\right)\right)\\
&=-\frac{N}{2}\frac{1}{|\Sigma|}|\Sigma|\mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)-\frac{1}{2}\mathrm{tr}\left(\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^{T}\mathrm{d}\Sigma^{-1}\right)\\
&=-\frac{N}{2}\mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)-\frac{1}{2}\mathrm{tr}(N\Sigma_x(-\Sigma^{-1})\mathrm {d}\Sigma\Sigma^{-1})\\
\Rightarrow\frac{\partial g}{\partial\Sigma}&=-\frac{N}{2}\left(\Sigma^{-1}-\Sigma^{-1}\Sigma_x^T\Sigma^{-1}\right)=\frac{N}{2}\left(\Sigma_x^T\Sigma^{-2}-\Sigma^{-1}\right)
\end{align*}
$$

> 注意$\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^{T}=\Sigma_x,\Sigma\neq\Sigma_x$，$\Sigma$和$\Sigma_x$的形状都是$p\times p$，且均为对称矩阵，因此可交换，同时其中使用到了行列式和逆矩阵的求导结果，使用了矩阵求导一章中的$(10)(11)$定理，可以对照着看

将求解结果带入原式，令导数为$0$，则可得到$\hat\Sigma$
$$
\begin{align*}
\frac{\partial\log\left(\prod_{i=1}^{N}P(x_i\mid \theta)\right)}{\partial\Sigma}=\frac{N_0}{2}\left(\Sigma_0^T\hat\Sigma^{-2}-\hat\Sigma^{-1}\right)+\frac{N_1}{2}\left(\Sigma_1^T\hat\Sigma^{-2}-\hat\Sigma^{-1}\right)&=0\\
\hat\Sigma&=\frac{1}{N}(N_0\Sigma_0^T+N_1\Sigma_1^T)
\end{align*}
$$
于是我们得出了模型三个参数的解析解，他们分别是$\hat\mu_0=\sum_{i=1}^{N_0}x_i,\hat\mu_1=\sum_{i=1}^{N_1}x_i,\hat\Sigma=\frac{1}{N}(N_0\Sigma_0^T+N_1\Sigma_1^T)$，如果感觉这样看上去有点抽象，那么结合到实际的问题中，意思就是现在我们有一堆样本点集$X$和其对应的分类结果$Y$，接着我们把$X$中同类的样本点分别取出来数一数他们的个数$N_0,N_1$，并求他们的均值$\mu_0,\mu_1$和方差$\Sigma_0,\Sigma_1$，均值部分我们直接拿来用，方差则把他根据种类点的个数加个权$\frac{1}{N}(N_0\Sigma_0^T+N_1\Sigma_1^T)$产生新的统一方差$\Sigma$，这样我们就构造出了两个高斯模型$\mathcal{N}(\mu_0,\Sigma),\mathcal{N}(\mu_1,\Sigma)$，之后我们碰到了一个新的样本点，就把他丢到这两个模型里面看看输出，哪个大就说明这个样本属于哪个分类

我们总结一下，**GDA就是假定分类样本遵循同方差高斯分布的前提下，求解各自高斯模型的过程**

### 逻辑回归

不过我们手动把样本代到两个模型再人力判断哪个大显然还是比较麻烦，有没有什么可以像一开始的符号函数一样一条算式表示出结果分类呢

我们首先得意识到我们求出来的两个高斯模型的含义是什么，没错，就是$P(x\mid y_0),P(x\mid y_1)$，意思就是我已知某一组样本是什么种类了，我们把样本点$x$带入模型中，就可以得到这个样本点属于这个种类的概率，但是我们的目标其实是需要当我知道$x$之后其属于某一分类$y$的概率也就是$P(y_0\mid x)$才对

听起来似乎有点绕，那么我们可以假设一个样本点$x$，我们把他带入两个模型的过程就相当于求解了这个样本点在两个模型下的概率$P(x\mid y_0),P(x\mid y_1)$，比如概率一个是$0.9$一个是$0.8$，于是我们通过比较猜想$x$属于第一个模型，由于两个模型输出的结果是不相关的，因此必须要求我们求解两次模型结果，但是如果模型是$P(y_0\mid x)$，因为只考虑二分类问题，有$P(y_0\mid x)+P(y_1\mid x)=1$，那么我们只需要任取一个模型比如$P(y_0\mid x)$，通过传给他$x$就可以获知属于该分类的概率，由于只有属于这个分类和属于另一个分类两种情况，因此只需要判断其输出是否大于$0.5$即可知道该点应该被如何分类，肉眼可见的减少了一个模型的计算量

所以如何把$P(x\mid y_0)$变成$P(y_0\mid x)$，这就需要借助我们强大的贝叶斯公式了
$$
P(y_0\mid x)=\frac{P(x\mid y_0)P(y_0)}{P(x\mid y_0)P(y_0)+P(x\mid y_1)P(y_1)}
$$
其中$P(y_0),P(y_1)$我们之前称之为先验概率，此处其实也可以这么理解，由于所求的$P(y_0\mid x)$是一个预测模型，我们得根据已有的样本来判断两种种类的个数分布情况，比如简单一点我们拿到的样本$X$里$N_0=N_1$，意味着我们假设获取到的样本是服从等分布的，之后我们就可以计算一下$P(y_0\mid x)$的结果是什么样的
$$
P(y_0\mid x)=\frac{\mathcal{N}(\mu_0,\Sigma)}{\mathcal{N}(\mu_0,\Sigma)+\mathcal{N}(\mu_1,\Sigma)}=\frac{1}{1+\frac{\mathcal{N}(\mu_1,\Sigma)}{\mathcal{N}(\mu_0,\Sigma)}}=\frac{1}{1+e^{-z}}\\
z=\log\frac{\mathcal{N}(\mu_0,\Sigma)}{\mathcal{N}(\mu_1,\Sigma)}
$$
其中$P(y_0\mid x)$的化简结果$\frac{1}{1+e^{-z}}$就是大名鼎鼎的Sigmoid函数，我们也知道了原来Sigmoid是由贝叶斯公式变过来的，但是由于这条式子里还是需要计算两遍模型结果，好像和传说中的逻辑回归还差点什么东西，这时候就需要我们尝试化简一下其中$z$部分了
$$
\begin{align*}
z &= \log\left(\frac{\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_0)^{T} \Sigma^{-1}(x-\mu_0)}}{\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_1)^{T} \Sigma^{-1}(x-\mu_1)}}\right)\\
&=-\frac{1}{2}\left((x-\mu_0)^{T} \Sigma^{-1}(x-\mu_0)-(x-\mu_1)^{T} \Sigma^{-1}(x-\mu_1)\right)\\
&=-\frac{1}{2}x^T\Sigma^{-1}x+\mu_0^T\Sigma^{-1}x-\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0+\frac{1}{2}x^T\Sigma^{-1}x-\mu_1^T\Sigma^{-1}x+\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1\\
&=\left(\mu_0^T\Sigma^{-1}-\mu_1^T\Sigma^{-1}\right)x+\left(\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1-\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0\right)\\
&=w^Tx+b
\end{align*}
$$
我们发现了一个让我们惊掉下巴的事实，这个$z$居然是一个线性方程，说好的高斯模型呢？我那么大个的两个高斯模型跑哪去了？没错，高斯模型确实消失了，不然为什么说逻辑回归牛逼呢，实际上对于两个方差一致的高斯模型来说，无论其是几维模型，模型的接触面都一定是一个超平面，而这一条线性方程所描述的正是这条高斯模型的接触面，而这个面正是在两个高斯模型下分割分类点的最优超平面

不止如此，我们之前一直假定两种种类点的分布是符合高斯分布的，实际上谁说点的分布一定是高斯分布呢，我们发现，在这个线性方程中，如果把$b$看作$w^T$的一部分，实际上我们从需要求解$\mu_0,\mu_1,\Sigma$三个参数变成了只需要求解一个参数$w^T$，虽然可能这个参数的解析解算式比起那三个参数要复杂，但是至少他是一个参数，意味着Sigmoid的存在给了我们一种可能，可以让我们不止跳过代入两个模型比较解答的过程，更可以让我们跳过求解$\mu_0,\mu_1,\Sigma$参数的过程

理论存在，我们不如尝试实践看看，在求解$\mu_0,\mu_1,\Sigma$三个参数的时候，我们曾经构造出了一条$P(x\mid \theta)=\mathcal{N}(\mu_0,\Sigma)^{1-y_i}\cdot\mathcal{N}(\mu_1,\Sigma)^{y_i}$的算式，如今我们依葫芦画瓢，令Sigmoid函数为$\sigma(w^Tx)$，则构造$P(x\mid \theta)=\sigma(w^Tx)^{1-y_i}\cdot(1-\sigma(w^Tx))^{y_i}$，接着同样通过MLE尝试求解解析解$\hat w$
$$
\begin{align*}
\hat w&=\arg\underset{\theta}{\max}\log\left(\prod_{i=1}^{N}P(x\mid \theta)\right)\\
&=\arg\underset{\theta}{\max}\sum^N_{i=1}\log\left(\sigma(w^Tx_i)^{1-y_i}\cdot(1-\sigma(w^Tx_i))^{y_i}\right)\\
&=\arg\underset{\theta}{\max}\sum^N_{i=1}\left((1-y_i)\log\sigma(w^Tx_i)+y_i\log(1-\sigma(w^Tx_i))\right)
\end {align*}
$$
在这边我们不妨先暂停一下，虽然还没解完，但是到此为止其实已经证明了$\hat w$的可解性，之所以在此停顿，是为了顺便引出另一个重量级公式$\sum^N_{i=1}\left((1-y_i)\log\sigma(w^Tx_i)+y_i\log(1-\sigma(w^Tx_i))\right)$，交叉熵公式，比起直接令其导数为零解出其解析解，因为其非线性的特征（例如我很难直接得出$\sigma(x)=0.6$的时候$x$应该取多少），我们更喜欢使用梯度下降的方法求解其极值，再往后，就得谈及深度学习了

回过头来，我们确实把三个参数变成了一个参数，但毕竟求解的参数减少了，难道对模型就一点影响都没有吗？并不然，参数的减少实际上不再把点约束在高斯分布之内，而是涵盖了所有化简后会产生此类结果的模型，因此此处参数的减少不仅不会对求解精度造成损失，而且还做到了缓解过拟合的作用，令模型的泛化能力更加出色

说了那么多，我们按照惯例做一个总结，**如果对比线性回归中我们采取的是线性模型+均方误差评价函数，那么逻辑回归就是sigmoid模型+交叉熵评价函数，而其推导本质则是在GDA的基础上，泛化高斯模型之后化简得出的**

## 降维

在机器学习中降维是一个非常重要的思想，在LDA中我们初窥降维的用途，我们用它提供了一种高维分类的方法，实际上，LDA只能算是降维方法一种比较粗浅的应用，降维的主要应用还是集中在提取特征方面，一方面合理化的降维可以去除一组高维数据中的可能冗余特征保证提取特征的精度，另一方面在进行特征提取的时候，合理化的降维，制作可视化样本分布图也可以帮助我们提取正确的特征，提高分类工作的成功率

也就是说这一章，我们不再针对一个明确的似然问题，也不会再提到有关模型，评价和求解方面的新内容，而是着眼于样本的预处理和后处理，对于样本的处理同样是在进行似然问题之前需要进行的重要的准备工作

### 拉格朗日乘子法（等式约束）

求极值我们都很熟悉，但是在这一节，我们需要探讨一下如何求解约束条件下的极值问题

那么什么是约束条件下的极值问题，这是一个典型的约束条件的极值问题：在保证$x^2y=3$的情况下，求$x^2+y^2$的最小值，如果用数学语言表达，就是这样
$$
\begin{eqnarray}
&&\min(x^2+y^2)\\
&&s.t.\quad x^2y=3
\end{eqnarray}
$$
你说这还不简单，我把约束条件的$y$用$x$来表示，接着代换回求解的函数，然后求导求解最小值不就完了，这种思想就是求解约束条件下极值问题的一种解决思路——消元法，但是其中无论是讨论$x,y$是否为$0$的情况，还是约束条件到底方不方便让一个变量由另一个变量表示都是十分随机的，那么是否存在一种无脑一些的方法可以不用考虑那么多直接求解呢

这就需要我们换一个角度理解这个问题，我们令$f(x,y)=x^2+y^2,g(x,y)=x^2y$，接着先来看求解函数，$x^2+y^2$实际上就是在三维空间的一条曲面，且当没有约束条件的情况下，我们易得$x=0,y=0$时取得曲面在$z$方向时的最小值$0$，然后再来看约束条件，$x^2y=3$实际上是规定了$x,y$的取值范围，本来$g(x,y)$也是一个曲面，但是由于对其沿水平方向进行了切割，所得的刚好就是$g(x,y)$在$z=3$上的等高线，因此，我们要保证取值点的$x,y$落在$g(x,y)=3$这条等高线的前提下，使$f(x,y)$的取值最小

如果有点晕，那么可以看看以下这张图

![img](https://s2.loli.net/2023/02/22/yEwhe9rAi3DKWv8.png)

知道了求解目标的几何意义，对我们求解函数极值有什么帮助呢，由于$g(x,y)=3$看作是一个平面上的线，因此我们可以尝试把$f(x,y)$也投影到平面上，同时为了保留其大小的含义我们也对其添加等高线，等高线上的各点函数值相同

由于等高线有无数条，从直觉上来说，我们也应该知道想要取得极值，必须保证$g(x,y)=3$的线和某条等高线仅有一个交点，而这不就是相切的定义吗，接着我们再用一下幼儿园就学过的两条曲线相切的性质，我们发现其中一定有一条“交点处的切线斜率相同”的定义，引申出来即为相切曲线共用一条切线，而我们又有定义等高线的切线与其梯度垂直的定义，这不巧了，既然两条都是等高线，那我是不是也可以说，当$f(x,y)$在$g(x,y)=n$约束下取到极值时，极值点的梯度同向，即$\Delta f(x,y)=\lambda\Delta g(x,y),\lambda\in\mathbb{R}$

![img](https://s2.loli.net/2023/02/22/qhOMRbxvaoVsIH3.png)

> 上图紫线即为$f,g$在它们等高线的切点$A$的梯度方向，不过究竟梯度是向圆心还是向外取决于$f,g$三维图像函数值相对较高点，对于这个问题来说$x,y$越大$f$取值越大，因此梯度方向指向圆心反方向，同理可得$g$梯度方向和$f$同向
>
> 需要注意梯度向量虽然指向函数值较大方向，但是它是二维的，不存在$z$轴上的分量，只恒落在$x,y$轴组成的等高线平面上

到此为止，求解$f(x)$的极值我们就有了两个条件，根据它们我们可以构建方程组
$$
\begin{cases}
\Delta f(x,y)=\lambda\Delta g(x,y)\\
g(x,y)=3
\end{cases}\Rightarrow
\begin{cases}
\frac{\partial f}{\partial x}-\lambda\frac{\partial g}{\partial x}=0\\
\frac{\partial f}{\partial y}-\lambda\frac{\partial g}{\partial y}=0\\
x^2y-3=0
\end{cases}
$$
其中求解$f,g$梯度需要分别分为求解$x$方向偏导梯度和$y$方向梯度两条式子，三条式子求解三个变量明显是可解的，这样，我们就把一个有约束的极值问题转换为了固定的解方程问题

这种方法是完全正确的，不过有些人觉得这么写还是不够优雅，为了让求解过程变得足够直观，额外又构造了一个函数$\mathcal{L}(x,y,\lambda)=f-\lambda g',g'=g-3$，如果我们把$\mathcal{L}$看作是一个关于$x,y,\lambda$的无约束的优化问题，直接通过求偏导等于$0$的方式求解，就会发现其偏导结果正好对应了上面的三条式子
$$
\mathcal{L}'=0\Rightarrow
\begin{cases}
\frac{\partial L}{\partial x}=\frac{\partial f}{\partial x}-\lambda\frac{\partial g}{\partial x}=0\\
\frac{\partial L}{\partial y}=\frac{\partial f}{\partial y}-\lambda\frac{\partial g}{\partial y}=0\\
\frac{\partial L}{\partial \lambda}=-(g-3)=0
\end{cases}
$$
到此为止，我们终于把一个有约束的优化问题转化为了无约束的优化问题，这个方法同样适用于更高维的$f,g$，总结一下，**仅从做题结果来说，使用拉格朗日乘子法求解等式约束的优化问题分为如下四步**

1. **构造求解函数$f$和约束条件函数$g$，且应当把约束条件转换为$g=0$的形式**
2. **定义拉格朗日乘子$\lambda$，并构造函数$\mathcal{L}=f-\lambda g$**
3. **将$f,g$替换为多维的自变量函数，并对每一维变量和$\lambda$分别求偏导，令偏导数为$0$**
4. **联立求解多元方程组，获得的自变量取值即为所需极值（如果有多个，说明存在多个极大和极小值，分别带入$f$验证即可）**

### Centering Matrix

在线性分类的讨论中，我们曾经令所有样本点的排列为$X=(x_1,x_2\dots x_N)$，但是无论是在求解均值还是方差的过程中我们似乎都没有用到$X$，由于从小老师就只教了一个个$x_i$加起来然后除个数的算法因此这么做也是合理的，但是在程序中，我们往往用一个矩阵$X$表示所有的样本点，如果按照我们的方法就得把$X$先拆了再求，那还不如不构造$X$，是否有一种在保留$X$的情况下直接求解均值和方差的做法呢

我们不妨试一试，注意假设所有样本点$x_i$都是列向量
$$
\begin{align*}
\bar{X}_{p\times 1}&=\frac{1}{N}\sum_{i=1}^{N}x_i\\
&=\frac{1}{N}\begin{pmatrix}
 x_1 & x_2 & \cdots & x_N
\end{pmatrix}\begin{pmatrix}
 1 \\ 1 \\ \vdots \\ 1
\end{pmatrix}\\
&=\frac{1}{N}X\mathbb{1}_N
\end{align*}
$$

$$
\begin{align*}
\Sigma_{p \times p} 
&=\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}-\bar{X}\right)\left(x_{i}-\bar{X}\right)^{T} \\

& =\frac{1}{N}
\begin{pmatrix}
    x_{1}-\bar{X} & x_{2}-\bar{X} & \cdots & x_{N}-\bar{X}
\end{pmatrix}
\begin{pmatrix}
    \left(x_{1}-\bar{X}\right)^{T} \\
    \left(x_{2}-\bar{X}\right)^{T} \\
    \vdots \\
    \left(x_{N}-\bar{X}\right)^{T}
\end{pmatrix}\\

&=\frac{1}{N}
\left(
    \begin{pmatrix}
    x_{1} & x_{2} & \cdots & x_{N}
    \end{pmatrix}
    -\bar{X} \cdot \mathbb{1}_{N}^{T}
\right)\left(\begin{pmatrix}
x_{1}^{T} \\
x_{2}^{T} \\
\vdots \\
x_{N}^{T}
\end{pmatrix}-\mathbb{1}_{N} \cdot \bar{X}^{T}\right) \\

&=\frac{1}{N}\left(X^{T}-\bar{X} \cdot \mathbb{1}_{N}^{T}\right)\left(X-\mathbb{1}_{N} \cdot \bar{X}\right)^{T}\\

& =\frac{1}{N}\left(X^{T}-\frac{1}{N} X^{T} \mathbb{1}_{N} \mathbb{1}_{N}^{T}\right)\left(X-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T} X\right) \\

& =\frac{1}{N} X^{T}\left(I_{N}-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T}\right)\left(I_{N}-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T}\right) X
\end{align*}
$$

其中，我们令$I_{N}-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T}$是一个$N\times N$的方阵，我们称他为centering matrix，记作$H$，可以证明的是$H^T=H,H^n=H$，由于$X^{T}-\bar{X} \cdot \mathbb{1}_{N}^{T}=X^TH^T$，因此这个$H$矩阵在数学意义上其实就是把相乘矩阵的均值置零，矩阵$X$的方差可化简为$\Sigma=\frac{1}{N} X^{T}HX$

与此同时，通过这个式子我们也可以说明其实方差的计算可以被分为两步，第一步$HX$表示把所有样本点整体移动到原点附近，第二步$(XH)^THX$则是把移动后的每一个样本点计算和原点的距离的平方相加，换一个角度来说，如果样本点原本的均值就是零，则$X^TX$就可以表示为其方差矩阵

> 需要注意由于$X$不是方阵，因此$X^TX\ne XX^T$，一个形状是$N\times N$另一个是$p\times p$，$H$的大小也是同理

### 奇异值分解（SVD）

在说奇异值分解之前，我们需要先回顾一下特征值分解的过程，首先我们需要了解到一个方阵的特征值代表了什么含义，说到特征值，我们就不得不提矩阵，特征值和特征向量相互之间的关系式
$$
Av=\lambda v
$$
其中$A$是分解矩阵，$v$是特征向量，$\lambda$是特征值，线代中我们往往只是知道这么一条式子但不知道特征值和特征向量的具体含义，实际上，任何一个方阵乘以一个向量所得结果一定是和该向量同维的另一条向量，因此每一个方阵从另一种角度也可以看作是给一个向量做了一次线性变换，而线性变换的过程包括伸缩和旋转，接着我们再来看这条有关特征值的式子，我们发现了，在这个式子中矩阵对特征向量做的线性变换居然能通过一个常数乘这个特征向量得到，而我们又知道常数只会对向量做伸缩变换，因此找寻特征值的过程总结一下就是找寻某一条不受方阵旋转变换影响的特征向量

![Image](https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuypRuABCGAYIouIazEuNcZTibpdjY39e0kHWiaUF1PHjEfXovvTWaFmlopSu9RgPicvVzdpcZdqpyhiaA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

> 上面是$M=\begin{bmatrix}
>  1 & 1\\
>  1 &0
> \end{bmatrix}$在二维空间的变换过程，我们可以发现$(1,0)$这个方向的向量方向完全不受坐标轴的拉伸和扭曲影响，意味着其就可以作为方阵$M$的一个特征向量

接着我们再复习一下特征值的分解过程，我们的目标是将方阵$A$分解成$Q\Sigma Q^T$形式，其中$Q$是由特征列向量排列成的矩阵，$\Sigma$是对角矩阵，其每一个对角元素皆为一个$A$的特征值，且由左上至右下依次减小，其中的求解过程大致可以分为三步

1. 构造计算$|A-\lambda I|=0$求得多个$\lambda$特征值
2. 把求解出来的特征值带回$(A-\lambda I)x=0$解线性方程组，求得各个特征向量$x$，由于我们只需要特征向量方向，大小并不重要，因此一般来说我们会把特征向量缩放成单位向量作为最终结果
3. 把各个特征值从大到小沿对角排列构造出$\Sigma$，特征向量与其相对应排列为$Q$，如果有重根则排列多次，保证$A$和$\Sigma$同维度

但是我们也提到了，特征值分解强制要求$A$是方阵，这也符合理解，不然和向量相乘出来也不是另一个同维度向量了，但是终究说到底还是方阵，于是就有人想是否可以把他拓展到任一矩阵中，于是就产生了奇异值分解

对于一个$A_{m\times n}$的矩阵，我们要将其分解成$U_{m\times m}\Sigma_{m\times n} V_{n \times n}^T$，且这个分解是任一矩阵均存在的，奇异值分解的具体过程如下

1. 计算$AA^T,A^TA$，易得这两个矩阵为实对称方阵

2. 求解他们的特征值和特征向量，注意到实对称方阵的特征向量一定互相正交，这两个方阵的特征值一定只差0的个数（后一个的严格证明可看下图）

   ![img](https://iknow-pic.cdn.bcebos.com/4a36acaf2edda3cc014e752d02e93901213f9211?x-bce-process=image%2Fresize%2Cm_lfit%2Cw_600%2Ch_800%2Climit_1%2Fquality%2Cq_85%2Fformat%2Cf_auto)

3. 令$AA^T$特征向量方阵为$U$，$A^TA$特征向量方阵为$V$，非零特征值取根号后按大小排布构成矩阵$\Sigma'$，之后根据$A$的形状把$\Sigma'$补零成$\Sigma$，其构造证明可以看以下式子
   $$
   \begin{cases}
   A=U\Sigma V^T \\
   A^T=V\Sigma^T U^T
   \end{cases}\Rightarrow
   AA^T=U\Sigma V^T\cdot V\Sigma^T U^T=U\Sigma^2 U^T
   $$
   同理可得$U,V$分别就是$AA^T,A^TA$的特征向量组，$AA^T,A^TA$的任一特征值的排布正是$\Sigma^2$

对于奇异值的实际作用，可以在接下来的PCA推导中看到，此处我们从奇异值的分布情况稍微提一下，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上。也就是说，剩下的90%甚至99%的奇异值几乎没有什么作用，例如我随便打一个矩阵
$$
A=\begin{bmatrix}
 1 & 2 & 3 &4  & 4\\
 1 & 2 & 3 &4  & 6\\
 4 & 5 & 7 & 8 &4 \\
 9 & 4 & 5 &2  &1
\end{bmatrix}\Rightarrow \Sigma=\begin{bmatrix}
 18.49 & 0 & 0 &0  & 0\\
 0 & 7.64 & 0 &0  & 0\\
 0 & 0 & 2.93 & 0 &0 \\
 0 & 0 & 0 &0.06  &0
\end{bmatrix}
$$

### 主成分分析（PCA）

经过了这么多的知识铺垫，我们终于可以来看看本章的重点PCA了，在此之前我们先思考一个问题，假如我现在拿到了几组样本点，我们怎么知道这些样本点到底可不可分呢，有时候我们总会在某些论文上看到画的五颜六色的散点图，但是这些图又是怎么画出来的呢，数据的处理固然重要，但是如何把数据用可视化的方法展示出来同样是十分重要的一环

#### 最大投影方差

在LDA一章中，我们详细介绍了如何通过合理的降维完成分类，PCA和他的思想有点像，但是在LDA中我们已经存在前提就是样本点数据是线性可分的，我们的目标就是对其进行分类，而PCA的目标并不是区分样本点，只是把我们无法观测到的高维样本点用我们可以理解的方式表示出来而已

降维最容易碰到的问题就是数据点重合在一起，比如$(1,2,3),(1,2,4)$两个点只有第三维坐标不一样，如果我们把他们沿$x,y$平面降维，也就是投影到$x,y$平面上，就会出现两个点在二维平面上叠在了一起，仅看降维后的平面我们完全无法区分这两个点，从观测角度显然是不利的，但是如果我们将其沿别的平面上投影，两个点就可能可以被分开，而当数据点变多，每一个数据点之间的距离越大，显然也就越利于我们分析

降维后重叠在一起的点我们可以称之为一种特征的损失，不仅是对我们观测不利，由于他把高维中两个不相关的点投影到了同一个位置，在降维的其他使用领域，比如降噪，压缩等任务中同样是不利的，如何找到一个尽可能让数据点投影上去能够被分开的超平面就成了我们需要克服的问题

在LDA中，我们使用了投影后的均值和方差来找到投影直线，在此处我们同样可以使用这个思路，既然需要在超平面中尽可能被分开，是不是意味着投影到这个超平面的数据点之间的方差要尽可能大呢？于是就有了最大投影方差的解决思路

和LDA不一样的是，此处我们不再仅限于投影到一条直线上，而是要取能使样本点方差尽可能大的前几条投影直线，将样本点投影到这些若干条直线之后用这些直线当作另一组坐标基重构样本点，易得所重构的超平面一定是能够时样本投影方差最大的超平面，同时由于这些直线个数可以根据实际需求任取，意味着降维到的维数也是可控的

于是现在的问题就是如何求解这一组投影直线了，实际上我们完全可以用类似LDA的方法求解，除了少求一次方差之外可以说其余步骤都是完全一致的，由于我们只需要最大化投影方差，因此我们甚至可以直接把方差计算作为用于最大化的损失函数，我们定义投影向量为$w_1$，直接套用LDA的化简结果
$$
L=w^T\frac{1}{N}\sum^{N}_{i=1}(x_i-\bar x)(x_i-\bar x)^T w=w^T\Sigma w
$$
然后我们发现这玩意好像有点问题，由于我们没有限定投影向量的长度，因此好像只要$w$越长这玩意结果就越大，这样的话就不管$X$什么事了，更无法确定$w$的方向，这显然不是我们想要看到的，为了公平起见，我们令$w$是单位向量，即$w^Tw=1$

于是问题就转化为了在约束条件$w^Tw=1$求解$L$的最大值，接下来就是拉格朗日乘子法出场的时候了
$$
L'=L-\lambda(w^Tw-1)
$$
同时需要注意我们的目标只是确定$w$分量的方向，因此我们可以仅对$w$求偏导令其等于$0$
$$
\frac{\partial L'}{\partial w}=2\Sigma w-2\lambda w=0\\
\Rightarrow\Sigma w=\lambda w
$$
这玩意不就是$\Sigma$方阵的特征方程吗，对$w$求偏导之后得出来的这条等式，用人话说就是对于$w$分量来说他所有的极值点都集中在他自己是作为$\Sigma$的特征向量上了，也就是我们想取到$L'$的最大值，就得去这些特征向量上找，不止如此，从这条式子我们还能知道$\Sigma w$的大小是受$\lambda w$控制的，因为我们没办法改变$w$的大小，因此求最大值的重担就落到了$\lambda$身上了，从大到小，我们每取一个特征值，相对应的特征向量$w$就作为样本点的一条投影向量

#### 正交基

如果你仔细想想就会发现好像什么东西不太对，我们在介绍拉格朗日乘子法的时候曾经说过，这个方法只能获知所求函数的多个极值，想知道求出来的哪个是最大或者最小值，正常还应该把他带回到原始的式子中看看结果，与此同时我们还知道既然这个$L'$是可导的，那么其在高维空间中的函数图像也一定是连续的，既然他图像又连续，又存在最大值，甚至我们还知道最大值就是特征值最大的那一条特征向量，也就是说我们知道所有的样本点一定是在这一条特征向量上的投影方差可以达到最大值，那么按理说把这条最大的特征向量任意旋转一个极其微小的角度，落在这条被微调过的向量上的投影方差一定是可以无限接近原始向量的，也就是说，除了最大特征值对应的那个特征向量，其余的特征向量所获得的投影方差一定小于这条微调过的向量，既然如此，又为什么要逐个取每一个特征向量呢，而且如此一来只有投影方差最大的那条向量可以被确定，又如何把一组样本点投影到多条向量之后重构空间呢？

这就得说到方差矩阵的特征向量的另一个重要性质了，没错就是正交性，由于方差矩阵（或者明确说是协方差矩阵）是一个对称矩阵，意味着其每一个特征向量之间一定是相互垂直（正交）的，于是你会发现一个惊人的事实，我们如果把样本数据照特征向量顺序逐个做投影，然后把投影结果（是一个值）排在一起作为另一组样本特征（是一组向量），如果我们能把各个数据点画在一个超维空间中，那么以上的步骤实际上就是把所有的点旋转了一下而已

![img](https://s2.loli.net/2023/02/23/nH1KxfyhwlU3ZiS.png)

如上图所示，求解所得的特征值最大的特征向量比如是$w_1$，我们的确可以看到各个样本点（蓝点）在其上面的方差是最大的，另一条特征向量与之垂直，在二维平面上则一定是绿色这条$w_2$，然后我们发现在这两条特征向量的映射下，如果把$w_1,w_2$分别看作是新的$x,y$轴，则所有的蓝色点仅做了平移和旋转变换，而且其中的平移变换还是因为为了便于观察我把特征向量挪到了样本点均值处，正常求解出来的特征向量是从原点出发的

仅作旋转变换最显著的好处就是不改变样本的分布，并且其保留了投影方差最大的特征向量，意味着假如我想把上面的蓝色点降维到一条直线上，那么只需要保留$w_1$一条向量和其上的投影点即可，对于更高维的样本点，由于我们有了特征值这一个衡量维度去判断特征向量的好坏，因此同理只取前$k$大特征值对应的特征向量，就可以在保证不改变样本点分布且尽可能以投影方差最大的前提下将样本降维到$k$维

#### 最小重构代价

如果说最大投影方差是顺着思路一步步推导出特征向量作为基底的过程，那么最小重构代价就是倒着进行，我们先假设一堆样本点已经在$w$的影响下从$p$维被降维到了$q$，接着通过比较降维前后数据点的偏移情况求解最优化正交基

![img](https://s2.loli.net/2023/02/23/p427boKtyAFLNPc.png)

由于这种方法不需要求解样本方差，因此我可以取样本中的任意点单独讨论，比如现在我有一个样本点$A$，在上一节当中我们已经明确需要寻找的投影向量必须相互正交，因此此处我们就任意构造出两条正交单位向量$w_1,w_2$，$A$点在两条基上的投影分别记为$D,E$，如果假设$A$点坐标$(B,C)^T=a$，根据点乘等于投影长度，易得$OD,OE$长度分别为$w_1^Ta,w_2^Ta$，那么我们就可以得到$a$的另一组向量表示$a=(w_1^Ta)w_1+(w_2^Ta)w_2$，你可能奇怪这有啥意义，搞那么半天得到的不还是$a$，为什么要如此多此一举

先别急，接下来我们要把他降到一维，按照上一节的说法，只需要丢弃一个维度就行了，比如我可以不要$w_2$，那么现在我们就只有一条向量$w_1$了，没有了$OD$来控制$A$在$w_2$上的分量，等于说$A$点被降维到了$A'$点上，照着上面的样子，我们易得$A'$坐标$a'=(w_1^Ta)w_1$

现在我们用统一的$a$和正交基来表示了降维前后的两点，并且将其映射回了原空间中，于是我们就开始思考，降维这个操作让原本的$A$点变成了$A'$点，我们是否可以定量的表示这个变换中的特征损失情况呢，然后你可能发现还真能，我只需要计算两点之间的距离$AA'=d$不就行了吗，明眼人都看得出来，要是$d$比较大，明显和原本的$A$的距离就越远，距离越远就越不相似，相对应的降维特征损失也就越多了

**这种在原始空间中计算降维后的点在重构回降维前的样本点的距离最小值的方法就叫做最小重构距离**，下面是计算过程，距离的计算直接使用欧氏距离即可，同时注意样本点不止一个且维度也不止二维，我们假设有$N$个$p$维样本点$x_1,x_2\dots x_N$，需要在$w_1,w_2\dots w_p$这一组正交基上被降维到$q$维，即抛弃后$p-q$个$w$，只使用$w_1,w_2\dots w_q$表示降维点$x'_1,x'_2\dots x'_N$，需要注意$x'$的维数也是$p$，可以参考上面的$a'$推导过程
$$
\begin{align*}
L&=\frac{1}{N}\sum_{i=1}^N\left \| x_i-x'_i \right \| ^2\\
&=\frac{1}{N}\sum_{i=1}^N\left \|\sum_{j=1}^{p-q}(w_j^Tx_i)w_j\right \|^2\\
&=\frac{1}{N}\sum_{i=1}^N\sum_{j=q+1}^{N}(w_j^Tx_i)^2\\
&=\sum_{j=q+1}^{N}\frac{1}{N}\sum_{i=1}^N(w_j^T(x_i-0))^2
\end{align*}
$$
其中如果我们把$x_i$看作是一组均值为$0$的样本，那么$\frac{1}{N}\sum_{i=1}^N(w_j^T(x_i-0))^2$就可以看作是$w_j^T\Sigma w_j$，我们要取其最小值，因此问题就变成了下面这样
$$
L=\arg\underset{\theta}{\min}\sum_{j=q+1}^{N}w_j^T\Sigma w_j\\
s.t.\quad w_j^Tw_j=1
$$
由于每一组$w_j$都是相互独立的，因此这和上一节的求解式子是一样的，结果同样是$\Sigma w=\lambda w$，不过需要注意的是出我们求解的是最小值，同时由于我们前提已经假设了$w$之间相互正交，因此$w_j$是全不相同的，因此求解的结果就是取前$p-q$个最小的特征值对应的特征向量作为解，而我们又已经假设了第$q+1$到$N$个$w$其实是在降维中被丢弃的，因此此处求解出来的结果就是在$p$降到$q$维的过程中不需要的，换个角度，留下来的那些特征向量正是前$q$个最大的，这和在最大投影方差一节中求解的结果是完全一致的

不过我们在其中假设了一下所有$x$都是以均值为$0$排布的，这也是解释了为什么在上一节中需要把特征向量挪到样本点均值为起始的原因

通过两次推导，到此我们可以总结一下PCA的一般操作过程，假设我们要把样本点集$X_{p\times N}$降维到$q$维，则只需要先计算样本的方差$\Sigma$的特征值，取前$q$个最大特征值的单位特征向量作为基底，然后把所有样本点与这些基底做点积计算投影长度，把投影长度重新排布就形成了降维后的样本点矩阵$X'_{q\times N}$

#### SVD和PCoA

了解了便于理解的PCA做法，然而从写程序或者是计算角度来说，这种方法其实并不方便，不仅要逐个遍历样本计算特征值和点积，还要把算完的内容重新排列组合，是否有什么快捷的手段可以直接得出PCA降维后的坐标结果呢

为了避免歧义，以下把样本点$X$的方差记为$S$，根据前面的结论，有$S=\frac{1}{N}X^TH^THX$，由于$HX$不是方阵，我们对$HX$进行奇异值分解记为$HX=U\Sigma V^T$

把奇异值分解结果带回$S$，则有$S=V\Sigma^T\Sigma V^T$，这样一来，我们就可以跳过先求解$S$再求解其特征值的步骤，直接求解$HX$的奇异值分解矩阵$\Sigma$和右乘特征向量矩阵$V$即可，然而实际做下来你却发现看似简单一点实际也是骗人的，因为求奇异值的过程等于已经间接求了一次$S$和他的特征值，实际并没有简便多少，而且之后的坐标映射同样需要手动完成

于是我们可以再想想有没有什么办法可以直接拿到降维后的坐标，由于我们知道了需要求解的特征向量组就是$V$，因此坐标的映射关系可以写作$HX\cdot V=U\Sigma V^TV=U\Sigma$，诶这下我们发现了$U\Sigma$正是降维后的坐标，与此同时我们还发现如果我们构造一个$T=HXX^TH^T$，那么按照化简$S$的经验，他化简之后正是$U\Sigma\Sigma^TU^T$，因此为了得到最终的坐标，我们完全可以直接对$T$进行特征值分解，取前$q$大的特征值取根号之后直接和其对应的特征向量矩阵$U$相乘获得所有的映射坐标值，这种不借助样本方差，直接求解降维坐标的方法就叫做PCoA

至此，我们可以把PCA类的降维问题总结成四步

1. **把所有样本点$X$统一移动到原点附近，也就是构造新样本点$X_1=HX$**
2. **把新样本点矩阵$X_1$乘他自己的转置$X_2=X_1X_1^T$，这个结果的形状是$N\times N$**
3. **求出来的这个方阵进行特征值分解，$X_2=Q\Sigma Q^T$，接着只取降维维度的前$q$个特征值和特征向量$Q',\Sigma'$**
4. **求解$X'=Q'\sqrt{\Sigma'}$获得降维坐标，其形状为$q\times N$**

## 支持向量机（SVM）

在第三章中我们通过假设各组样本点的分布情况，推出了似乎十分厉害的逻辑回归，但是我们也提到了，线性分类之所以叫线性分类，就是因为他没有办法分类非线性可分的样本点，为了能够区分此类样本，深度学习提供了一种神经网络的结构，通过叠加多层感知机达到非线性划分的目的，但是在神经网络还没广泛流行起来的时期，说起非线性分类问题，就不得不提到在当时大名鼎鼎的SVM算法了

抽象来说，在这一章我们需要讨论的是除了极大似然估计这种普适性方法之外的另一种特殊的构造评价函数的方法，以及在这个评价函数模型之下不同于一般求导就可以求解的方法，SVM还利用了另一种名为SMO的方法进行求解的思路

让我们看看还是那么一个简单的线性模型，对于构造其的评价函数和求解方法，SVM做出了怎样的贡献

### 拉格朗日乘子法（不等式约束）

#### 梯度角度

上一章中我们曾经提到了拉格朗日乘子法在等式约束下的应用，但是实际情况中不止有约束条件是等式的情况，还可能出现不等式约束的求解问题，我们不妨把前面用过的式子变一变，尝试讨论一下不等式约束条件下的函数极值求解
$$
\begin{eqnarray}
&&\min(x^2+y^2)\\
&&s.t.\quad x^2y\ge 3
\end{eqnarray}
$$
同样的，我们也可以把他的等高线图画出来

![image-20230306171654238](https://s2.loli.net/2023/03/06/F24I51a7BQzmhbi.png)

由于我们限制了$x^2y\ge 3$，又因为梯度总是指向函数值增大的方向，因此在$\ge$约束条件下，梯度方向一定指向蓝色区域内，而我们又知道约束条件限制了$x$的取值范围，因此$x,y$同样只能取到蓝色范围的区域内，由于我们需要求解$f(x,y)$在约束条件下的最小值，因此此时易得最小值点$(x,y)$一定在蓝色区域的边缘线切点$B$上取到，也就是说我们分析了半天，对于这个问题来说，$x^2y\ge 3$实际上和$x^2y=3$的解法是完全一致的，即同样有
$$
\begin{cases}
\Delta f(x,y)=\lambda\Delta g(x,y)\\
g(x,y)=3
\end{cases}
$$
不过其实有人也想到了，这种解法并不是唯一的，比如我把约束条件的不等号改一改方向，变成限制$x^2y\le 3$

![image-20230308131241693](https://s2.loli.net/2023/03/08/LQnemkY2uAZGUJo.png)

此时我们发现，约束条件似乎并不对求解$f(x,y)$的最小值产生什么影响，甚至我们可以完全不管什么$x^2y\le 3$，直接求解$f(x,y)$的最小值$A$就是答案了

究其原因，我们发现控制最小值点究竟是在约束区域内还是约束区域边缘取到同样可以通过切点处的梯度方向来确定，**对于不等式约束来说，我们规定约束已经被化为$\le$的形式，则如果约束条件和原图像在切点处的梯度方向相反，则最小值在约束区域边缘处取到，反之最小值在约束区域内取到，且最小值就等于$f(x)$的最小值**，如上图中$f,g$在$B$点的梯度明显方向是相同的，因此约束$g$不起作用，直接求解$f$最小值即可

> 注意其中$f,g$都是关于$x$的函数，其中$x$是一个包含$x_1,x_2\dots x_i$的集合，即$f,g$都是$i$维函数，每一个自变量就是一个$x_i$，比如上面例子中$i=2$

#### 数学角度

说着容易，但是如果你想把他化作数学式子的形式，就会发现根据相切处梯度方向来分段两个函数似乎不太容易，意味着我们必须要先求解一次切点梯度方向，再根据求解结果选择最终求解函数求解一次最小值，由于求解过程必须被分成两步，因此在等式约束中的构造$\mathcal{L}=f-\lambda g'$然后直接求解一次偏导显然就不太对了，不过这并不代表这个函数的构造有问题，我们看下面这个转化方程组
$$
\begin{cases}
\underset{x}{\min} f\\
g\le0
\end{cases}\Rightarrow
\begin{cases}
\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}\\
\mathcal{L}=f+\lambda g\\
\lambda\ge 0
\end{cases}
$$
看上去晕晕的，怎么又$\min$又$\max$的，其实也非常好理解，我们从里往外看，$\underset{\lambda}{\max} \mathcal{L}$就是只把$\lambda$看成自变量，由于$f,g$和$\lambda$都没什么关系都可以看成常数，求解函数取到最大值时的$\lambda$就变得很简单了

- 当$g>0$的时候为了取得最大值则必须要求$\lambda\to +\infty$，于是$\underset{\lambda}{\max} \mathcal{L}\to +\infty$，此时我们发现$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$由于$\underset{\lambda}{\max} \mathcal{L}\to +\infty$了所以无论$x$取什么只要让$g>0$了最后的结果就一定是正无穷

- 当$g<0$的时候，由于存在约束$\lambda\ge 0$，因此$\lambda=0$时$\mathcal{L}$取最大，此时的$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}=\underset{x}{\min} f$，这就是我们原方程组的目标优化函数，而又有$\underset{x}{\min} f<+\infty$，因此只要用一个额外的约束让$\lambda$的最小值存在而最大值不存在，就可以保证$x$的取值不可能让$g$大于$0$，即满足原方程组的约束条件$g\le0$

  > 通过这一部分分析也可以发现从数学角度来说$\lambda\ge0$的原因和从图像角度的梯度反向是一个问题的两种解释，在数学方面最小值为$0$刚好可以约去$\mathcal{L}$的$g$部分使得求解式子恰巧等于原问题$\underset{x}{\min} f$

- 而当$g=0$的时候，我们发现无论$\lambda$取什么东西都不会对$\mathcal{L}$的结果产生影响，而又因为$\underset{\lambda}{\max} \mathcal{L}$的含义就是$\mathcal{L}$取到最大值时的$\lambda$取值，既然$\lambda$任取啥都不影响$\mathcal{L}$，因此我们完全可以忽略掉$\underset{\lambda}{\max}$了，这也是为什么在等式约束的时候我们只需要求解$\underset{x}{\min} \mathcal{L}$就行了，并不是两个式子有什么不同，而是在过程中我们省略了无作用的$\underset{\lambda}{\max}$而已

综上所述，我们推导出了原方程组和转化方程组的等价性，从结果上来说，我们将一个$x$约束$g\le0$的问题转化成了新增变量$\lambda\ge0$约束问题，并且重新构造了求解条件$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$来统一化$x,\lambda$的求解，看似这样的转化把原有的问题搞复杂了，但是在下一节我们就会详细讨论这样变化在特殊$f,g$场景下的优越性

事实上还可以证明对于等式和不等式的多条约束，同样可以进行如下的转化
$$
\begin{cases}
\underset{x}{\min} f\\
g_1=0\\
\vdots\\
g_N=0\\
k_1\le0\\
\vdots\\
k_M\le0\\
\end{cases}\Rightarrow
\begin{cases}
\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}\\
\mathcal{L}=f+\sum^N_{i=1}\eta_ig_i+\sum^M_{i=1}\lambda_ik_i\\
\lambda_1\ge 0\\
\vdots\\
\lambda_M\ge 0\\
\end{cases}
$$
而这就是拉格朗日乘子法的完整形态

### 函数对偶性

在上一节中我们提出了拉格朗日乘子法的完整形式，但是我们发现转化出的方程式比起原有的方程式并没有简便多少，照这样下去我们还是无法求解出不等式约束下的优化通解，在这一节中我们引入转化方程式求解函数$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$对偶函数$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L}$，看看这两个函数之间有什么关系

说是对偶函数，实际上其实就是改了一下$\max$和$\min$的顺序而已，这有什么难的，为了解释清楚对偶函数之间的关系，我令$x=g,y=f$，尝试通过参数方程的模式把$f,g$一起画在一个坐标系下，先别问为什么，我们就假设这两函数的参数方程画出来如图$ABCDE$这样

![image-20230309214058806](https://s2.loli.net/2023/03/09/wpxL1caAGKi5eJo.png)

看上去好像很奇怪，两个函数怎么会变成这个样子，都说了是假设，如果对参数方程不是很理解，那么可以类比比如$x=\sin x,y=\cos x$的时候画出来就是一个圆，那么$x=f,y=g$能画出这种图像也不是很奇怪了

此时我们来看看$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L},\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$两个对偶函数值在这个图像上会表示成什么样子，我们仍然假定约束为$\lambda\ge0$

- 先来看$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$，上一节我们提到它等价于$\begin{cases}
  \underset{x}{\min} f\\
  g\le0
  \end{cases}$，翻译成中文就是取$g$小于等于$0$的部分中的$f$的最小值作为输出，我们可以用一条直线从上往下扫过图形中$g\le0$的部分并取折线上投影到$y$（也就是$f$）的位置作为输出（如图中直线$G_1,G_2\dots $），然后我们发现当直线为$AG$的时候取到$f$最小值，我们不关心此时$x$取什么，但我们知道$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}=G$

- 然后我们看复杂一些的$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L}$，其中的$\mathcal{L}=f+\lambda g=y+\lambda x$，我们假设$\mathcal{L}=0=y+\lambda x$此时就是一条过原点斜率为负（$\lambda\ge0$）的直线，同时输出值$0$是这条直线过纵轴的截距

  - 对于前半部分$\underset{x}{\min} \mathcal{L}$表示过图形上任意点的任意斜率为负的直线，使其截距最小，则其要么过$A$点要么过$D$点，正如图$H_1,H_2$所示，其中$DJ$表示斜率为$0$时的直线，确定了直线必过$A$或$D$
  - 我们再来看$\underset{\lambda}{\max}$部分，他表示在前面$\underset{x}{\min} \mathcal{L}$的基础上，通过改变斜率的大小，使直线的截距最大

  上面那么多翻译成人话就是找一条过$A$点或过$D$点的直线，令其截距最大，哦你这么说我就明白了，那不就是连接$AD$后过纵轴的截距吗，没错，于是我们推导出$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L}=H$

从图像中很直观的看出纵坐标$H<G$，即$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L}<\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$，而这就是传说中的函数弱对偶关系，既然有弱对偶关系，那一定有强对偶关系吧，没错，下面我们再来看一张图

![image-20230309220344313](https://s2.loli.net/2023/03/09/xriCujI2VHQTbNm.png)

如果$f,g$围成的图案如上面的蓝色区域这样，那么根据相同的规则求解出来的$=\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$，这就是函数的强对偶关系，而这两个图像的区别就是其凸性，**若由求解问题$f$和约束条件$g$围成的图形在平面中呈现为凸函数或凸集，则其恒具有强对偶关系，我们可以通过求解其对偶问题的解来变相求解原问题的解**

另外可证明的是**对于任意函数弱对偶关系成立**，且**当$f$为凸函数，$g$为仿射函数**时恒有强对偶关系成立（即满足**凸优化问题\+Slater条件**，此处不证），因此对于求解问题为凸函数的问题来说，拉格朗日乘子法的求解过程可以再次转化为

> 凸函数的定义就是选取函数图线上的任意两点其连线部分恒在图像上方，凸集的定义则是闭合区域内选取任意两点连线均在闭合区域范围内，也可以用求二阶导大于零的方式判断凸性
>
> 而仿射函数可以简单理解为线性函数，即$g=Ax+b$此形式函数

$$
\begin{cases}
\underset{x}{\min} f\\
{\color{red}f\text{ is convex function}}\\
g_1=0\\
\vdots\\
g_N=0\\
k_1\le0\\
\vdots\\
k_M\le0\\
\end{cases}\Rightarrow
\begin{cases}
{\color{red}\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}}\\
\mathcal{L}=f+\sum^N_{i=1}\eta_ig_i+\sum^{M}_{i=1}\lambda_ik_i\\
\lambda_1\ge 0\\
\vdots\\
\lambda_M\ge 0\\
\end{cases}
$$

### KKT条件

当约束问题满足强对偶关系的情况下，我们求解对偶问题其实比起求解原问题要方便不少，原因是对偶问题的求解可以推出一些容易求解的结论，最著名的就是传说中的KKT条件，这东西听起来挺唬人，但是在我们有了前两节的只是积累之后会发现其实这玩意并没有什么难点

我们令$\mathcal{L}=f+\sum^N_{i=1}\eta_ig_i+\sum^M_{i=1}\lambda_ik_i$，而$\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}$在$x=x^*,\eta_i=\eta_i^*,\lambda_i=\lambda_i^*$上取到解值，即$\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}=\mathcal{L}(x^*,\eta^*,\lambda^*)$，同理令$\underset{\lambda,\eta}{\max} \underset{x}{\min} \mathcal{L}=\mathcal{L}(x^+,\eta^+,\lambda^+)$，此外由于$\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}$和原方程等价，因此也可以推出$\underset{x}{\min}f=f(x^*)=\mathcal{L}(x^*,\eta^*,\lambda^*)=\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}$，于是我们对对偶问题$\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}$进行如下推导
$$
\begin{aligned}
\max _{\eta,\lambda} \min_x \mathcal{L}(x,\eta,\lambda) 
&=\min_x \mathcal{L}(x,\eta^+,\lambda^+) \\
& \leq \mathcal{L}\left(x^{*}, \eta^{+}, \lambda^{+}\right) \\
& =f\left(x^{*}\right)+\sum_{i=1}^{N} \eta_{i}^{+} g_{i}+\sum_{i=1}^{M} \lambda_{i}^{+} k_{i}\\
& =f\left(x^{*}\right)+\sum_{i=1}^{M} \lambda_{i}^{+} k_{i}\\
& \leq f\left(x^{*}\right) \\
& =\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}(x,\eta,\lambda) 
\end{aligned}
$$

我们发现其中存在两个小于等于号，由于我们已经假设约束问题满足强对偶关系，有$\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}=\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}$，因此其中的两个小于等于号只能取等号，那么我们就可以得到以下两个结论

1. $x^*=x^+$，他们都能让$\mathcal{L}$取到最小值，根据极值点梯度为零，于是我们在等式约束中求解拉格朗日的办法仍旧能用，即$\left.\frac{\partial \mathcal{L}}{\partial x}\right|_{x=x^{*}}=0$，针对$\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}$我们仍能通过求$x$偏导为零的方式求解出最优值
2. $\sum_{i=1}^{M} \lambda_{i}^{+} k_{i}=0$意味着对于$k_i<0$的部分最优值$\lambda_i^+=0$恒成立，这其实在推导不等式约束中转化方程式的等价关系中也提到过，$\lambda$只会在$k=0$的时候取到非零值，进一步的，当$\lambda$取到非零值时$k=0$，$x$一定落在$f,k$的切点上，即问题转化为求解等式约束的问题

整合这么多结论，我们就可以得出最终求解对偶问题可以使用的五个求解条件
$$
\begin{cases}
\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}\\
\mathcal{L}=f+\sum^N_{i=1}\eta_ig_i+\sum^{M}_{i=1}\lambda_ik_i\\
\lambda_1\ge 0\\
\vdots\\
\lambda_M\ge 0\\
\end{cases}
\Rightarrow
\begin{cases}
\frac{\partial \mathcal{L}}{\partial x}=0\\
g_i(x)=0,i=1,\dots,N\\
k_j(x)\le0\\
\lambda_j\ge 0\\
\lambda_j k_j(x)=0,j=1,\dots,M
\end{cases}
$$
有了这些结论，在下一节通过实际求解SVM的最优值问题来看看KKT条件是如何运用在实际问题的求解中的

### 硬间隔

接下来我们考虑一个最简单的问题，即平面上有两组点，他们是线性可分的，既然是线性可分的，那么一定会有多条区分线，硬间隔要做的就是找到所有区分线当中最有可能是真实区分线的线

![image-20230311130755058](https://s2.loli.net/2023/03/11/AyX9ZrCoNhGFu6n.png)

如图我们有一堆蓝色和橙色的可线性分类的点，硬间隔假设了默认情况下区分线到两组分类点离这条线最近点的距离都是相同的，比如图中紫色和绿色实线，他们到离他们最近的两个样本点$A,B$的距离都是一样的，在保证距离相同的情况下，这个距离最大值对应的分隔线就是硬间隔所需要求解的最终直线，比如图中由满足要求的紫线和绿线产生的距离$AC>AD$因此我们判断紫线更符合硬间隔分类要求

如何用数学语言表示这么一个模型结构呢，我们不妨一个一个条件对要求进行拆分

1. 我们假设这条直线的方程为$w_1x_1+w_2x_2+b=0$，注意此处考虑了二维样本点，如果令$w=(w_1,w_2),x=(x_1,x_2)$则直线还可以表示为$w^Tx+b=0$，该式子可以表示更高维的样本点情况，同时需要注意向量$w$是直线的法向量（可以尝试把直线挪到原点则$w$和所有$x$点积结果为$0$可证）
2. 由于$A,B$点和这条直线距离相同，因此有$w^Tx_a+b=-c,w^Tx_b+b=c$，两式相减有$w^T(x_b-x_a)=2c$，我们发现该式相当于是直线的法向量和向量$\overrightarrow{AB}$点乘，根据点乘运算规则有$w^T(x_b-x_a)=\left \| AB \right \|\cos\alpha \| w  \|=\left \|  AC\right \| \| w  \|=2c$
3. 根据硬间隔规定条件，我们需要最大化$\left \| AC \right \|$长度，也就是需要最大化$\frac{2c}{\| w  
   \|}$，又因为$\| w  \|=\sqrt{w_1^2+w_2^2}$和把他放在分母都不太好求，且$c$一定是正数，因此问题还可以转化为求解$\frac{\|w\|^2}{4c^2}$的最小值
4. 当然仅有这一条件直接求解也是不行的，由于我们刚刚搞半天都是建立在这条直线能够区分两组分类点的基础上去寻找最好的那条直线，因此我们还需要给他添加一些约束条件，假设我们有$N$个样本$x_i$，他们对应的分类标签是$y_i$，当$y_i=-1$的时候样本点被分在直线下方，$y_i=1$的时候样本点被分在直线上方，且这些点到分隔线的距离必须大于等于间隔距离$c$，则约束条件可以这么写$y_i(w^Tx_i+b)\ge c,i=1,\dots,N$
5. 由于$c$一定是正数，因此我们可以把$c$除到左边来，并令$\frac{w}{c}=w',\frac{b}{c}=b'$，则有$y_i(w'^Tx_i+b')\ge 1$

综上，我们就写出了硬间隔的求解方程，注意由于之前也推导出$\frac{w}{c}$可以看作一个整体，因此下面直接让$w,c$进行了合并为一个参数$w$，$b$同理，另外$\|w\|^2=w^Tw$，在求解函数前加二分之一的系数是为了之后求导的时候好求一点，系数是多少不影响最后求解结果
$$
\begin{cases}
\min\frac{1}{2}w^Tw\\
1-y_i(w^Tx_i+b)\le 0,i=1,\dots,N\\
\end{cases}
$$
然后我们就惊讶的发现这玩意和我们前几节推导了半天的不等式约束下的拉格朗日乘子法完全吻合，更牛逼的是他的求解问题$\|w\|^2$明显是一个凸函数，约束条件$y_i(w^Tx_i+b)$明显是一个仿射函数，意味着这玩意还满足强对偶关系，那KKT条件不就来了吗，心动不如行动，我们直接带入KKT条件看看能不能求解一下
$$
\begin{align*}
&\begin{cases}
\underset{w,b}{\min}\frac{1}{2}w^Tw\\
1-y_i(w^Tx_i+b)\le 0,i=1,\dots,N\\
\end{cases}\\
\Rightarrow&
\begin{cases}
\underset{\lambda}{\max}\underset{w,b}{\min}\mathcal{L}\\
\mathcal{L}=\frac{1}{2}w^Tw+\sum^N_{i=1}\lambda_i\left(1-y_i(w^Tx_i+b)\right)\\
\lambda_i\ge0,i=1,\dots,N
\end{cases}\\
\Rightarrow&
\begin{cases}
\frac{\partial \mathcal{L}}{\partial w}=0\\
\frac{\partial \mathcal{L}}{\partial b}=0\\
1-y_i(w^Tx_i+b)\le 0\\
\lambda_i\ge 0\\
\lambda_i\left(1-y_i(w^Tx_i+b)\right)=0\\
i=1,\dots,N
\end{cases}
\end{align*}
$$

先计算偏导为零的部分
$$
\begin{align*}
\frac{\partial \mathcal{L}}{\partial w}=0&\Rightarrow w-\sum^N_{i=1}\lambda_iy_ix_i=0\\
&\Rightarrow w^*=\sum^N_{i=1}\lambda_iy_ix_i\\
\frac{\partial \mathcal{L}}{\partial b}=0&\Rightarrow\sum^N_{i=1}\lambda_iy_i=0\\
\end{align*}
$$

此处虽然只使用了偏导为零的部分，但是我们需要额外注意一个十分重要的结论，即$\lambda_i\left(1-y_i(w^Tx_i+b)\right)=0$这条式子，由于之前我们证明过了在KKT条件中要么$\lambda=0$要么$k=0$，对于这条式子也是适用的，而当$1-y_i(w^Tx_i+b)=0$的时候，此时我们发现$x_i$这一点刚好落在上图中的虚线上（即垂直距离分割线为$\pm c$的直线上），我们完全可以把其看作$A,B$中的一个，此外所有的点都必然不能使$1-y_i(w^Tx_i+b)=0$，因此他们的$\lambda$都是$0$，同时由于$\sum^N_{i=1}\lambda_iy_i=0$，因此还易得唯二的两个$\lambda\ne0$的点$A,B$的$\lambda$值一定相等

也就是说，**对于所有的样本点来说，除了$A,B$两个距离分割线最近的点之外，其余所有点的$\lambda_i=0$，进一步的，还有$\lambda_a=\lambda_b$，我们称这两个分属于两个类别的特殊样本点为支持向量**

之后我们可以把这两个偏导结果结果带入$\mathcal{L}$
$$
\begin{align*}
&\underset{\lambda}{\max}\underset{w,b}{\min}\mathcal{L}(w,b,\lambda)\\
=&\underset{\lambda}{\max}\mathcal{L}(w^*,b^*,\lambda)\\
=&\underset{\lambda}{\max}\left(\frac{1}{2}\left(\sum^N_{i=1}\lambda_iy_ix_i\right)^T\sum^N_{j=1}\lambda_jy_jx_j+\sum^N_{i=1}\lambda_i-\sum^N_{i=1}\lambda_iy_i\left(\sum^N_{j=1}\lambda_jy_jx_j\right)^Tx+\sum^N_{i=1}\lambda_iy_ib^*\right)\\
=&\underset{\lambda}{\max}\left(\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum^N_{i=1}\lambda_i-\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)\\
=&\underset{\lambda}{\max}\left(\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)\\
\Rightarrow&\begin{cases}
\underset{\lambda}{\max}\left(\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)\\
\sum^N_{i=1}\lambda_iy_i=0
\end{cases}
\end{align*}
$$
化简出来的这条式子是一个仅关于$N$个$\lambda$的等式约束优化问题，你说这我会，再来一次拉格朗日不就完了吗，可以是可以，不过在此处由于样本点可能比较多，导致在求解梯度的时候需要算一堆导数，况且你求一求也会发现这玩意关于一个$\lambda$的偏导结果乱七八糟的，求一个都够呛，要求$N$个并不是很方便

### SMO

因此在此处往往会使用SMO方法来计算最优化的$\lambda$，至于SMO是什么玩意，如果具体来说不只是篇幅不够可能说完了还懂不了，其推导过程确实有点复杂，但是可以简单说一说他的求解过程，整体过程比较类似梯度下降

1. 先把所有的$\lambda$都赋值为$0$，需要格外记住的是每一个$\lambda$对应一个$x$，他们是不可分割的，$\lambda$控制了一个$x$到底是支持向量还是其他划水向量

2. 另外由于之前求偏导为零的时候并不能直接获得由$\lambda$表示的$b$值，因此在SMO的过程中我们还需要顺便优化一下$b$，因此我们还要再赋值一个$b$，随机把他赋一个值就行了（也可以是$0$）

3. 接着就开始循环以下这些步骤，由于一开始我们的$\lambda$一开始都是随意取得，而我们知道KKT条件中已经规定了$\lambda$将要变成的样子了，只是不知道哪个$\lambda$会变而已，于是我们就可以对着KKT条件来看下面的步骤什么时候终止

   - 首先根据KKT条件找支持向量，即找$1-y_i(w^Tx_i+b)=0$的时候，$w^T$我们可以用$w^*$带掉，$b$带不掉就用我们第一步赋值的$b$代，于是我们遍历每一个$x_i$看看前面算出来的$1-y_i(w^Tx_i+b)$哪个离$0$最远
     $$
     \begin{cases}
     g_i=\sum^N_{i=1}\lambda_iy_ix_i^Tx_i+b\\
     \underset{x_i}{\max}|1-g_i|\\
     \end{cases}
     $$

     > 一开始由于所有$\lambda$都是$0$所以任取一个$x$就行了

   - 这样我们就找到了一个$x_1$，但是这还不够，我们还要找第二个$x$，第二个$x$的找法是求解下面式子的最大值，其中计算完的$E$在接下来还需要用
     $$
     \begin{cases}
     E_i=g_i-y_i\\
     \underset{x_i}{\max}|E_1-E_i|
     \end{cases}
     $$

   - 现在我们手握$x_1,x_2$，接下来就可以开始更新他们所属的$\lambda_1,\lambda_2$了，我们让新的$\lambda_1,\lambda_2$值为如下值
     $$
     \begin{cases}
     \eta=x_1^Tx_1+x_2^Tx_2-2x_1^Tx_2\\
     \lambda_1^{new}=\lambda_1+\frac{y_1(E_1-E_2)}{\eta}\\
     \lambda_2^{new}=\lambda_2+y_1y_2(\lambda_1-\lambda_1^{new})
     \end{cases}
     $$
     
     > 需要额外注意在更新$\lambda$的时候和梯度下降不一样的是更新结果是由偏导为$0$求出来的，也就是$\lambda^{new}$这个值一定是在这一次$b$下能够使$x_1,x_2$能满足KKT条件的最优值而不是相对更优值，只不过随着之后$b$的更新他们可能会发生偏差，这也是为什么看上去和梯度下降一样需要反复遍历更新的原因
     
   - 还没结束，接下来我们还要让$b$更新一下
     $$
     \begin{cases}
     b^{new}=-E_{1}-y_{1}x_1^Tx_1\left(\lambda_{1}^{new}-\lambda_{1}\right)-y_{2}x_1^Tx_2\left(\lambda_{2}^{new}-\lambda_{2}\right)+b&\lambda_1^{new}>0\\
     b^{new}=-E_{1}-y_{1}x_1^Tx_2\left(\lambda_{1}^{new}-\lambda_{1}\right)-y_{2}x_2^Tx_2\left(\lambda_{2}^{new}-\lambda_{2}\right)+b&\lambda_2^{new}>0\\
     b^{new}=\frac{1}{2}b^{new}&\lambda_1^{new},\lambda_2^{new}=0
     \end{cases}
     $$
     
     > 在两个新$\lambda$都大于零或都等于零的时候随便选一个算就行了，只不过在都等于零的时候需要取一半

4. 于是一直重复选样本$x_1,x_2$，更新$\lambda$，更新$b$的操作，直到某一时刻找不到合适的样本$x_1$了，也就是遍历$x$之后取到的$|1-g_i|$的最大值已经足够小了，就可以宣布已经完成了SMO的更新

5. 此时我们手中拿着一堆已经完成优化的$\lambda^*$和$b^*$，把其带入$w^*$表达式求解就完了
   $$
   w^*=\sum^N_{i=1}\lambda_iy_ix_i
   $$

6. 终于我们完成了SVM的优化问题，拿到了分割线函数方程$(w^*)^Tx+b^*=0$，接下来我们就可以愉快的带入预测样本点预测结果了，根据其输出的正负性判断类别即可

SMO的过程就像是一个拧螺丝的过程，当一块木板上有一堆螺丝孔的时候，我们往往会先把一个螺丝拧到最紧，接着一个一个拧其他螺丝，然而当我们拧完一圈之后，却发现在拧别的螺丝的时候一开始拧的若干螺丝在这个过程中又松了，于是我们继续取那个最松的螺丝继续拧紧，直到我们在加固了若干次之后感觉所有螺丝都紧了，才决定结束，如果我们可以额外长出一堆手同时拧紧所有螺丝当然只需要拧一次足以，但是面对无法多几只手的现实，一个一个螺丝拧也不失为一种好的选择

### 软间隔

之所以不把软间隔和硬间隔放在一起说，主要还是因为软间隔实际上就是在硬间隔的基础上引入了一个正则项，本质上的流程和硬间隔其实没什么区别，但是由于其中进一步引入了变量$\varepsilon $，导致无论是推导KKT条件还是SMO算法看上去都显得十分复杂，容易看着看着就晕了，因此其推导KKT条件的部分单独拿出来说，在结尾也会提到加入了软间隔之后SMO将会如何变化

在硬间隔中我们提到，我们假设所有的样本点都是完全线性可分的，意味着你用一条线，就一定可以把一类样本分在线的一边，另一堆样本分在另一边，然而大多数时候情况并没有我们想象中那么理想，有些时候样本确实是可分的，但是其边界线却不那么明显，会有个别不听话的样本跑到线的另一边去，软间隔要做的就是针对那些跑到线另一边去的样本一些容忍度，即我允许你跑过去，但是不能跑太多了

![image-20230313192021389](https://s2.loli.net/2023/03/13/DdrSqLnP8eE3gcM.png)

在图中我们可以发现，假设我们选择了$A,B$作为支持向量，此时比如$C$点其实并没有落在支持向量所控制黄虚线的下面，但是软间隔允许这种情况发生，只不过它会根据$C$点和黄虚线的距离$a$给一个惩罚值$\varepsilon $，期望函数可以在惩罚值和支持向量间隔最大值之间找一个平衡

想要求得每一个点和支持向量控制线的距离其实十分简单，利用点到直线距离公式$|y_i(w^Tx_i+b)-c|$即可，只不过这个绝对值可不能乱取，由于我们只需要给没被分好的点一个惩罚，而对于本身就已经分的挺好的比如$D$点惩罚值设为$0$就行了（即没有惩罚），因此最终这个惩罚函数$k$写出来是这样的
$$
\xi_i=\max(0,1-y_i(w^Tx_i+b))\\
k_i=C\xi_i
$$
其中$\xi_i$被称为铰链损失函数（Hinge Loss），而$C$就是其惩罚的程度，和深度学习中的ReLU有点像，只对落在直线上方的项起作用，且对于作用项，输出值和其与直线的偏移程度成正比

有了它，我们就可以改变我们的目标函数和约束条件了，顺便我们还可以直接推出加了惩罚项之后的KKT条件
$$
\begin{align*}
&\begin{cases}
\xi_i =\max\left(0,1-y_i(w^Tx_i+b)\right)\\
\min \frac{1}{2}w^Tw+C\sum^N_{i=1}\xi_i\\
y_i(w^Tx_i+b)\ge1-\xi_i\\
\xi_i\ge0
\end{cases}\\
\Rightarrow
&\begin{cases}
\mathcal{L}={\textstyle{\frac{1}{2}}}w^Tw+C\sum_{i=1}^{N}\mathbf{\xi}_{i}+\sum_{i=1}^{N}\lambda_{i}\left(1-y_{i}\left(w^Tx_i+b\right)+\xi_{i}\right)-\sum^N_{i=1}\mu_i\xi_i\\
\underset{\lambda,\mu}{\max}\underset{w,b,\xi}{\min}\mathcal{L}\\
\lambda\ge0\\
\mu\ge0
\end{cases}\\
\Rightarrow
&\begin{cases}
\frac{\partial \mathcal{L}}{\partial w}=w-\sum^N_{i=1}\lambda_iy_ix_i=0\\
\frac{\partial \mathcal{L}}{\partial b}=\sum^N_{i=1}\lambda_iy_i=0\\
\frac{\partial \mathcal{L}}{\partial\xi_i}=C-\lambda_i-\mu_i=0\\
\lambda_i\left(1-y_i(w^Tx_i+b)+\xi_i\right)=0\\
\mu_i\xi_i=0\\
i=1,\dots,N
\end{cases}
\end{align*}
$$
由于又多了$N$组互补松弛条件$\mu_i\xi_i=0$，因此我们似乎可以拿它来做点事情，我们都知道当$\xi_i>0$的时候样本点一定是越界了，此时根据互补松弛条件则一定有$\mu_i=0$，将其带入$C-\lambda_i-\mu_i=0\Rightarrow\lambda_i=C$，再结合之前已经推导过的松弛互补条件，则一定有
$$
\begin{cases}
x_i分类正确 & y_i(w^Tx_i+b)>1 & \lambda_i=0\\
x_i是支持向量 & y_i(w^Tx_i+b)=1 & 0<\lambda_i<C\\
x_i要被惩罚 & y_i(w^Tx_i+b)<1 & \lambda_i=C
\end{cases}
$$
拿着这玩意，就可以快乐的进行接下来的SMO了，什么你说你不会做？其实非常简单，因为我们发现搞到最后被影响的只有$\lambda$的范围而已，在进行$\lambda$更新的时候要注意其值不能超过$C$，即当$\lambda^{new}>C$的时候让其$=C$就完了，然后在选$x$的时候也不能无脑取最大，优先选择$0<\lambda_i<C$的$x_i$就行了，其余的包括$b$的更新，循环过程其实都是一样的，此处就不详细提了

### 核函数

我们一直都在假设所有数据点至少是线性可分的，然而实际当中大多数情况样本点其实并不线性可分，这么一来难道之前推了半天的式子都没用吗？当然不是，我们来看我们当时用来求解$\lambda$的式子
$$
\underset{\lambda}{\max}\left(\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_j{\color{red}\boldsymbol{x_i^Tx_j}}\right)\\
$$
当时我们只关注到了这个式子是一个仅与$\lambda$有关的式子，但是我们没有注意到在在求解$\lambda$的过程中我们其实并不需要完全知道每一个样本点的向量长什么样，而是只需要他们互相之间的点乘结果！

你可能不知道这意味着什么，这意味着SVM的复杂度和样本维度无关！在前一节介绍SMO的时候我们就会发现，SVM求解模型时的大部分时间都拿去求解$\lambda$了，相对应的，在求解$\lambda$的过程中我们发现$x_i^Tx_j$永远都是绑定在一起的，也就是说，在这个过程中我们只需要一开始求解出每一个样本相互之间的点乘结果即可，之后直接拿着这个点乘结果去用，除了最后求解$w^*$的时候需要管一下$x$长什么样，其余时间真实样本长什么样根本就不重要！

既然样本维度和时间复杂度没什么关系，那么原本线性不可分的样本点就有救了，因为我们虽然不能像神经网络一样直接把非线性映射后的结果代替原有样本$x$，但是我们可以把这个结果拼接在$x$后面，反正$x$到后面都是点积出来一个值，哪怕我给他增加一亿维只要能算点积那对于求解就不是问题

![](https://www.pianshen.com/images/203/bc90b628d86cf6b885b51c2c3d3b0ce3.png)

更进一步，甚至我都可以不用知道对$x$增加维度之后$x$究竟长什么样，如果我们可以直接获得在某种增维方式之后$x'$的点乘结果和原有的$x$之间的关系，即有一个函数$\mathcal{K}(x_1,x_2)=(x'_1)^Tx'_2$，那么我们就可以跳过求解$x'$的过程，直接把$x$丢进这个函数$\mathcal{K}$里取他的输出就行了

**这种直接跳过升维操作直接以原维度样本作为参数进行计算获取点乘结果的函数就叫做核函数**

事实上这种函数也是容易找到的，我们先来看看点乘在数学运算中到底代表了什么，我们令两个向量$x=(x_1,x_2),y=(y_1,y_2)$，则$x^Ty=xy^T=x_1y_1+x_2y_2$这非常简单，然后我们要对$x,y$进行一些神奇的变化，我令$x'=(x_1^2,\sqrt2x_1^2x_2^2,x_2^2),y'=(y_1^2,\sqrt2y_1^2y_2^2,y_2^2)$，这样我们等于说就给原本二维的$x,y$升了一维，此时我们可以尝试算一算他们的点乘
$$
\begin{align*}
(x')^Ty'&=x_1^2y_1^2+2x_1^2x_2^2y_1^2y_2^2+x_2^2y_2^2\\
&=(x_1y_1+x_2y_2)^2\\
&=(x^Ty)^2\\
&=\mathcal{K}(x,y)
\end{align*}
$$
我们发现，原来我们只需要简单的给原样本的点乘结果加个平方就等于实现了一次升维，事实上容易证明你给他加多少次他实际上就升维了多少次，这种核函数叫做多项式核，这种核听起来挺美好，但是由于你还是得手动指定升到多少维因此仍有局限性

令一个著名的核函数叫做高斯核，他长这样
$$
\mathcal{K}(x,y)=e^{\left(-\frac{\left \|x-y\right \|^{2}}{2\sigma^{2}}\right)}
$$
我们从他的指数部分化简，且中间使用指数形式的泰勒展开，同时为了计算方便我们令$\sigma^2=1$（在实际情况中也是作为超参数进行调整）
$$
\begin{align*}
-\frac{\left \|x-y\right \|^{2}}{2\sigma^{2}}&=\frac{1}{2\sigma^2}(-x_{1}^{2}+2x_{1}y_{1}-y_{1}^{2}-x_{2}^{2}+2x_{2}y_{2}-y_{2}^{2})\\
&=\frac{1}{2\sigma^2}(-x^2-y^2+2x^Ty)\\
&=-\frac{x^2+y^2}{2}+x^Ty\\
\Rightarrow e^{\left(-\frac{\left \|x-y\right \|^{2}}{2\sigma^{2}}\right)}
&=e^{-\frac{1}{2}\left(x^2+y^{2}\right)}e^{x^Ty}\\
e^{x}=\sum_{n=0}^{\infty}{\frac{x^{n}}{n!}}&\Rightarrow
e^{-\frac{1}{2}\left(x^2+y^{2}\right)}\sum_{n=0}^{\infty}{\frac{\color{red}(x^Ty)^{n}}{n!}}
\end{align*}
$$
我们发现，高斯核在结合了泰勒展开之后展开的结果居然是一个趋向于无限次累加的多项式核（红色标注处），并且为了防止加的过多导致数值过大，他还贴心的为每一个多项式结果都加了一个阶乘级别的权值，使得次数越低的升维结果占的比重越大

从另一个角度来说，点乘也可以看作是衡量两个向量之间相似度的标准，**因此高斯核的计算结果也可以看作是两条向量在通过多项式核升到无限维之后相互之间的相似程度**

一般上我们会令$\frac{1}{2\sigma^2}=\gamma$作为更简单的超参数，并且由于其也是指数级别的，我们可以通过调整$\gamma$来控制模型的精度，比如当我增大$\gamma$，会发现原本看上去相似度还可以的两条向量的相似度急剧下降，反映到最终的模型上，就会让高维上的两组点更不容易区分（因为可能同类点之间的相似度也不高），导致最后得出的$w^*$维度过高，于是可能会出现过拟合的问题，相反，过小的$\gamma$会让高维中的点非常容易被区分，训练好的模型容易出现欠拟合

### SVM vs MLP

如此多的篇幅来介绍SVM的推导过程，也体现出了SVM的重要性，接下来我们通过比较他和传统多层感知机模型，对本章进行总结，同时也可以进一步发现SVM的优劣势

- SVM理论上只能解决二分类问题，而MLP可以控制输出神经元的多少通过添加softmax来实现多分类
- SVM的求解过程使用了SMO，本质上还是通过偏导为零的方式即求部分参数解析解来进行最优化，而MLP则采用梯度下降逼近极值而非直接求解
- SVM的时间复杂度和样本数量正相关，但是与样本维度基本无关，且理论上无法减少必需的优化参数量，而MLP的时间复杂度其实与样本数量和样本维度都有关，但是可以通过增减神经元数量控制复杂度
- 因此SVM更适合被用于小型训练集且样本维数较多的场景，而MLP对小型训练集由于在不降维的情况下输入层参数永远与样本维度相同，因此十分容易产生过拟合
- SVM需要调整的超参数是核函数中的$\gamma$和软间隔中引入的$C$，而传统MLP中只需要调整梯度下降学习率$\epsilon$即可

## 指数族分布

聊完了几种不同的评价函数和求解方法，自然的，接下来我们就要着眼于模型上的问题了，在之前，你会看到我们一言不合就开始上高斯模型，但是究其原因，为什么高斯模型会被广泛的使用在各种需要假设分布的问题中，其实就是因为其属于指数族分布，因此这一章我们就以高斯分布为例，看看指数族分布到底为似然问题中模型的选择提供了怎样的思路

### 高斯分布指数族形式

既然我们说高斯分布是一种指数族分布，那么我们就需要知道指数族分布到底是什么，指数族分布的公式如下
$$
P(x\mid\eta)=h(x)\exp(\eta^{T}\phi(x)-A(\eta))
$$
先不管其中的各种符号的实际含义，只需要知道这是一个在知道了参数$\eta$的情况下输入$x$求解分布概率的密度函数即可，可以发现从定义上来说就和高斯分布非常相似，接下来我们对最简单的一维高斯分布进行变形
$$
\begin{align*}
P(x\mid\theta)&={\frac{1}{{\sqrt{2\pi}}\sigma}}\exp\left(-{\frac{(x-\mu)^{2}}{2\sigma^{2}}}\right)\\
&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right)\\ &=\exp(\log(2\pi\sigma^{2})^{-\frac{1}{2}})\exp\left(-\frac{1}{2\sigma^{2}}(x^{2}-2\mu x+\mu^{2})\right)\\
&=\exp\left(-\frac{1}{2\sigma^{2}}(x^{2}-2\mu x)-\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\log(2\pi\sigma^{2})\right)\\
&=\exp(
{\color{red}\underbrace{\begin{pmatrix}
\frac{\mu}{\sigma^2} & -\frac{1}{2\sigma^2}
\end{pmatrix}}_{\eta^T}}
{\color{green}\underbrace{\begin{pmatrix}
x\\
x^2
\end{pmatrix}}_{\phi(x)}}
-{\color{blue}\underbrace{\left(\frac{\mu^{2}}{2\sigma^{2}}+\frac{1}{2}\log(2\pi\sigma^{2})\right)}_{A(\eta)}}
)\\
\eta=\begin{pmatrix}
\eta_1\\
\eta_2
\end{pmatrix}=
\begin{pmatrix}
\frac{\mu}{\sigma^2}\\
-\frac{1}{2\sigma^2}
\end{pmatrix}&\Rightarrow
\exp\left(
\begin{pmatrix}
\eta_1 & \eta_2
\end{pmatrix}
\begin{pmatrix}
x\\
x^2
\end{pmatrix}-
\left(-\frac{\eta_{1}^{2}}{4\eta_{2}}\,+\,\frac{1}{2}\,\log(-\,\frac{\pi}{\eta_{2}})\right)
\right)
\end{align*}
$$
可以发现变形操作还是比较容易的，同时在变形的过程中我们也发现$\eta$是一个列向量，$\phi(x)$是一个逐元素标量函数，$A(\eta)$则是一个全元素标量函数，需要注意的是此处只讨论$x$是一个值的情况，对于$x$是向量的情况相对应的$\eta$也要变成一个矩阵，此处不讨论

在知道了高斯分布原来真的是指数族分布之后，我们在下一节详细研究一下其中的这些函数的含义

### 对数配分函数

我们都知道概率密度函数在实数域上的积分为$1$，但是明明概率密度函数就只是一个函数而已，万一我推导出一个函数的积分是$1.1$，难道就不能作为概率密度函数了吗

由于每一个函数都有其特殊性，我们无法保证其积分一定为$1$，因此就有了配分函数的概念，事实上这个概念也是非常直觉的，对于任意在定义域积分结果为实数的函数$f$来说，只需要将其除以自己的积分结果则一定可以保证最终结果为$1$，因此理论上来说，任何值域在正数上且积分为有限值的函数都可以构造为一种概率密度函数

接着我们来看看指数族分布的函数表示，尝试寻找一下他的原函数长什么样
$$
\begin{align*}
P(x\mid\eta)&=h(x)\exp(\eta^{T}\phi(x)-A(\eta))\\
\Rightarrow P(x\mid\eta)&=h(x)\exp(\eta^{T}\phi(x))\exp(-A(\eta))\\
\Rightarrow P(x|\eta)\exp(A(\eta))&=h(x)\exp(\eta^{T}\phi(x))\\
\Rightarrow \int_{x}P(x|\eta)\exp(A(\eta))d x&=\int_{x}h(x)\exp(\eta^{T}\phi(x))d x\\
\Rightarrow \operatorname{exp}(A(\eta))&=\int_{x}h(x)\exp(\eta^{T}\phi(x))d x\\
\Rightarrow P(x\mid\eta)&=\frac{h(x)\exp(\eta^{T}\phi(x))}{\int_{x}h(x)\exp(\eta^{T}\phi(x))d x}
\end{align*}
$$
可以发现其原函数原来就是除了$A(\eta)$剩下来的部分，也就是说这个$A(\eta)$在上一节中我们推出来似乎有点复杂并不是白复杂的，他的结构必须满足为原函数的积分，不过由于其外部还加了一个$\exp$指数，因此我们更喜欢叫他对数配分函数，意味着$A(\eta)$实际上是配分函数的$\log$形式

### 充分统计量

了解了对数配分函数，接下来来看看另一个有些特别的函数$\phi(x)$，我们通过推导已经发现他和一般的函数都不太一样，作为一个逐元素标量函数，他的输出是一排向量而不是一个值，那么为什么要搞这么一个怪怪的函数出来呢，我们不妨在上一节推导的基础上再往下推推看
$$
\begin{align*}
\exp(A(\eta))&=\int_{x}h(x)\exp(\eta^{T}\phi(x))d x\\
\Rightarrow\frac{\rm d }{\rm d \eta}\exp(A(\eta))&=\frac{\partial }{\partial \eta}\int_{x}h(x)\exp(\eta^{T}\phi(x))d x\\
\Rightarrow\exp(A(\eta))A'(\eta)&=\int_{x}h(x)\exp(\eta^{T}\phi(x))\phi(x)d x\\
\Rightarrow A'(\eta)&=\int_{x}\frac{h(x)\exp(\eta^{T}\phi(x))}{\exp(A(\eta))}\phi(x)d x\\
\Rightarrow A^{\prime}(\eta)&=\int_{x}h(x)\exp(\eta^{T}\phi(x)-A(\eta))\phi(x)d x\\
\Rightarrow A^{\prime}(\eta)&=\int_{x}P(x|\theta)\phi(x)d x\\
\Rightarrow A^{\prime}(\eta)&=E_{P(x|\eta)}[\phi(x)]\\
\Rightarrow \begin{pmatrix}
\frac{\partial A(\eta)}{\partial \eta_1}\\
\frac{\partial A(\eta)}{\partial \eta_2}
\end{pmatrix}&=\begin{pmatrix}
E[\phi_1(x)]\\
E[\phi_2(x)]
\end{pmatrix}
\end{align*}
$$
如果对推出来这个结果没什么概念，我们可以用高斯分布的模型对结果进行一个验证
$$
A(\eta)=-\frac{\eta_{1}^{2}}{4\eta_{2}}\,+\,\frac{1}{2}\,\log(-\,\frac{\pi}{\eta_{2}}),\phi(x)=\begin{pmatrix}
x\\
x^2
\end{pmatrix}\\
\begin{pmatrix}
\frac{\partial A(\eta)}{\partial \eta_1}\\
\frac{\partial A(\eta)}{\partial \eta_2}
\end{pmatrix}=\begin{pmatrix}
-\frac{\eta_1}{2\eta_2}\\
\frac{\eta_1^2}{4\eta_2^2}-\frac{1}{2\eta_2}
\end{pmatrix}=\begin{pmatrix}
\mu\\
\mu^2+\sigma^2
\end{pmatrix}=\begin{pmatrix}
E[x]\\
E[x^2]
\end{pmatrix}
$$
这样我们就验证了我们推导结果的正确性，与此同时，我们发现虽然单看$\phi(x)$这个函数似乎没什么意义，但是如果将其和均值结合，我们就会发现当我们拿到一堆样本$x$，如果只求解这组样本中的$E[\phi(x)]$就可以间接通过$A^{\prime}(\eta)=E[\phi(x)]$反向求解出所有的参数$\eta$，反映到高斯分布中，这也是为什么在[第一章第一节](#极大似然估计（MLE）)中我们通过极大似然估计推出高斯分布的参数$\mu,\sigma$的极大似然解中只需要样本的均值（$E[x]$）和方差（$E[x^2]-E[x]^2$）就可以构建出整个模型的原因

**由于把$\phi(x)$看作统计量的指标时求解其在样本上的均值就可以直接构建出拟合模型，因此我们称$\phi(x)$为指数族分布的充分统计量**

换而言之，当我们有一堆样本且现在想要用这些样本去生成一个指数族分布模型的时候（即一个似然的过程），我们当然可以通过MLE等方法求解参数，但是另一个更简便的方法是直接求解这个指数族分布模型的充分统计量的均值，我们拿着充分统计量的（对于高斯模型来说就是$E[x]=\frac{1}{N}\sum^{N}_{i=1}x_i$和$E[x^2]=\frac{1}{N}\sum^{N}_{i=1}x_i^2$）均值就等于拿到了整个模型，庞大的样本信息自然可以直接丢弃了

有了充分统计量的辅助，指数族模型的似然求解将变得异常简单，但是模型求解的方便并不是我们喜欢用它的根本理由，一定还有什么让其具有和别的分布模型表现更优越的地方

### 信息量和熵

现在我们突然拿到了一堆样本，我们希望用这些样本似然出一个分布模型，但是我该用什么模型呢？我当然希望我选择的这个模型最好是这些样本背后真正服从的分布模型，然而我怎么知道一堆样本最有可能服从什么分布呢？

在此之前，请允许我稍微离开一下机器学习的内容，到隔壁信息论的范围中去借一点理论知识，接下来要介绍一下信息量和熵两个概念，首先我们看一下最经典的有关信息量的例子

> 信息奠基人香农（Shannon）认为“信息是用来消除随机不确定性的东西”。也就是说衡量信息量大小就看这个信息消除不确定性的程度。
>
> “太阳从东方升起了”这条信息没有减少不确定性。因为太阳肯定从东面升起。这是句废话，信息量为0。
>
> “吐鲁番下中雨了”（吐鲁番年平均降水量日仅6天）这条信息比较有价值，为什么呢，因为按统计来看吐鲁番明天不下雨的概率为98%（1-6/300），对于吐鲁番下不下雨这件事，首先它是随机不去确定的，这条信息直接否定了发生概率为98%的事件------不下雨，把非常大概率的事情（不下雨）否定了，即消除不确定性的程度很大，所以这条信息的信息量比较大。这条信息的情形发生概率仅为2%但是它的信息量却很大，上面太阳从东方升起的发生概率很大为1，但信息量却很小。
>
> 从上面两个例子可以看出：信息量的大小和事件发生的概率成反比，香农定义信息量的公式有$\cal I(x)=-\log(p(x))$
>

有了信息量的定义，那么我们接下来看看信息熵的公式
$$
\mathcal H(x)= -\sum p(x) * \log(p(x))
$$
其实在理解了信息量的含义之后，如果我们再把$x$看作在某模型下的所有情况，可以发现信息熵的公式其实就是某一个概率模型下的信息量加权和，其权值正是每一种情况对应的概率值，意味着信息熵越大，这个模型可以表现出的总信息越多，这个模型的不确定性越高

不确定性这个词听着听抽象的，举个例子，假如我有一枚硬币，现在我预测他抛出正面概率为$0.5$，当然反面也是$0.5$，此时这个模型是不确定的，因为假如现在我告诉你我抛了一次硬币，你并不好猜这个硬币会抛到什么面，但是如果此时我说这个硬币抛出正面的概率是$0.9$，抛出反面的概率是$0.1$，那这个模型相较于上一个模型不确定性就低了，相信有脑子的人都会预测抛出来的是正面，正是因为我们认为抛出正面这件事是比较确定的

如果你计算这两个模型的信息熵，就会发现前者的熵比之后者更加高，也就是其不确定度更高

### 最大熵和均匀分布

假设现在我们拿到了一堆样本，你唯一知道的是这堆样本有几种类型，但是就到此为止了，你没有这堆样本来源的和分布的任何信息，比如我告诉你我手上样本的值就只有两种，但它可能是一群人的性别，也可能是抛了一百次硬币的数值汇总，甚至可能只是一堆毫无意义的零和一

此时你对样本一无所知，现在摆在你面前的问题是你需要预测这个样本最可能属于什么分布，听起来有点强人所难，但是由于我们有了信息熵这个工具，意味着我们可以定量的衡量各种模型的不确定度，由于此时我们仅知道样本的种类数其他一无所知，因此按理来说我们希望这个模型也应该是满足样本种类数的所有模型中最不确定的，也就是我们需要求解满足种类分布的所有模型中信息熵最大的一个，这就是最大熵原理

与其说最大熵是求解所有模型中不确定最大的那个，倒不如说只是你给的信息不足以让我假设不确定度小一些的模型，在最大熵理论的基础和给定的先验条件下，样本满足的模型是绝对唯一的

于是我们假设我们得到的样本种类为$N$，通过求解信息熵的极大值看看在完全未知的情况下模型应该是什么样的，即求解以下式子极值
$$
\begin{cases}
\max \mathcal{H}(x)=\max-\sum_{i=1}^N p_i * \log(p_i)\\
s.t.\quad \sum_{i=1}^Np_i=1
\end{cases}
$$
使用拉格朗日乘子法
$$
L(P,\lambda)=\sum_{i=1}^{N}p_{i}\log p_{i}+\lambda(1-\sum_{i=1}^{N}p_{i})\\
{\frac{\partial L}{\partial p_{i}}}=\log p_{i}+p_{i}\,{\frac{1}{p_{i}}}-\lambda=0\\
p_{i}=\exp(\lambda-1)\\
1-\sum_{i=1}^{N}\exp(\lambda-1)=0\Rightarrow\exp(\lambda-1)=\frac{1}{N}=p_i
$$
可以发现求解结果的$p_i=\frac{1}{N}$，这不就是均匀分布吗，于是我们知道了，在没有任何已知情况下，我们假设模型所有种类的概率都是相等的是最合理的，这也符合我们的常识，比如我说随机从$0,1$中抽一百个数，正常人也会猜$0,1$的个数是接近两两对半分的，即两种数的取值概率相等

### 统计量函数最大熵

在什么条件都不知道的情况下，均匀分布模型是能够拟合数据情况的最优模型，但是在现实中，大多数情况我们还是知道一些样本信息的，毕竟样本都拿到手上了，偷偷看一眼也不是很过分，在我们观察样本的过程中，就产生了一定量的先验信息，那么在这些先验信息的基础上，我们又可以获得怎么样的最优模型呢

现在我们假设我们拿到的先验信息是从样本集抽取出来的统计特征，比如一堆样本的均值方差，当然不止如此，你可以任意取样本中你喜欢的统计特征，我们将这些统计量排成一个向量记作$f(x)=\begin{pmatrix}f_1(x)\\\vdots\\f_n(x)\end{pmatrix}$，比如$f_1(x)=x$意味着我取样本的所有值的均值$E[x]$作为一个先验信息，在我取得了$n$个样本的先验信息$f(x)$之后接着我就带着这些先验信息看看在满足这些先验信息的基础上最大熵模型长什么样，意味着此时比起什么都不知道的情况，模型唯一多了一点知道的信息就只是我们给他的若干统计量先验信息，样本具体长什么样他还是不知道

我们令在样本上获取的先验信息$E[f(x)]=\Delta$，则求解模型变成
$$
\begin{cases}
\max \mathcal{H}(x)=\max-\sum_{i=1}^N p(x_i) * \log(p(x_i))\\
\sum_{i=1}^Np(x_i)=1\\
E_p[f(x)]=\sum_{i=1}^Np(x_i)f(x)=\Delta
\end{cases}
$$
他表示我们通过最大熵求得的模型必须同样满足先验的统计量信息，比如我在样本上发现样本的均值都是$0$，那么我要求求解获得的模型均值也为$0$，同样使用拉格朗日乘子法
$$
\mathcal{L}(x,\lambda_{0},\lambda)=\sum_{i=1}^Np(x_i)\log p(x_i)+\lambda_{0}(1-\sum_{i=1}^Np(x_i))+\lambda^{T}(\Delta-\sum_{i=1}^Np(x_i)f(x)),\\
\frac{\partial L}{\partial p(x_i)}=\mathrm{log}\,p(x_i)+1-\lambda_{0}-\lambda^{T}f(x)=0\\
p(x_i)=\exp({\color{red}\underbrace{\lambda^{T}}_{\eta^T}}{\color{green}\underbrace{f(x)}_{\phi(x)}}-{\color{blue}\underbrace{(1-\lambda_{0}}_{A(\eta)})})
$$
我们发现求解出来正是指数族分布的公式，我们假设的统计量函数正代表了指数族分布中的充分统计量，意味着对于任意包含统计量先验信息的样本，最大熵基础上求解的不确定度最高的模型正是指数族分布，当我们假设我们获取统计量是样本的均值$E[x]$和平方均值$E[x^2]$情况下，带入$f(x)$求得的$\lambda,\lambda_0$正是高斯分布

与此同时这个例子也进一步说明了为什么在求解高斯模型的时候只需要样本的充分统计量就可以构建模型，并不是高斯模型不想知道别的样本信息，而是高斯模型公式就是产生于只知道均值和方差两个先验信息的条件下的最大熵模型，假如你真的硬要把其他统计量放进去，那么根据最大熵原理最优模型就会改变，你就不能再用高斯模型的公式求解问题了

## EM算法

如果你有印象，我们[曾经说过](#模型 评价 求解)，在某些时候我们虽然能够构造出评价函数，但是存在某些情况可能评价函数在求导之后无法求解出解析解，在此之前，我们往往使用梯度下降或者SVM中的SMO通过这种不断迭代逼近的方法来进行求解，而EM算法，同样提出了一种更加广义的迭代算法，使在面对无解析解的评价函数时能够使用一种神奇的迭代算法逼近函数的极值

EM算法准确来说并不是指某一种特定的算法，而是指一种具有通用性泛化能力的求解方法，无论是我们曾经说过的SMO算法，还是在聚类算法中最出名的k-means算法，从过程上来说，我们都可以说他们有着EM算法的影子，他同时也成为了很多结论性工作的解释方向，他的应用之广泛给了我们不得不学习的理由

### Jensen不等式

在引入KKT条件的时候，我们曾经引入过凸函数的定义，回顾一下，我们提到当某一个函数上的任意两点之间的连线值始终大于这两点之间的函数值，则我们就说这个函数是一个凸函数，或者更通俗的理解，函数图像画出来始终是呈现一个碗一样的形状的函数就是凸函数（下凸函数），他的代数定义如下
$$
t f(x_{1})+(1-t)f(x_{2})\geq f\left(t x_{1}+(1-t)x_{2}\right)
$$
Jensen不等式说起来就十分简单，因为他只是在凸函数的定义上稍微变化了一下，具体来说，我们发现对于凸函数的定义来说，似乎加法两侧的系数$t$和$1-t$相加就等于1，于是Jensen不等式就在这里进行了延展，我们让某一组数$\lambda_i$的累加结果为1，即$\sum_{i=1}^n\lambda_i=1$，对于凸函数的定义就是$n=2$的特殊情况，对于一般情况，则可以写出
$$
f(\sum_{i=1}^{M}\lambda_{i}x_{i})\leq\sum_{i=1}^{M}\lambda_{i}f(x_{i})
$$
理解这个式子还是比较简单的，但是我们怎么证明他是对的呢，诶，我们发现我们已经知道了$n=2$的时候是成立的，而$n=1$的时候也显然是成立的，现在要让我们推导$n=k$的情况，这不就是数学归纳法吗，事实上Jensen不等式的正确性正是用数学归纳法推导的，此处我们就不推了，如果你也懒得推，那就把它当作是一个结论记住吧

现在我们换一种思路，之前我们只是为了满足凸函数的定义让$\sum_{i=1}^n\lambda_i=1$，但是此时我们如果把每一个$\lambda_i$都看作是一个概率，我们发现这么一来，如果我们只挑着不等式左边的$\sum_{i=1}^{M}\lambda_{i}x_{i}$来看，我们再把$x_i$看作是所有的样本空间的样本点，那么这个连加的操作就变成了求解在样本空间的期望了，而这一特性在$f(x_i)$上同样成立，只不过如果我们将$f(x_i)=y_i$，这个样本空间$y$和样本空间$x$是不一样的，因此从概率角度来说Jensen不等式还有下面这种写法
$$
\mathbb{E}[f(X)]\leq f(\mathbb{E}[X])
$$
上面的一段话如果不是很理解，那么不妨来看下面这个例子，现在有一枚只能骰出三个数字的骰子，我们知道他骰出1的概率是0.3，2的概率是0.5，3的概率是0.2，因为不存在能骰出第四个数，因此概率的总计值$0.3+0.5+0.2=1$，意味着我只要骰出骰子了，就一定会产生点数，此时如果假设我骰了无限次，请问最后点数的平均值为多少？

计算方法很简单，这也就是期望的定义，即$\mathbb{E}(x)=\sum_{i=1}^3p(x_i)x_i=0.3*1+0.5*2+0.2*3=1.9$，也就是说在骰出足够多次之后，点数的均值会在1.9附近

但是这并没有结束，现在我们的游戏规则是，除了正常的能骰出三个数字的骰子之外，我们还有一个平方道具，这个道具可以使用任意次，他的功能是让从上一次使用道具至今的所有点数的均值做平方操作，比如我骰了三次，分别是1,2,3，显然他们的均值是$(1+2+3)/3=2$那么我使用一次这个道具，最后的输出点数就是$2^2=4$，然后我又骰出了1,1，他们的均值是1，再使用一次这个道具，输出的结果就是$(2^2*3+1^2*2)/5=2.8$

那么问题来了，假设还是上面那个三面骰，现在让你骰1000次，如何使用道具才能尽可能让输出结果最大呢

如果我们把这个道具看作是$f$，由于他是一个凸函数，那么根据Jensen不等式，在我们的运气满足平均水平的时候，由于理论上有$\mathbb{E}[f(X)]\leq f(\mathbb{E}[X])$，因此我们的最优策略是等到1000次骰完之后统一使用一次道具才是收益最高的

什么，你说还是不理解，那就看看下面这张图吧，他取了a,b的概率都为0.5的最特殊情况，此时的期望正是在他们的正中心，实际上我们可以知道期望的值一定是在a,b之间的，而这其实也就是凸函数的定义，通过这种特殊情况，我们发现$\mathbb{E}[f(X)]\leq f(\mathbb{E}[X])$也是显然的

![img](https://pic1.zhimg.com/80/v2-c6ea0537af6cd4ceb25705c6ccc8575c_720w.webp)

与此同时，同样很容易推得，当$f$确定的情况下，当且仅当只存在一个$x$的时候，也就是比如骰一个只有一面的骰子，此时当然只有一种情况，无论$f$是什么不等式的左右两边都是一样的

### 高斯混合模型（GMM）

在此之前，如果你还记得，我们在GDA的时候埋下了一个伏笔，当时我们提到了，当我们拿到了一堆样本且我们知道这些样本里面每个样本属于哪个类的时候，我们会先假定两个类都服从同方差的高斯分布，接着根据不同样本不同的类别标签，分别计算两个类的均值和方差从而确定最终的两个高斯模型

其实已经完美的解释了什么是高斯混合模型，只不过在GDA中我们求解两个高斯模型的目的是为了比较每一个样本点在两个高斯模型下被取样出来的概率从而实现分类，这也是为什么我们需要让两个高斯模型的方差统一，同样是为了让分类问题变得更加公平，但是在更普遍的情况中，我们其实并不关心样本到底属于什么类别，甚至于可能我都不知道这些样本中有哪些类别，我们只是单纯的需要一个包含了多个不同高斯分布的模型而已

举例来说，我现在希望拟合一个全中国人身高的模型，之后我就能够通过这个模型生成无限多的人类身高样本了，这时候我们会发现，首先男人和女人的平均身高一定是不一样的，意味着很可能如果我们把所有人的身高排布在坐标轴上，那么男人和女人的平均身高周围或许都会密集的聚集着很多样本，这时候显然我们不能只用一个高斯模型进行拟合，于是，我们可以假设男人和女人的身高分别满足两个不同的高斯模型

所以高斯混合模型的模型函数长什么样呢，其实非常简单，当我们知道若干个高斯模型的参数之后，只需要将他们加起来就可以了
$$
p(x)=\sum_{i=1}^N\alpha_i\mathcal{N}(\mu_i,\sigma_i),\sum_{i=1}^N\alpha_i=1
$$
其中的$\alpha$其实和我们学习指数族分布的$A(\eta)$一样是一个配分函数，只是为了满足这个模型的概率密度函数在定义域积分为$1$而已，与此同时他还有控制每个模型的权重的作用（比如男人比女人会多一些，那么给男人的高斯分布的权重就可以大一些）

#### GMM的极大似然

首先我们需要确定的是对于广义的GMM来说，我们其实是不知道每一个样本的标签的，我们只是拿到了一堆样本知道他们是一堆身高的数据，但是我们并不知道哪些样本是男的，哪些是女的，更确切的说，我们根本不关心这个数据是男的还是女的，我的目的只是根据数据拟合一个拥有两个高斯分布的模型而已

好了，现在模型定下来了，下一步不就是求解参数吗，我们一看，哇，这模型刚好又是个概率密度函数，同时样本同样也是满足独立的，这不是明摆着要用极大似然估计作为评价函数的问题吗，于是你开心的把它代入了极大似然估计的公式中
$$
\hat\theta = \arg\max_\theta\sum_{i=1}^N\log(\sum_{i=1}^N\alpha_i\mathcal{N}(\mu_i,\sigma_i))
$$
看上去好像挺正常的，我们之前不也是这么做的吗，的确没错，但是接下来你就会发现出现问题了，第一个问题就是你发现你没办法对这个函数进行求导，在之前，$\log$里面往往都是连乘，或者说我们之所以加$\log$就是为了简单的求导连乘的参数，然而在这里$\log$里面变成了连加，而我们如果把$\log$去掉，那么外边的连加又会变成连乘，陷入了无法求解的死循环中，这可怎么办呢

### 隐变量估计

我们通过高斯混合模型的求解过程引导出了我们在求解方法上遇到的问题，这种模型也确定了，评价函数也没什么问题，唯独在最后的求解方法上卡住的感觉一定不好受，为了解决这个问题，我们必须对其进行泛化，我们统一称这种类型的问题为隐变量估计问题

所以隐变量估计问题是一个什么问题呢？其实，它所要探讨的问题相较于GDA这种经典的分类问题来说其实就只少了一点，即此时我们不知道每个样本属于哪些种类而已

在这个问题中，男生和女生两个类别是样本没有提供给我们的，或者换种说法，男生和女生只是我们在知道了样本是什么东西之后根据经验得出来男女的身高分布可能分别属于不同分布，因此我们把我们假设出来的这两个类别叫做隐变量，我们在指定模型的前提下对这两个隐变量进行估计模型参数的过程就被称之为隐变量估计问题

同时需要注意，之所以叫隐变量估计问题，原因就是我们存在两组变量，第一个当然是抽样出这些样本的每个模型参数，而第二个就是我们每一个样本属于的类别隐变量，它比起我们正向推导的混合高斯模型，反而像是从另一种角度解释了混合高斯模型需要解决的问题，没错，这里的隐变量，正是混合高斯模型里面的每一个高斯分布

细心的小伙伴可能发现了，没有标签，根据先验信息进行的模型参数预测问题，这怎么和无监督学习那么像，没错，隐变量估计问题下的每一种解决方法，我们都可以称之为广义上的无监督学习

### EM迭代过程

如何求解隐变量的估计问题，或者说如何解决这种混合高斯模型的似然问题，自然就是EM算法需要干的事了，实际上，EM在学习起来是一种十分特别的方法，至少对我来说，在有实际问题的场景中，他的迭代过程显得是如此明确，但是一旦把这个过程使用数学进行推导和证明，各种符号和定义又会显得十分繁琐，让人不知从何下手，因此，在这一节中，将会较为简单的过一遍在实际场景下的EM迭代过程，之后，会将重点放在他的正确性证明和推导过程中

EM的迭代过程其实只需要看几张图，我们就用混合高斯模型的题目作为范例，我们假设我们现在有一堆一维的身高数据，每一个数据样本都是一个浮点值，并且我们知道这一堆样本是来自于男生和女生的身高且男女生身高的分布分别符合两个不同的高斯模型，而我们的目标就是找到这两个高斯模型，能够尽可能的拟合所有的数据点，换句话说，我们找到的模型应该就是最有可能或者最接近男女生实际高斯模型的模型

由于身高数据是一维的，因此我们可以将其表示在一个数轴上，比如这样

![image-20230504162936763](https://s2.loli.net/2023/05/04/oETeYKQj8cpzIDd.png)

接下来，我们就要使用EM算法找出最能够拟合这些点的两个高斯模型

1. 我们先随机初始化两组不同的参数$\mu_1,\sigma_1,\mu_2,\sigma_2$，这样我们就有了两个不同的高斯模型

   ![image-20230504163156176](https://s2.loli.net/2023/05/04/rwIj69GPsKgdMLU.png)

2. 此时，在第一步的假设前提下，我们有了第一组模型参数的确定值，此时我们就要使用这组模型参数去预测另一组参数也就是每一个样本点属于的隐变量的种类，我们把在已知分布预测样本种类的步骤称为E步

   ![image-20230504164706191](https://s2.loli.net/2023/05/04/urI5Ez4jFWCOePJ.png)

   如上图，我们和在GDA当中的预测步骤一样，对于每一个样本点，看看他在两个模型中被哪一个筛选出来的概率更高，我们就认为这一个样本点属于哪一个类别

3. 接下来，我们将所有样本点的种类当作已知量，扔掉第一步中假设的所有模型参数，那么现在问题就变成了简单的根据类别确定高斯分布的问题，我们只需要计算在第二步中得出来的两组样本的均值和方差就可以确定模型参数，这一步就是M步

   ![image-20230504170022715](https://s2.loli.net/2023/05/04/XimtSNQKov53rCg.png)

4. 至此算作一个循环，我们通过先确定隐变量种类，再反推模型参数的方式进行了模型参数的一次迭代，还需要注意的是，正因为隐变量被称之为隐变量，因此他们并不是我们最终求解的目标，我们也说了隐变量的种类都是我们假设出来的，他只是我们用来迭代模型参数的一个过程工具而已，接下来，我们只需要一直重复EM过程，最终模型参数会收敛到一个极值中，于是我们便得到了两个最优的模型参数，接下来只需要根据经验分配每一个模型属于哪一个种类即可（比如我认为蓝色模型最终表示男生的身高模型，黄色模型表示女生的身高模型）

   ![image-20230504170444038](https://s2.loli.net/2023/05/04/tgwEajq4ZsFdzKx.png)

   ![image-20230504170459788](https://s2.loli.net/2023/05/04/m5PxfHTDyS6wNMe.png)

   ![image-20230504170531328](https://s2.loli.net/2023/05/04/HaIPexOyqsuClRv.png)

   ![image-20230504170544522](https://s2.loli.net/2023/05/04/vzEw2uY1OaC7KXD.png)

综上所述，从这个生动的例子中可以看到，EM算法和我们曾经提到的SMO算法确实十分类似，都是通过几个变量固定一个之后另一个变化达到的互相迭代，最终收敛到一个最优值当中，只不过EM的这种迭代过程更加直观，不过我在开头就说过了，理解这个迭代过程是容易的，难就难在，这种你更新一下，我更新一下的方法到底为什么是正确的呢，我怎么知道你确定出来的模型参数是不是最优的结果，甚至你怎么知道他能够最终收敛，毕竟如果仅看这个迭代过程，相信没有一个人会觉得这玩意是合理的，这明显就是玄学

### EM算法推导

在这一节，我们要从隐变量估计的角度重新推导一次评价函数的式子，其实你会发现无论从哪个角度理解，最后的公式其实都是差不多的，混合高斯模型的似然问题只是隐变量估计问题中的一个特例而已

首先，我们还是要把极大似然估计写出来，然后尝试加入所谓的隐变量
$$
\hat\theta = \arg\max_\theta\sum^n_{i=1}\log p(x_i\mid\theta)
$$
#### 含隐变量的极大似然

我们仍旧以两个样本点为例，即此时我们知道这堆样本点来自于两个高斯模型，这样我们仍然令两个高斯模型的参数是$\theta=(\mu_1,\sigma_1,\mu_2,\sigma_2)$，并且每个样本点除了有一个数据$x_i$（比如对于前面的问题来说就是身高）之外，还有一个种类$z_i$，表示这个样本点来自于哪个高斯模型（比如$z_1$表示男生，$z_2$表示女生），对于这个问题来说，我们同样可以依样画葫芦写出他的极大似然估计式
$$
\begin{align*}
\hat\theta &= \arg\max_\theta\sum^n_{i=1}\log p(x_i\mid\theta)\\
&= \arg\max_\theta\sum^n_{i=1}\log\sum^2_{j=1} p(x_i,z_j\mid\theta)\\
&= \arg\max_\theta\sum^n_{i=1}\log \left(p(x_i,z_1\mid\theta)+ p(x_i,z_2\mid\theta)\right)\\
\end{align*}
$$
这一步其实就是边缘概率的公式而已，我们将隐变量$z_i$看作是另一个离散的概率分布，那么只需要累加每一个身高数据$x_i$和他属于的是男生还是女生的概率之和即可算出有关$x$的边缘概率，从而仍能达到正统极大似然估计也就是$\hat\theta = \arg\max_\theta\sum^n_{i=1}\log p(x_i\mid\theta)$公式

复习了这些内容，我们就可以对上述的式子进行泛化了，由于我们的问题可不只是男女生的身高问题，意味着隐变量$z_i$的个数我们是不知道的，同时我们也不一定只使用高斯模型进行模型拟合，因此$\theta$的内容也是不知道的，我们只能知道它会由多个参数组成，于是，对于存在着隐变量的问题来说，我们的求解目标就变成了如下式子
$$
\hat\theta = \arg\max_\theta\sum^n_{i=1}\log\sum_z p(x_i,z\mid\theta)
$$

> 注意这里虽然看上去只写了一个$\sum_zp(x_i,z\mid\theta)$，但是实际上是省略了$z$的下标的简略写法，并不是代表只有一个$z$，完整写法可以写成比如$\sum_{j=1}^mp(x_i,z_j\mid\theta)$这个样子，其中$m$表示隐变量$z$有几类（比如例子中的男生女生就是两类），此外其实如果我们不好对隐变量进行分类，例如$z$是一连串连续的数值，那么其中的连加还可以变成积分，从而写成$\int_zp(x_i,z\mid\theta)\mathrm{d}z$这样，不过由于积分不太好理解，因此在接下来的推导过程中还是会尽量使用连加进行，他们是完全可以互相转换的

可以发现，这个式子和我们把高斯混合模型带入到极大似然估计的式子是一致的

#### ELBO

对于一般的隐变量问题来说，我们最困难的地方就是这个$z$到底应该怎么取，比如上面的男女生身高问题，对于拿到的任意一个身高数据来说，我们并不知道如何分配这个身高的性别从而导致$\theta$的最大化陷入困难

对于上面的这条式子来说，同样的这个讨厌的$z$就是阻挡我们求解的罪魁祸首，不过我们注意到，对于求解的最终目标来说，这个$z$其实是不存在的，这与我们在高斯混合模型中说的实际是不关心分类的结果是一个道理，同时为了消去$z$，我们还为式子添加了一个$\sum_z$来累加$z$的所有可能

于是某一天，一个人想到了Jensen不等式，其实，对于任何一个存在着概率累加或者是积分的式子来说，我们都可以将其看作是一个求期望的过程，只不过和我们在[Jensen不等式](#Jensen不等式)中说过的一样，此时的求解期望的目标不再是简单的$E[x]$，而是一个带函数的期望$E[f(x)]=\sum_xp(x)f(x)$

因此，对于前一节中推导出的似然式子来说，我完全可以强行将其构造成一个期望的样子
$$
\begin{align*}
\arg\max_\theta\sum^n_{i=1}\log\sum_z p(x_i,z\mid\theta)
&=\arg\max_\theta\sum^n_{i=1}\log\sum_z q(z)\frac{p(x_i,z\mid\theta)}{q(z)}\\
&=\arg\max_\theta\sum^n_{i=1}\log\mathbb{E}_{q(z)}[\frac{p(x_i,z\mid\theta)}{q(z)}]
\end{align*}
$$
其中的$\frac{p(x_i,z\mid\theta)}{q(z)}$可以看作是一个有关于$z$的函数，而$q(z)$则是一个我们想象出来的关于$z$的分布，至少在这一步来说，我们只是为了构造出期望的形式而引入的$q$，我们此时也不知道这个$q$有什么用，不过由于这个函数我们并不能判断他的凹凸性，因此其实没办法利用到Jensen不等式，不过我们并不是没有办法，我们可以换一种思路，将它外层的$\log$函数看作是$f$，而把里面的部分统一看作为一个$x$，由于$\log$函数是凹函数，因此我们可以使用Jensen不等式进行不等变换
$$
\begin{align*}
\sum^n_{i=1}\log\mathbb{E}_{q(z)}[\frac{p(x_i,z\mid\theta)}{q(z)}]
&\ge \sum^n_{i=1}\mathbb{E}_{q(z)}[\log\frac{p(x_i,z\mid\theta)}{q(z)}]\\
\Rightarrow\sum^n_{i=1}f(\mathbb{E}_{q(z)}[g(z)])&\ge\sum^n_{i=1}\mathbb{E}_{q(z)}[f(g(z))]
\end{align*}
$$

> 注意到在进行不等式变化的时候，就不能再带着$\arg\max$进行推导了，因为归根到底$\arg\max$输出的是一个参数而不是最大值，这也是为什么我们在接下来的问题中急需将这个不等号改为等号的原因

此时我们如果把上面推导过的式子连在一起看，就会发现一条十分牛逼的不等式
$$
\sum^n_{i=1}\log p(x_i\mid\theta)\ge\sum^n_{i=1}\mathbb{E}_{q(z)}[\log\frac{p(x_i,z\mid\theta)}{q(z)}]
$$
这条不等式的第一个牛逼点在于他的不等号不是大于而是大于等于，第二个牛逼点在于我们能够通过Jensen不等式的定义去求解出不等号取等时的值，还有第三个牛逼点，就是我们只需要确定某个$q(z)$之后，这个不等式就变成恒等式了，意味着接下来我们可以用右边的式子完全替代左边的式子！

于是右边的这个式子就叫做左边式子的ELBO，全称是Evidence Lower BOund，也叫证据下界，举个通俗但不那么恰当的例子，我们都知道$x^2+y^2\ge2xy$，此时我们就可以把$2xy$看作是$x^2+y^2$的一个下界（ELBO也就可以看作是专指概率密度情况下的下界），而一旦存在某一个约束使$x=y$，那么不等式的左右就恒等了，这也是我们将要做的

![image-20230510144808731](https://s2.loli.net/2023/05/10/XJq9xQlitjLIFP5.png)

> 蓝色的是$x^2+y^2$，红色的是$2xy$，可以发现只有在$x=y$的时候不等号可以取等

#### E步

听起来很玄幻，但是我们完全可以进行推导，在[Jensen不等式](#Jensen不等式)一节中我们提到过，当变量$x$仅有唯一取值$c$的时候，Jensen不等式取等，引申到这个不等式中，由于我们将$\log$看作$f$，因此他的$x$就是$\frac{p(x_i,z\mid\theta)}{q(z)}$，想要让不等式取等，我们就要让这玩意仅有唯一取值$c$，即他的结果不受$z$的变化所影响

此外，我们还有一个隐含的条件一直没有使用，就是我们在构建期望的时候假设出来的这个$q$，为了让期望的构造成立，他必须是满足概率分布所满足的一切条件，即有$\sum_zq(z)=1$
$$
\begin{align*}
&\begin{cases}
\frac{p(x_i,z\mid\theta)}{q(z)}=c\\
\sum_zq(z)=1
\end{cases}\\
\Rightarrow&
1=\frac{1}{c}\sum_zp(x_i,z\mid\theta)\\
\Rightarrow&c=p(x_i\mid\theta)\\
\Rightarrow&q(z)=\frac{p(x_i,z\mid\theta)}{p(x_i\mid\theta)}=p(z\mid x_i,\theta)
\end{align*}
$$
于是我们便求出来了能使不等式恒成立的$q(z)$了，可以发现最终能够使等式成立的$z$分布正是在$x,\theta$控制下的后验分布，如果回想我们在[迭代过程](#EM迭代过程)中的情形，在已知某两个高斯模型的情况下，我们根据已知的模型参数和样本数据，对每一个样本点在两个模型下的抽样概率进行比较，取得概率较大的点作为本轮分类点$z$，这么一长串话所要表达的意思，正是上面求解出的$p(z\mid x_i,\theta)$这一个式子所要表达的意思

于是我们知道了，**将前一次的参数看作已知量，求解隐变量$z$在参数和样本的后验分布的过程就叫做E步**

#### M步

求得了$q(z)$长什么样，按照我们一开始的想法，接下来就可以一路连等下去了，事实上也的确如此，就让我们把上面所有的推导内容进行一个汇总，看看在EM的理论下最后的求解公式到底长什么样
$$
\begin{align*}
\hat\theta &= \arg\max_\theta\sum^n_{i=1}\log p(x_i\mid\theta)\\
&=\arg\max_\theta\sum^n_{i=1}\log\sum_z p(x_i,z\mid\theta)\\
&=\arg\max_\theta\sum^n_{i=1}\mathbb{E}_{q(z)}[\log\frac{p(x_i,z\mid\theta)}{q(z)}]\\
&=\arg\max_\theta\sum^n_{i=1}\sum_zq(z)\log\frac{p(x_i,z\mid\theta)}{q(z)}\\
&=\arg\max_\theta\sum^n_{i=1}\sum_zp(z\mid x_i,\theta')\log\frac{p(x_i,z\mid\theta)}{p(z\mid x_i,\theta')}
\end{align*}
$$
需要格外注意，这里$q(z)$的求解过程是先于求解这一串最大值的，对应到之前的迭代过程就是在给定模型下的数据点分类，而这个$q(z)$的目的是完成对于隐变量$z$的计算，由于我们是假设了在给定参数下$q(z)$的极端情况，因此对于整个求解式来说，一个先行的$z$是重要的，没有预先规定好隐变量的取值$z$，这个连等式连等都等不下去，因此在求解优化$\theta$的时候千万不能把$q(z)$里面包含的$\theta$当作参数参与优化了，这里我也格外使用了$\theta'$来标识

不过这在实际的计算过程中其实出错的概率也不高，因为我们会发现在[迭代过程](#EM迭代过程)中，我们理所当然的把其中的$\theta'$使用了上一次的参数直接带入进去计算出的$z$，而在实际求解模型参数的过程中这个$z$自然而然的就直接拿求解后的值参与运算了，求解$\theta$的过程也是十分自然的

同时我们也发现了，由于$\theta'$的存在，在单次优化的过程中是不保证$\theta$能够和常理中的极大似然估计一样一下就求得解析解，由于我们只对部分的$\theta$进行优化，因此$\hat\theta$也就不是最优值，得到了之后我们也能通过新的$\hat\theta$获得一个新的$q(z)$，逐次迭代，期望可以通过迭代的过程获得更优的$\theta$，最后，我们可以写出总的EM算法在第t步的迭代表达式
$$
\begin{align*}
\theta^{(t+1)}&=\arg\max_\theta\sum^n_{i=1}\sum_zp(z\mid x_i,\theta^{(t)})\log\frac{p(x_i,z\mid\theta)}{p(z\mid x_i,\theta^{(t)})}\\
&=\arg\max_\theta\sum^n_{i=1}\sum_z\left(p(z\mid x_i,\theta^{(t)})\log p(x_i,z\mid\theta)-p(z\mid x_i,\theta^{(t)})p(z\mid x_i,\theta^{(t)})\right)\\
&=\arg\max_\theta\sum^n_{i=1}\sum_zp(z\mid x_i,\theta^{(t)})\log p(x_i,z\mid\theta)
\end{align*}
$$

> 请别忘了要始终把$p(z\mid x_i,\theta^{(t)})$看成是很多个（因为$z$不止一个）和求解的$\theta$无关的常量，这样上面的等式的约去过程也就显然了，另外由于前面对于$z$的连加的存在千万不要顺手把作为系数的$p(z\mid x_i,\theta^{(t)})$也给约掉了，不同的常系数是会对极值造成影响的

**在计算出$q(z)$之后返回去对整个式子求解最大值参数的过程就叫做M步**

### EM收敛性证明

到处为止，我们证明了在[迭代过程](#EM迭代过程)中整个步骤在数学原理上的可行性以及方法产生的评价指标背景，如果对上面的一连串内容进行总结，我们可以说，**我们将一个极大似然估计的问题通过特例化其中的隐变量$z$的分布，巧妙的将其变为了一个最大化期望的问题**，但是方法带来的代价就是对于每轮计算的分步性和迭代性，我们期望在足够多的迭代之后这个过程可以收敛

但是我们会发现，在这个过程中，我们只证了方法的可行性，但是却没有证明方法的收敛性，如果用另一个迭代算法也就是最广泛使用的梯度下降为例，在一般情况下我们都会认为下降的过程是收敛的，但是在学习的过程中我们也会格外注意，一旦你的学习率设置的太大，或者本身求解的函数过于陡峭，看似美好的梯度下降算法同样也有发散的危险，而现在EM看起来连超参数都没有，如何证明这个过程是收敛的呢？

这就得从头讲起了，我们首先需要明确，如何确定一个函数他是收敛的，对于极大似然估计的问题来说，我们的评价指标也就是求解的最大值函数说到底是一个概率的乘积，即$\hat\theta=\arg\max\prod_xp(x\mid \theta)$，只不过在求解的过程中我们可以通过加$\log$的方式来将乘法问题变成加法问题的形式从而方便求解，观察这个式子，我们会发现由于概率范围在零到一之间的特性，只要我们的概率分布函数选的不出问题，这个连乘的结果最大也一定不会超过一的，这么一来我们就证明了这个函数的有界性

在有界性的前提下，若要证明一个函数收敛，下一步如果这个函数单调，那么我们就可以证明其收敛，你可能会说这不是显然的吗，比如我假设$Q(\theta^{(t+1)},\theta^{(t)})=\sum^n_{i=1}\sum_zp(z\mid x_i,\theta^{(t)})\log p(x_i,z\mid\theta^{(t+1)})$，那么显然的，因为我本身这个$\theta^{(t+1)}$就是出自$Q$的$\arg\max$的，显然有$Q(\theta^{(t+1)},\theta^{(t)})\ge Q(\theta,\theta^{(t)})\ge Q(\theta^{(t)},\theta^{(t)})$，好了得证了，这还不简单

> 此处只是为了避免混淆所以假装$Q$函数有两个参数，实际上对于每一次迭代来说第二个参数$\theta^{(t)}$始终是固定的，并不能看作是这个函数的一个变量，写在此处仅作为区分而已

看似很美好，但是你忽略了一件事，就是我们的所有推导过程都是建立在最初的极大似然估计的基础上的，但是最后我们求解的$\theta^{(t+1)}$的这个式子显然不是极大似然估计，并且我们为了让等式成立在推导过程中还引入了一个额外的后验概率，像上面这么简单的推导是不严谨的

我们还是得从最基本的$\log p(x\mid\theta)$出发，通过人为的引入我们加的那个后验概率对收敛性进行证明，于是，我们的目标就是证明出在上面的推导式子下，有$\log p(x\mid\theta^{(t+1)})\ge\log p(x\mid\theta^{(t)})$

首先，我们还是使用最基础的边缘概率的计算公式对初始式子进行一次变化
$$
\log p(x\mid\theta)=\log p(x,z\mid\theta)-\log p(z\mid x,\theta)
$$
由于我们引入了隐变量$z$，因此这条式子是显然的，接下来我们想想在正常的EM中，有了一个假定出来的参数之后下一步要干什么，没错，就是使用这个参数和样本数据对所有样本点的隐变量$z$计算后验概率，也就是进行分类，于是此时我们有了$p(z\mid x,\theta^{(t)})$，为了将这个先验信息也加到我们推导的过程中，我们将上面式子的左右两边分别对于$p(z\mid x,\theta^{(t)})$求期望，那么就变成了如下式子
$$
\begin{align*}
\log p(x\mid\theta)&=\log p(x,z\mid\theta)-\log p(z\mid x,\theta)\\
\Rightarrow\sum_zp(z\mid x,\theta^{(t)})\log p(x\mid\theta)&=\sum_zp(z\mid x,\theta^{(t)})\log p(x,z\mid\theta)-\sum_zp(z\mid x,\theta^{(t)})\log p(z\mid x,\theta)\\
\end{align*}
$$
对于式子的左半部分，由于$\log p(x\mid\theta)$与$z$无关，因此对其求期望就相当于对一个常数求期望比如$\mathbb{E}[1]=1$，因此左边部分仍旧是$\log p(x\mid\theta)$，而右边部分对于减法的第一项来说，我们发现就是前几段假设过的$Q$函数，也就是求解$\theta^{(t+1)}$的目标函数，我们也证明过$Q(\theta^{(t+1)},\theta^{(t)})\ge Q(\theta^{(t)},\theta^{(t)})$了，因此关键就到了最后一项身上，我们不妨令$H(\theta,\theta^{(t)})=-\sum_zp(z\mid x,\theta^{(t)})\log p(z\mid x,\theta)$，假如我们可以得证$H(\theta^{(t+1)},\theta^{(t)})\ge H(\theta^{(t)},\theta^{(t)})$，那么证明问题就迎刃而解了，即如下
$$
\log p(x\mid\theta^{(t+1)})=Q(\theta^{(t+1)},\theta^{(t)})+H(\theta^{(t+1)},\theta^{(t)})\ge Q(\theta^{(t)},\theta^{(t)})+H(\theta^{(t)},\theta^{(t)})=\log p(x\mid\theta^{(t)})\\
$$

### 懒人定理

在尝试推导这个不等式之前，请允许我再次插入一个证明过程之外的定理，它对于我们理解之后甚至之前的证明工作都有十分重要的作用，同时也当作无聊的证明过程中的一个调味剂，避免你对于枯燥的证明过程起反感

我们还是拿熟悉的骰子举例，假设我们现在有一枚可以投出负数的三面骰（你同样可以不用关心三面骰是怎么制成的），它可以骰出三个数字，分别是$-1,0,1$，骰出每一个数字的概率都是相等的，同样的，我们还有一个函数小道具$f$，它可以把骰出的骰子数值进行平方，即$f(x)=x^2$，那么请问这个骰子在经过了这个函数之后的分布是怎么样的

首先我们知道原有的分布是均匀分布，即$p(-1)=\frac{1}{3},p(0)=\frac{1}{3},p(1)=\frac{1}{3}$，这是显然的，我们假设经过了这个道具之后的分布为$q$，则其实非常容易求得它的分布，由于$-1,1$两种情况的平方都是$1$，因此有$q(1)=p(-1)+p(1)=\frac{1}{3}+\frac{1}{3}=\frac{2}{3},q(0)=p(0)=\frac{1}{3}$，这也是显然的，但是我们的目的其实并不只是求解分布那么简单，接下来我需要求解两个期望$\mathbb E_x[f(x)]$和$\mathbb E_f[f]$

诶，你说这两玩意怎么感觉这么像，怎么感觉已经有点晕了，那么你可以换一种角度，对于$\mathbb E_x[f(x)]$的直观理解在[Jensen不等式](Jensen不等式)一章中已经通过举例说明了，而对于$\mathbb E_f[f]$来说，你完全可以将其看作是另一个问题——假设我们现在有一枚可以抛出$0,1$的硬币，抛出这两个数字的概率分别是三分之一和三分之二，现在我们要计算抛足够多次之后硬币点数的期望

以下是针对这两个问题的计算结果
$$
\mathbb E_x[f(x)]=\sum_{i=1}^3p(x_i)f(x_i)=\frac{1}{3}*1+\frac{1}{3}*0+\frac{1}{3}*1=\frac{2}{3}\\
\mathbb E_f[f]=\sum_{i=1}^2q(f_i)f_i=\frac{1}{3}*0+\frac{2}{3}*1=\frac{2}{3}
$$
我们发现，这两玩意居然是完全相等的，从直觉角度，你可能觉得这两相等不是理所当然的，本来就应该是相等的才对，但是从背后得出这两个期望的过程来说，他们从原理上可以理解成两个看似完全不同的问题，但是他们能够得出一样的期望，意味着至少对于求解期望这个过程来说，他们的含义是一致的

总结一下，我们就得出了懒人定理，上面只是通过举一个特例的方式说明他们相等，对于严格证明部分，可以参考[这篇文章](https://zhuanlan.zhihu.com/p/462755872)，于是对于特定函数有关某分布模型的期望来说，恒有
$$
\mathbb E_x[f(x)]=\sum_xp(x)f(x)=\sum_fq(f)f=\mathbb E_f[f]
$$
所以我们知道这个定理有什么用呢，实际上，他为我们提供了一种简单的解释函数期望的途径，如果看过Jensen不等式一节都会发现，在其中我们举的那个$\mathbb E_x[f(x)]$的例子实际上并没有什么实际意义，我们笼统的用一个不明所以的道具来代指这个期望希望求解的内容，看完你好像知道了这个函数期望的那么一点意思，然而实际上如果换一个函数，换一个分布问题，你还是完全不知道这个期望函数究竟要干什么，至此，我们也终于可以对函数期望进行一个定义

**所谓$\mathbb E_x[f(x)]$，实际上就是在知道了$f$的映射关系$f(x)$的前提下，懒得求解$f$分布函数$q$的情况下求解有关$f$分布的期望$\mathbb E_f[f]$的公式而已**，更直白一点，其实就是我们知道了一个分布，又知道了有关这个分布的样本决定的另一个映射关系，现在我们要求解这组映射关系的分布的期望，我们就可以借助所谓的函数期望进行求解

还是不理解？没关系，在下一节中，我们就会用懒人定理重新理解一次曾经介绍过的信息熵函数

### 再谈信息熵

接下来我们又要跑去信息论里面借一下概念了，在此之前我们首先来复习一下信息量和信息熵的概念，我们在[信息量和熵](#信息量和熵)一节中曾经说过，信息量就是衡量某一个事件发生之后所包含的信息的公式，而信息熵则是衡量某一个模型不确定度的式子，将他们分别用$\mathcal{I},\mathcal{H}$来表示，则有

$$
\mathcal I(x)=-\log(p(x))\\
\mathcal H(x)= -\sum p(x) * \log(p(x))
$$

在前几节中，如果你有印象，我提到了，任何累加的公式其实在某一方面都可以看作是一个求解期望的过程，对于信息熵来说，我们同样发现了他的累加符号，并且我们还发现了，他的结构和上一节刚提到的函数期望$\sum p(x)f(x)$的结构简直一模一样，意味着这个信息熵公式，实际上背后蕴含着一个求解期望的行为

是的，这时候，不如拿我们上一节刚证明的懒人定理来试着解释一下这个期望具体的含义

我们发现，由于信息量$\mathcal I_p(x)=-\log(p(x))$，因此信息熵的公式可以直接写作$\mathcal H(x)= \sum p(x) * \mathcal I_p(x)=\mathbb E_\mathcal I[\mathcal I_p]$，因此，**信息熵其实代表的就是一个模型信息量分布的期望**，这也是为什么我们可以用信息熵来衡量一个模型所含有信息量的多少

### 交叉熵

但是接下来的内容可能就会比较晕了，下面我们来看另一条公式，我们称之为交叉熵，如果你有印象，我们在[逻辑回归](#逻辑回归)一章中曾经以他作为sigmoid模型的评价函数，在这个式子中，$q$可以看作是一个未知的分布
$$
\mathcal E(x)=-\sum q(x) * \log(p(x))
$$
你说这题我会，这不也就是有关信息量的期望吗，将他化简为$E_\mathcal I[\mathcal I_p]$不就完事了，但是化简完你发现有问题了，这玩意怎么和上面求出来的信息熵式子长一个样，同时在介绍懒人定理的时候可能你就发现了，既然最后的化简结果只和$f$有关，那前面的分布概率在这个期望中起到了什么作用呢

实际上稍微想一想就可以知道了，因为$f$并不是凭空出现的，$f$的分布究竟长什么样是受$x$的分布长什么样控制的，这里也是同理，虽然我们固定了信息量$f$的函数结构，但是$x$的分布改变了，因此虽然求的都是信息量的分布，但是两个式子的结果在$q\ne p$时一定是不同的

于是我们可以这么称呼交叉熵——**$p$模型的信息量在$q$模型上的分布的期望，对于一般的函数期望，我们也可以统一称之为函数$f$在$x$分布上的期望，**于是接下来，问题就在于这个结果到底代表着什么，实际上，我们发现交叉熵的含义蕴含着两个模型，同时他的输出至少是一个可以用信息量衡量的指标，而信息量又代表着一个事件的不确定度，因此我们可以称**交叉熵是使用$p$模型去确定$q$模型的不确定度的期望**，进一步说，我们可以用交叉熵去衡量两个模型之间的相似性，因为取极限时$q=p$，此时$p$模型可以完全确定$q$，也就是此时的不确定度应该最小，而这时我们会发现交叉熵坍缩为信息熵

因此，我们可以说，**信息熵是最小的交叉熵，交叉熵可以用来衡量模型的相似度，交叉熵越大，模型越不相似，并且由于上面的交叉熵是用$p$去确定$q$，因此无论是数学推导还是理性分析来说，交叉熵都是不满足交换律的**

### KL散度（相对熵）

理解了信息熵和交叉熵之后，我们终于可以看看所谓的相对熵了，他也叫KL散度，这玩意看上去挺唬人，其实就是只纸老虎，想要解释他其实一句话就说完了，这一节的主要工作还是把搁置了半天的收敛性证明给补充完了

所以相对熵究竟是什么呢，其实就是我们刚刚不是说交叉熵可以用来衡量两个模型之间的相似性嘛，但是其实细心的小伙伴也发现了，不同模型的交叉熵之间是不好比较的，由于信息熵和模型的复杂度有关并且始终大于零，因此当只拿到了一个交叉熵的时候，我们并没有办法一眼看出这个交叉熵背后两个模型之间到底有多相似，正因为还是需要信息熵这个最小量来直观的确定模型的相似度，相对熵就出现了

相对熵也就是KL散度说白了就是交叉熵减去信息熵，完整的公式和符号如下
$$
\begin{align*}
\mathcal D(q\mid\mid p)&=-\sum q(x) * \log(p(x))-\left(-\sum q(x) * \log(q(x))\right)\\
&=\sum q(x)\log\frac{q(x)}{p(x)}
\end{align*}
$$
从含义上来说，我们可以将其解释为，**用$p$去确定$q$相较于用$q$去确定自己增大了多少不确定度，即$p$和$q$的相似度**

同时，我们也说过了，信息熵是最小的交叉熵，因此这个相对熵一定是大于零的，知道了这个，回过头来看我们EM收敛性证明的最后一步，一切的问题都迎刃而解了，怕你忘了，我将推导的最后步骤重写一次
$$
\log p(x\mid\theta^{(t+1)})=Q(\theta^{(t+1)},\theta^{(t)})+H(\theta^{(t+1)},\theta^{(t)})\ge Q(\theta^{(t)},\theta^{(t)})+H(\theta^{(t)},\theta^{(t)})=\log p(x\mid\theta^{(t)})\\
$$
又已知$H(\theta,\theta^{(t)})=-\sum_zp(z\mid x,\theta^{(t)})\log p(z\mid x,\theta)$，如果我们将$H(\theta^{(t+1)},\theta^{(t)})-H(\theta^{(t)},\theta^{(t)})$，则有
$$
\begin{align*}
H(\theta^{(t+1)},\theta^{(t)})-H(\theta^{(t)},\theta^{(t)})&=-\sum_zp(z\mid x,\theta^{(t)})\log p(z\mid x,\theta^{(t+1)})-(-\sum_zp(z\mid x,\theta^{(t)})\log p(z\mid x,\theta^{(t)}))\\
&=-\sum_zp(z\mid x,\theta^{(t)})\log \frac{p(z\mid x,\theta^{(t)})}{p(z\mid x,\theta^{(t+1)})}\\
&=\sum_x q(x)\log\frac{q(x)}{p(x)}\ge0
\end{align*}
$$
看，有了KL散度大于零的定义之后，证明过程其实只需要显然一下就可得了

到此为止，我们完整证明了EM的收敛性，EM算法也从数学角度证明了可行性，EM算法确实是一个很强大的算法，也代表着一种具有跨时代意义的思想，但是他也并不是没有局限性的，最大的注意点就是EM的推导是基于极大似然估计的，因此对他的任何评价指标（也就是评价最后获得参数的好坏）都是由极大似然估计的定义所提供的，想要使用它就必须满足可使用极大似然估计的条件——样本独立，并且对于分布个数的选择也是需要仔细考量的超参数

总的来说，EM算法与其说是一种方法，倒不如说更像是一种思想，他证明了在一定条件下，参数之间的循环迭代的优化方式是能够严格收敛的，使得我们在面对存在隐变量的样本模型时不再束手无策，也为更多的延伸算法和工作提供了一种全新的思路
