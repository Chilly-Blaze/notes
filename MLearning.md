# Machine Learning

学深度学习学上瘾了，于是打算直接梭哈机器学习

> 虽然顺序有点不对，但是在看这个之前最好看过深度学习那个笔记不然可能看不懂

## 高斯分布

### 极大似然估计（MLE）

概率就是抛硬币之前我们知道加入我们抛出去一次有50%概率为正面，也就是我们知道一个模型的所有信息去预测可能的结果

而似然则是我们拿到了一万次抛硬币五千次正面的结果得到正面出现的可能是50%，也就是根据结果去重构模型的参数

他们的关系大概是**已知样本可以通过似然推导出模型参数，已知参数的模型可以通过概率预测出未知样本**

在机器学习中的高斯模型和极大似然估计就是在重复这个过程，我们会拿到我们也不知道从哪里随机抽取出来的一堆已知样本，然后我们假设这么多样本是从一个高斯模型当中抽取出来的，之后根据这些样本通过极大似然估计重构高斯模型，再用高斯模型去预测未知内容的概率

比如我现在收集了一万个人的身高信息，第$i$个人的身高记为$x_i$，即比如$x_1=180$表示第一个人身高180cm，此时我们根据这一堆$x$似然出一个模型，之后如果有需要模拟生成一万个人的身高，我们就可以用这个模型捏造一堆符合分布的样本

接下来的问题就是如何通过定量的样本求得模型的参数了，比起抛硬币，人的身高实际上是连续分布的，因此便有了概率密度函数$f$，需要注意的是概率密度函数的值并不是该点可能的概率（因为对于连续分布的每一点来说概率都是无限小），但是我们可以用这个值来表示该点和其他点相比被筛选出来的难易程度，因此我们有了极大似然估计求解的四步骤

1. 写出带未知参数的概率密度函数，对于高斯分布，他是 $f(x) = \frac{1}{\sqrt{2\pi }\sigma }e^{(-\frac{(x-\mu)^{2}}{2\sigma^{2}})}$

2. 把已知的所有样本$x$带入这个式子，表示每一点被筛选出来的比较概率（并不是真实概率）

3. 把这么多的带参数的$f$乘在一起加了$log$之后构造出似然函数（之后也可叫做损失函数）

   >至于为什么要相乘，我知道你很急但先别急，此处埋下一个伏笔，目前你只要知道似然函数就是把概率相乘取$log$即可

4. 对损失函数关于$\mu$和$\sigma^2$求偏导之后求解导数为零时的极大值$\hat{\mu},\hat{\sigma}^2$


之后我们可以得到通解$\mu_{MLE}=\bar x=\frac{1}{N}\sum^{N}_{i=1}x_i,\sigma^2_{MLE}=\frac{1}{N}\sum^{N}_{i=1}(x_i-\mu)^2$，不过用极大似然估计法推导出来的这个方差计算结果实际是偏小的（也叫有偏估计），真实的无偏估计方差应该是$\sigma^2=\frac{1}{N-1}\sum^{N}_{i=1}(x_i-\mu)^2$关于有偏和无偏估计是什么玩意此处先不提，只需要知道这个极大似然估计的方法并不是完美无瑕的就行

### 多维高斯分布

如果我们收集了不止一个身高信息，还有体重信息，此时求解的高斯分布也不再是一个维度了，就需要使用多维高斯分布模型，他的公式如下
$$
\mathcal{N}(\mu, \Sigma)=\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)}
$$
其中$x,\mu$都是向量，$\Sigma$是一个协方差矩阵（也就是半正定矩阵），$D$是$x$的维度，指数部分$\Delta=(x-\mu)^{T} \Sigma^{-1}(x-\mu)$我们可以将其看作是一组同心椭圆或者是等高线，椭圆上的每一个点带进去会有相同的结果，也可以把他看作是一个距离表达式，只不过此时的距离相等量不再是一个圆到圆心的距离而是椭圆，这被称为马氏距离，当$\Sigma$为单位矩阵的时候$\Delta=(x-\mu)^2$也就是我们熟知的欧氏距离，$\Sigma$控制了同心椭圆的原点位置和旋转角度

### 高斯分布概率定理

大部分高斯分布的推论都是以下这个式子推出来的，他表示所有的高斯分布经过了线性变换之后一定还是一个高斯分布
$$
x \sim \mathcal{N}(\mu, \Sigma), y \sim A x+b \Rightarrow y \sim \mathcal{N}\left(A \mu+b, A \Sigma A^{T}\right)
$$
之后我们如果令
$$
x=\begin{pmatrix}
x_a \\
x_b
\end{pmatrix},\mu=\begin{pmatrix}
\mu_a \\
\mu_b
\end{pmatrix}, \Sigma=\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}
$$
其中$x_a,x_b$是对向量$x$进行分块，原本$x,\mu$长度为$p$则$a+b=p$，则我们可以将$x$看作是$x_a$和$x_b$的联合分布概率，现在我们的需求是根据联合分布概率去求解边缘概率分布和条件概率分布，在此之前我们引入一条式子

首先我们需要搞清楚，我们求解的是分布情况，而我们又知道对于高斯分布的线性变换一定是高斯分布，因此我们的最终目标一定是求解边缘概率分布和条件概率分布的$\mu$和$\Sigma$两个值即可

1. 边缘概率分布$p(x_a)$其实非常好求，因为我们可以将$x_a$写作$x_a=\begin{pmatrix}\mathbb{I}&\mathbb{O}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}=Ax+b$，接着照着上面的式子，容易得到$x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$

2. 条件概率分布$p(x_b|x_a)$则需要花一点功夫，我们需要先引入一条式子
   $$
   x'=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a
   $$
   需要注意这条式子可以说没有任何意义，他只是为了辅助得到条件概率的工具罢了，同时之后如果要求解$p(x_a|x_b)$则需要把这几条式子的$a,b$都进行对换，然后我们可以得到
   $$
   x'=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbb{I}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}=Ax+b
   \\ \mu'=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\\Sigma'=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
   $$
   然后我们如果把第一条式子变一变，即可得到$x_b$的求解式
   $$
   x_b=\mathbb{I}\times x'+\Sigma_{ba}\Sigma_{aa}^{-1}x_a=Ax+b\\
   \mathbb{E}[x_b]=\mu'+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
   \mathbb{D}[x_b]=\Sigma'\\
   x_b\sim\mathcal{N}(\mathbb{E}[x_b],\mathbb{D}[x_b])
   $$
   由于这个$x_b$的$\mu$和$\Sigma$是由$x_a$控制的，因此其实这个$p(x_b)$正是等于$p(x_b|x_a)$，于是我们得到了
   $$
   x_b|x_a\sim\mathcal{N}(\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a+\Sigma_{ba}\Sigma_{aa}^{-1}x_a,\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab})
   $$

知道了如何用联合概率求解边缘概率和条件概率，我们不妨反过来再看看如何用条件概率和边缘概率求解联合概率

现在我们已知$x\sim\mathcal{N}(\mu,\Lambda^{-1}),y|x\sim\mathcal{N}(Ax+b,L^{-1}),y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})$，需要求解$p(y),p(\begin{pmatrix}x\\y\end{pmatrix})$

1. $p(y)$根据公式可以直接算出$y\sim \mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)$，计算过程中注意$\epsilon$和$x$相互独立因此均值和方差加法计算均可以分解后独立计算

2. 根据上面结论其实易得
   $$
   \mathbb{E}[\begin{pmatrix}x\\y\end{pmatrix}]=\begin{pmatrix}\mu\\A\mu+b\end{pmatrix}\\
   \mathbb{D}[\begin{pmatrix}x\\y\end{pmatrix}]=\begin{pmatrix}\Lambda^{-1} & Cov(x,y)\\Cov(y,x) &A\mu+b\end{pmatrix}
   $$
   由于$Cov(x,y)=Cov(y,x)$因此我们只需要求一个即可，根据协方差公式$Cov(x,y)=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])]$易得$Cov(x,y)=\Lambda^{-1}A^T=A\Lambda^{-1}$带入即可得$\begin{pmatrix}x\\y\end{pmatrix}\sim(\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1} & \Lambda^{-1}A^T\\A\Lambda^{-1} &A\mu+b\end{pmatrix})$

之后如果想求$p(x|y)$则可以根据上面联合概率求条件概率的方式求解，此处不提

## 线性回归

这一节的大部分内容在深度学习中都有提到过因此此处更注重于数学角度的分析

### 最小二乘法（LSE）

最小指的就是最小化损失函数，二乘指的就是均方误差，只是把这个过程换了一个名字罢了，以下是这个过程中的一些注意点

- $L(W)=\sum_{i=1}^{N}\left\|W^{T} x_{i}-y_{i}\right\|^{2}$，其中我们可以在$x_i$中增加一个$x_{i0}=1,w_0=b$实现线性关系中的$b$偏置
- 让$X=(x_1,x_2\dots x_N)^T$，则$L(W)=(W^TX^T-Y^T)(XW-T)$，$W$和$Y$都比$X$少一维，只有神经元的数量能控制$W,Y$第二维的大小
- $\frac{\partial L(W)}{\partial W}=2 X^{T} X W-2 X^{T} Y=0$可解得$W$的解析解$\hat{W}=(X^TX)^{-1}X^TY$，其中$(X^TX)^{-1}X^T$是$X$的左逆（或者是伪逆），这是用求导方式最小化损失函数
- 从另一种角度看$L(W)$还可以看作是在高维空间中找出一个可以垂直于指定向量$y$的超平面，即最小化向量$y$在$X$向量组构成超平面中的投影，从这个角度来说，我们应该让$x$的数量$N$大于等于维度$p$，保证在高维中可以形成平面（类比于我们无法拿一条二维向量在三维空间构成一个平面，至少得有两条线才能形成平面）
- 我们假设现在有一群数据点我们知道他满足$y=f(W)+\epsilon,\epsilon\sim \mathcal{N}(0,\sigma^2)$这个模型，它让产生的数据在高斯噪声的作用下落在实际直线两侧，由于我们知道了模型公式和样本，便可以使用最大似然估计求解参数$W$，我们会发现求解的结果关于$W$需要最小化的对象与最小二乘法的损失函数完全一致，这也能说明**最小二乘法隐含了样本噪声服从高斯分布的假设**

### 正则化

我们之前说根据求导结果$\hat{W}$具有解析解$(X^TX)^{-1}X^TY$但是一方面$X^TX$并不一定可逆，另一方面当样本数量少于数据特征的情况下容易出现过拟合，一种解决方式就是正则化

正则化的统一框架为$L'(W)=L(W)+\lambda P(W)$，当$P(W)=\left|  \right | W\left |  \right | ^2$的时候就是著名的Weight Decay（也叫L2正则化，Ridge正则化，岭回归，权重衰减等），此时如果再次计算解析解会发现结果变成了$\hat{W}=(X^TX+\lambda I)^{-1}X^TY$，由于给$X^TX$加了正则项保证其可逆性，同时从损失函数的角度也降低了过拟合风险

### 最大后验估计（MAP）

假设现在同样是求解抛硬币的概率模型，同样是抛了十次，只不过这回我们抛出了十次正面零次反面，如果这回我们什么也不管拿极大似然估计做，不出意外我们会得出正面概率百分百反面概率零的极端模型参数，但是我们常识知道，只能抛出正面的硬币似乎并不是那么常见，我们更希望这种极端模型的结果能够出现的少一些，这种样本过少造成的偶然事件导致的模型偏差是不是看上去很像是过拟合会造成的结果？在上一节当中我们讨论了传统意义上的正则化缓解模型过拟合的方法，而现在我们不妨从另一个角度——先验概率的角度重新审视一下正则化的数学推导

首先我们要把最大后验估计的计算公式列出来，他长这样
$$
\hat{\theta} = \arg\underset{\theta}{\max} P(\theta\mid X)
$$
其中$X$就是我们抛了十次的结果，$\theta$就是将要求解的正面的概率，整个式子用大白话说就是当我们抛出十次正面零次反面的情况下求解可以让出现这个情况的概率最大化的模型参数，这么一听怎么和极大似然估计没什么区别呢，确实，在介绍极大似然估计的时候为了方便理解我也说的是在已知样本的情况下求解参数，然而当时我也埋下了一个伏笔，相信单看似然函数你可能就十分奇怪为什么把样本代到模型里乘一乘就可以求解模型参数了，完全搞不懂是什么原理，实际上我换个问法你可能就明白了

已知当前抛硬币正面朝上的概率是60%，求解抛三次出现正正反情况的概率

理所当然的，因为每次抛硬币的操作都是独立的，我们只需要简单的$0.6*0.6*0.4$就可以计算出正正反的概率为$0.144$，你发现了，在这个问题中使用乘法显得那么理所当然，于是我们知道了，似然函数实际上根本就不是$P(\theta\mid X)$的函数式，而是$P(X\mid \theta)$（已知模型参数求解某一序列样本发生的概率）的函数式

这么说来难道说我们学了半天的极大似然估计本身就是错的吗，这可不一定，我们不妨来寻找一下$P(\theta\mid X)$和$P(X\mid \theta)$有什么关系，这时候就需要掏出伟大的贝叶斯公式了
$$
P(\theta\mid X)=\frac{P(X\mid \theta)P(\theta)}{P(X)}
$$
对于这个式子，在我们拿到了某次样本之后实际上$P(X)$由于只着眼于这一次的样本因此其值恒为常数1可略去，于是我们知道了，实际上$P(\theta\mid X)$和$P(X\mid \theta)$的差距仅仅在$P(\theta)$身上，而这个$P(\theta)$实际上就是大名鼎鼎的先验概率，当其为常函数时，在求解$\theta$最大值的时候$P(\theta\mid X)$和$P(X\mid\theta)$相等，最大后验估计也就变成极大似然估计了，对应到实际问题中就是每一种概率情况能取到的概率都是一样的，引申到抛硬币问题就是正面朝上的最终概率是50%和100%的情况都是等可能的，不存在一个人的理性思维觉得硬币正反面概率分别是50%的可能大些的情况（也就是没人事先看过硬币有一个大致的判断）

整合了贝叶斯公式之后，我们需要求解的模型参数也变成了这样
$$
\hat{\theta} = \underset{\theta}{\operatorname{argmax}} P(X\mid\theta) P(\theta)
$$
但是如果我们事先看了一眼硬币，发现这个硬币好像挺均匀的，发生投出去十次全是正面的概率直觉感觉就并不高，此时我们就可以给$P(\theta)$求解参数也定一个模型比如我们认为他也服从高斯模型$P(\theta)=\mathcal{N}(0.5,\sigma^2)$，结合高斯分布的图像，表示我认为正面出现概率为$0.5$的概率最高，其余概率随和$0.5$的距离增大而下降

为了计算方便，我还是令$P(\theta)=\mathcal{N}(0,\sigma^2)$，同时为了让结果直观一点，我再令上面的$\theta=W,X=y$，然后我们计算一下最大后验概率的化简结果
$$
\begin{aligned}
\hat{W} & =\underset{W}{\operatorname{argmax}} \log (P(y \mid W) P(W)) \\
& =\underset{W}{\operatorname{argmax}} \log \left(\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(y-W^{T} X\right)^{2}}{2 \sigma^{2}}} \frac{1}{\sqrt{2 \pi} \sigma_{0}} e^{-\frac{\|W\|^{2}}{2 \sigma_{0}^{2}}} \right)\\
& =\underset{W}{\operatorname{argmax}}\left(-\frac{\left(y-W^{T} X\right)^{2}}{2 \sigma^{2}}-\frac{\|W\|^{2}}{2 \sigma_{0}^{2}}\right) \\
& =\underset{W}{\operatorname{argmin}}\left( \frac{\left(y-W^{T} X\right)^{2}}{2 \sigma^{2}}+\frac{\|W\|^{2}}{2 \sigma_{0}^{2}}\right) \\
& =\underset{W}{\operatorname{argmin}}\left( \sum_{i=1}^{N}\left(y_{i}-W^{T} x_{i}\right)^{2}+\frac{\sigma^{2}}{\sigma_{0}^{2}}\|W\|^{2}\right)
\end{aligned}
$$
你没看错，化简结果就是Weight Decay，我们加上的先验概率反映到结果上竟就是给模型加上了一个正则化的条件

综上所述，我们总结一下，**LSE是模型噪声服从高斯分布的MLE，MLE是先验概率服从等概率分布的MAP，Weight Decay正则化方法则是先验概率服从和样本同方差高斯分布的MAP**

## 线性分类

在线性回归问题中，我们画了一条线用于拟合一堆样本点从而达到预测未知样本的目标，线性分类则是要画一条线去分割两堆不同的样本点，因为都是画线，因此笼统来说线性分类可以看作是线性回归的一个变形，只不过如果纯拿线性回归的方法来处理线性分类的任务效果并不佳，因此线性分类问题需要解决的就是如何画出这一条线的问题

于是我们需要回忆一下在线性回归问题中我们是怎么画出拟合线的，当时我们假设数据点将会围绕拟合直线模型呈高斯分布，之后推出了使用最小二乘法构造损失函数，求解损失函数的最小值获得了拟合线方程（即$w$参数），如今换到了线性分类问题，我们是否也可以通过某种假设构造出一个损失函数求解分类线方程参数呢
$$
\text{线性回归} \xrightarrow{激活函数} \text{线性分类}
$$
其实从道理上来说，从高斯分布入手的线性分类器的最终形态一定是逻辑回归，然而逻辑回归可不是从天上掉下来的，如果现在直接把一个sigmoid函数丢给你相信你一定懵的一批，要知道sigmoid为什么能够起作用，我们就得跟随着前人的脚步，一步步学习逻辑回归的演化史

首先我们得先定义几个分类问题中用得上的参数，通过几个参数也能够更直观的理解分类问题的解决内容

有$N$个在$p$维空间内的点集合$X=\{x_1,x_2\dots x_n\}$，其中$x_i$表示第$i$个数据点，是一个$p$长度的列向量，与每一个$x_i$匹配的是$N$个$y_i$集合$Y$，由于此处我们只考虑二分类问题，因此$y_i$不仅是一个标量而且仅能取到$0$或$1$，即$y_i \in \{0,1\}$，$(x_i,y_i)$表示第$i$个$x$和$y$的对应关系，同时由于我们定死了用一条线来分类的要求，因此只需要求解一个参数$w$，令某函数模型$f(w^TX)=Y$即可实现分类目标，其中$w$维度为$p\times 1$，也是一个向量，为了找到这个$w$，还需要有一个评价函数或者说是损失函数$L(f)$，通过最小化损失函数来找到最优参数$w$

一下子看到那么乱七八糟的参数一定十分摸不着头脑，接下来不妨用一个最傻瓜的解决分类问题的思路来看看这些参数在实际问题中大概能怎么用

### 感知机算法（PLA）

虽然我们之前一直说画线什么的，但是那只是在二维平面上便于理解的说辞罢了，实际上由于样本点的特征是多维的，因此无论是线性回归还是分类，所得到的所谓直线其实是高维空间下的一个超平面，想要用一个超平面分割两组数据点，其实就是让平面上方的点赋值$1$，下方的赋值$0$，用数学语言就是$f(x)=\mathrm{sign}(w^Tx)$，其中$\mathrm{sign}$叫符号函数同时也是激活函数，当$w^Tx>0$的时候输出$1$，不然输出$0$，我们假设现在已经找到了这么一条完美的平面$w^Tx$，这时候无论遇到什么未知样本点$x_i$，我们只需要丢给$f$，他的输出就可以直接作为我们的$y_i$，通过这个函数我们也能够理解为什么给线性回归问题套一个激活函数就可以实现分类的依据了，这个分类函数是完美的，但是不完美的地方就在于这个平面我们求不出来，由于符号函数是分段函数，意味着我们无法通过无脑带入连乘的似然函数之后求导求解解析最优解$w$，这时候我们就需要另外找一个损失函数，我们构造这样的函数，通过最小化函数结果寻找最佳值$w$
$$
L(w)=\sum_{(x_i,y_i)}-(y_i-0.5)w^Tx_i
$$
含义其实也很简单，由于需要秉承着符号函数的意志，因此这个损失函数做的就是想尽可能让$y_i-0.5$和$w^Tx_i$同号，这样从另一个角度同样也满足了类似符号函数的定义，同时该函数可导使得我们可以通过梯度下降的方法接近最小值点，至于为什么不通过导数为0求解最小值，你试试这玩意求导完是一个不含$w$的东西，说明在一组样本下他是没有最小值的，我们想一下也知道只要我$w$​向梯度方向增的越多，当然损失函数也会越大，因此这时候就需要我们多找几组样本进行重复实验

显然，这种所谓的感知机算法是有局限性的，不过他的局限性不在于激活函数$f$构造的不好，事实上这个$f$正是绝对理想情况下的理想结果，他的问题在于损失函数$L$构造的过于刻意了以至于效果也肉眼可见的不佳，但是他提供的梯度下降方法为包括深度学习在内的复杂模型不易求参数解析解的情况下逼近极值提供了解决思路，而其理想情况下的符号函数作为区分函数则带来了另一个重量级的非线性分类器——SVM（我们会留作第五章专门讲解）

### 矩阵求导

上一节说是说让你试试求导，但是相信大多数人对于矩阵的求导还是一脸懵逼的，于是我们得花一点时间来说一说矩阵求导法则

首先我们得知道什么矩阵可以被求导，我们知道一个求导首先需要一个函数和自变量，大概写成这个样子$\frac{\partial f}{\partial x}$

$x$是一个矩阵相信应该没什么问题，主要就是$f$会长什么样，同样是传入一个矩阵，常见的$f$可以分为两个大种类，分别是全元素标量函数和逐元素标量函数，其中全元素标量函数传入一个矩阵，函数会提取出其中的所有元素进行计算得到一个标量值，比如$f(X)=a^TXb$，逐元素标量函数同样是传入一个矩阵，函数会对其中每一个元素进行同样的操作得到一个矩阵，比如$f(X)=\sin(X)=[\sin (X_{ij})]$

接下来就可以看一些直接可用的求导公式了，我们令$x_i$为标量，$x$为列向量，$X$为矩阵，$\mathrm {tr}  (X)$表示计算$X$的迹（矩阵对角元素相加和），$f$为全元素标量函数，$\sigma$表示逐元素标量函数，$\mathrm dX$表示$X$的全微分，$\odot$表示逐元素相乘，和$X$形状一致，基于这几个元素，有以下几个恒成立公式
$$
\begin{align}
x_i &= \mathrm {tr}(x_i)\tag1\\
\mathrm {tr}(x_1A+x_2B) &= x_1\mathrm {tr}(A)+x_2\mathrm {tr}(B)\tag2\\
\mathrm {tr}(X)&=\mathrm {tr}(X^T)\tag3\\
\mathrm {tr}(ABC)&=\mathrm {tr}(CAB)=\mathrm {tr}(BCA)\tag4\\
\mathrm {d}(AB)&=\mathrm {d}AB+A\mathrm {d}B\tag5\\
\mathrm {d}(X^T)&=(\mathrm {d}X)^T\tag6\\
\mathrm {d}f(X)&=\mathrm {tr}(\frac{\partial f(X)}{\partial X^T}\mathrm {d}X)\tag7\\
\frac{\partial f(X)}{\partial X^T}&=(\frac{\partial f(X)}{\partial X})^T\tag8\\
\mathrm {d}(AXB)&=A\mathrm {d}XB\tag9\\
\mathrm {d}|X|&=|X|\mathrm {tr}(X^{-1}\mathrm {d}X)\tag{10}\\
\mathrm {d}X^{-1}&=-X^{-1}\mathrm {d}XX^{-1}\tag{11}\\
\mathrm {d}(A\odot B)&=\mathrm {d}A\odot B+A\odot \mathrm {d}B\tag{12}\\
\mathrm {d}\sigma(X)&=\sigma'(X)\odot \mathrm {d}X\tag{13}\\
A^T(B\odot C)&=(A\odot B)^TC\tag{14}
\end{align}
$$

根据这些公式，我们不妨来求一求$f=a^TXX^Tb$关于$X$的导数

首先确认$f$是一个全元素标量函数，因为如果$X$的形状为$m\times m$则$a,b$形状一定是$m\times 1$的，最后结果是$1\times 1$的矩阵，即为标量，然后确认我们需要求解的结果是$\frac{\partial f(X)}{\partial X}$也是一个标量，之后开始求解
$$
\begin{align*}
(1)&\Rightarrow& \mathrm{d} f(X)&=\mathrm {tr}(\mathrm {d}f(X))=\mathrm {tr}(\mathrm {d}(a^TXX^Tb))\\
(9)&\Rightarrow&&=\mathrm {tr}(a^T\mathrm {d}(XX^T)b)\\
(5)&\Rightarrow&&=\mathrm {tr}(a^T(\mathrm {d}XX^T+X\mathrm {d}X^T)b)\\
(2)&\Rightarrow&&=\mathrm {tr}(a^T\mathrm {d}(X)X^Tb)+\mathrm {tr}(a^TX\mathrm {d}(X^T)b)\\
(6)&\Rightarrow&&=\mathrm {tr}(a^T\mathrm {d}(X)X^Tb)+\mathrm {tr}(a^TX(\mathrm {d}X)^Tb)\\
(4)&\Rightarrow&&=\mathrm {tr}(X^Tba^T\mathrm {d}X)+\mathrm {tr}((\mathrm {d}X)^Tba^TX)\\
(3)&\Rightarrow&&=\mathrm {tr}(X^Tba^T\mathrm {d}X)+\mathrm {tr}(X^Tab^T\mathrm {d}X)\\
(2)&\Rightarrow&&=\mathrm {tr}((X^Tba^T+X^Tab^T)\mathrm {d}X)\\
(7)&\Rightarrow& \frac{\partial f(X)}{\partial X^T}&=X^Tba^T+X^Tab^T\\
(8)&\Rightarrow&\frac{\partial f(X)}{\partial X}&=(\frac{\partial f(X)}{\partial X^T})^T=ab^TX+ba^TX
\end{align*}
$$
我们发现了，利用迹求解矩阵导数的要点就是通过取标量函数全微分的迹，通过分离$\mathrm dX$，构造出形如$(7)$一致的全微分和导数转换的形式，便可容易求得矩阵的导数结果

### 线性判别分析（LDA）

既然直接用符号函数区分样本无法求解超平面，研究者们开始了新一轮的尝试，这次他们决定直接跳过构造激活函数，用一种名为降维的方法，直接从损失函数的角度下手解决分类问题

这种方法的思想也十分简单，在上一节中我们是要找一个超平面（在样本点为二维时则为直线）分割两组数据点，这次我们不再拘泥于这个平面，而是实实在在的构造一条新的过原点的直线（注意这回无论是高维还是低维都是直线），然后我们让所有的数据点全部投影到这条直线上，接着我们只关注两组数据点在这条直线上的投影点，然后改变直线的参数$w$即斜率，看看哪条直线对应的投影点能够将两组数据点分得更开，我们就使用这条直线

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201121151154839.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0NzY2ODgz,size_16,color_FFFFFF,t_70#pic_center)

> 空间内相同的两组红蓝数据点，明显第二条的投影点更能够区分数据，此处只画了二维情况，高维同理

这条过原点的直线当然不是为了分割数据点，但是这种另辟蹊径的方法居然看上去还挺有效果（至少在这个二维图上），而想要求解这条投影直线，我们只需要解决两个问题，一是如何找到投影点，二是如何评价投影点的分布

对于投影点实际上非常简单，我们用幼儿园就学过的向量点积即可解决，我们令所求投影直线方向上的单位向量为$w$，接着取每一个样本点向量$x_i$与其做点乘$w^Tx_i$，向量的点乘结果即为样本点向量在这条直线上的投影距离，假如我们取这条投影直线为坐标轴，则投影距离即可转换为投影坐标，$w^TX$将会产生一条向量，其中每一维度的值就是一个样本点在直线上的投影，于是我们可以构造出一个映射$(w^Tx_i,y_i)$，表示这条向量的每一个值对应的类别$y_i$

![img](https://s2.loli.net/2023/02/15/5BZFm2QlXT1Dn4b.png)

如何评价这条直线的好坏呢，我们挑出映射当中所有$y_i=0$的点$w^Tx_i$（例如上图的红点）组成一个向量，记为$u_0$，同时记其中有$N_0$个元素，同理挑出另一个向量（蓝色的点）组成$u_1$，其中有$N_1$个元素，接着我们构造损失函数如下
$$
L(w)=\frac{(\bar{u}_0-\bar{u}_1)^2}{\Sigma_{u_0}+\Sigma_{u_1}}\\
\bar{u}_0=\frac{1}{N_0}\sum^{N_0}_{i=1}u_0,\bar{u}_1=\frac{1}{N_1}\sum^{N_1}_{i=1}u_1\\
\Sigma_{u_0}=\frac{1}{N_0}\sum^{N_0}_{i=1}(u_{0_i}-\bar{u}_0)(u_{0_i}-\bar{u}_0)^T\\
\Sigma_{u_1}=\frac{1}{N_1}\sum^{N_1}_{i=1}(u_{1_i}-\bar{u}_1)(u_{1_i}-\bar{u}_1)^T
$$
这个损失函数的意义就是让同类点之间的方差$\Sigma$尽量小，令异类点之间的均值距离$|\bar{u}_0-\bar{u}_1|$尽量大，这也是符合我们直觉的，更满意的是这个函数是可导的，意味着我们可以通过求导的方式获得这条投影直线的最优解，将参数和$u$都用$w^Tx$代换，可以化简出以下式子
$$
L(w)=\frac{w^T(\bar{x}_0-\bar{x}_1)(\bar{x}_0-\bar{x}_1)^Tw}{w^T(\Sigma_{x_0}+\Sigma_{x_1})w}
$$
其中$\bar{x}_0$指$X$里的所有输出值$y_i=0$的样本点（向量）的均值，$\Sigma_{x_0}$则为方差，$\bar{x}_1,\Sigma_{x_1}$同理，这个式子分子分母计算结果都是标量，因此可以写成分数形式，但是千万不要把上下的$w^T,w$约掉了，因为其虽然写成这样但是其本质还是分子矩阵乘分母矩阵的逆

由于分子和分母的中间项和$w$都没什么关系，我们令$(\bar{x}_0-\bar{x}_1)(\bar{x}_0-\bar{x}_1)^T=\Sigma_b,(\Sigma_{x_0}+\Sigma_{x_1})=\Sigma_w$，然后我们可以对其进行求导，尝试求解他的解析解，同时我们应该注意我们希望求解$w$的方向而非大小，因此对于其中的常系数部分我们可以用$C$代替
$$
\begin{align*}
\frac{\partial L(w)}{\partial w}=&(w^T\Sigma_bw)(w^T\Sigma_ww)^{-1}=0\\
&2\Sigma_bw\cdot(w^T\Sigma_ww)^{-1}+(w^T\Sigma_bw)\cdot(-1)(w^T\Sigma_ww)^{-2}\cdot2\Sigma_ww=0\\
&\Sigma_bw-(w^T\Sigma_bw)\cdot(w^T\Sigma_ww)^{-1}\cdot\Sigma_ww=0\\
&(\bar{x}_0-\bar{x}_1)(\bar{x}_0-\bar{x}_1)^Tw-C(\Sigma_{x_0}+\Sigma_{x_1})w=0\\
&(\bar{x}_0-\bar{x}_1)C=C(\Sigma_{x_0}+\Sigma_{x_1})w\\
&w=C(\Sigma_{x_0}+\Sigma_{x_1})^{-1}(\bar{x}_0-\bar{x}_1)
\end{align*}
$$

其中我们遇到了对于$w^T\Sigma_bw$的求导，我们令$g=w^T\Sigma_bw$，可得$g$是一个全元素标量函数，则他的详细求导过程如下
$$
\begin{align*}
\mathrm{d}g&=\mathrm{tr}(\mathrm{d}(w^T\Sigma_bw))\\
&=\mathrm{tr}(\mathrm{d}(w^T)\Sigma_bw)+\mathrm{tr}(w^T\Sigma_b\mathrm{d}w)\\
&=\mathrm{tr}(w^T(\Sigma_b)^T\mathrm{d}w)+\mathrm{tr}(w^T\Sigma_b\mathrm{d}w)\\
\Rightarrow \frac{\partial g}{\partial w}&=(\Sigma_b+(\Sigma_b)^T)w^T
\end{align*}
$$
又因为$\Sigma_b$是协方差矩阵，对角对称因此$\Sigma_b=(\Sigma_b)^T$，$\frac{\partial g}{\partial w}=2\Sigma_bw^T$得到推导的结果，于是，我们得到了只需要让$w$和$(\Sigma_{x_0}+\Sigma_{x_1})^{-1}(\bar{x}_0-\bar{x}_1)$同方向，就可以得到我们理想当中的投影向量

传统意义上的LDA到这就结束了，我们确实找到了一条向量$w$可以满足理论上的降维和分类要求，但是感觉好像缺了什么东西，没错，LDA只提供了求解损失函数的思路，但是没告诉我们分类的界限在哪里，也就是激活函数是什么，我们的确拿到了一个向量和在这个向量上的一堆投影点，但是此时如果再来一个点我们并没有一个判断标准去衡量这个点的输出分类结果

不过这并不是什么难事，我们不妨把投影向量和投影点看作是另一个样本模型，这回我们从需要在高维寻找一个超平面变成了在一条直线上寻找分离两组数据的分界点，这也是降维操作的核心

综上所述，**LDA就是让若干组高维的样本点能够在最有区分度的前提下投射到一条直线上的过程**

### 高斯判别分析（GDA）

那么如何找到这个分界点呢，聪明的人可能会有一个想法，我是否可以把一条直线上的两组数据看作是由两个一维高斯生成的点集呢，接着遇到一个新的点，我们只需要把这个点带入两个解出的高斯模型，看看它更可能由哪个高斯模型生成我们便判断他的类别

![img](https://s2.loli.net/2023/02/16/5IwNtoxkKpChqHQ.png)

例如上图我们根据橙蓝两组样本求解出了两个高斯分布模型（蓝线和橙线），取其交点的$x$坐标即为分界点，之后再来一个样本点，若落在绿线的左边，我们则认为他是蓝色这一组，落在绿线右边则认为是橙色一组

不过我们发现单求两个高斯模型似乎蓝色的模型好像明显肥一点，在之后的预测任务中也理所当然会占优一点，而控制胖瘦的主要原因还是高斯模型中的$\Sigma$方差一项，为了保证两个求解模型的公平，我们会令两个模型的方差相等

那么接下来的问题就在于怎么求解这两个模型了，同时由于两个模型具有相等的方差项，因此无法单独进行求解，我们不得不老老实实用MAP或者MLE求解，同时既然我们也学过了多维高斯分布了，那又为什么非要降维了以后求一维的高斯呢，我们不妨把原有的样本点集合$X$就看作是两个高维高斯模型抽取出来点集合，直接求解这两个高维高斯模型不就好了

这种思想就是GDA，他假设空间内的每一组样本点均服从同方差的高斯分布，求解出高斯模型之后根据未知点在哪个高斯模型被筛选出来的概率高来判断未知点的分类结果

明显的，这个问题可以通过MLE求解，在正统GDA方法中其实是用MAP做的，他假设分类结果$y$的先验概率符合伯努利分布，不过由于伯努利分布的参数和两个高斯模型都没什么关系，因此此处直接假设先验符合等概率分布，另MAP退化成MLE求解参数解析解

我们不妨复习一下MLE的公式，对于$x$独立分布的情况下，他往往长这样$\hat{\theta} = \arg\underset{\theta}{\max} P(x\mid \theta)=\arg\underset{\theta}{\max}\log\left(\prod_{i=1}^{N}P(x_i\mid \theta)\right)$，唯一需要动点脑子的就是其中的似然函数$P(x\mid \theta)$，在GDA中，我们令他为$P((x_i,y_i)\mid (\mu_0,\mu_1,\Sigma))=\mathcal{N}(\mu_0,\Sigma)^{1-y_i}\cdot\mathcal{N}(\mu_1,\Sigma)^{y_i}$，注意到由于存在两个模型，因此在已知参数的情况下为了获取某一样本点的概率，我们不止要传给似然函数样本点的数据，还需要给他的种类信息，他才能给我们返回正确的概率值，这也是符合常理的

接下来就是对其进行求导了，我们从简单的入手，先看看$\mu_0,\mu_1$怎么求解，其实我们不求也能大致猜到他们的最优值应该就是每一组分布点的均值向量，求解只是进一步证实我们的假设，和上一节类似，其中的$N_0,N_1$分别代表两种类别点的个数$N_0+N_1=N$，$C$表示和所求内容无关的常量，接下来以求解$\hat\mu_0$为例
$$
\begin{align*}
\arg\underset{\theta}{\max}\log\left(\prod_{i=1}^{N}P(x_i\mid \theta)\right)&=\arg\underset{\theta}{\max}\sum_{i=1}^{N}\left(\log \mathcal{N}\left(\mu_{0}, \Sigma\right)^{1-y_{i}}+\log \mathcal{N}\left(\mu_{1}, \Sigma\right)^{y_{i}}\right)\\
&=\arg\underset{\theta}{\max}\sum_{i=1}^{N}(1-y_i)\log\left(C e^{-\frac{1}{2}(x_i-\mu_0)^{T} \Sigma^{-1}(x_i-\mu_0)}+C\right)\\
&=\arg\underset{\theta}{\max}\sum_{i=1}^{N_0}(x_i-\mu_0)^{T} \Sigma^{-1}(x_i-\mu_0)
\end{align*}
$$
将$(x_i-\mu_0)^{T} \Sigma^{-1}(x_i-\mu_0)$求导后易得$\hat\mu_0=\sum_{i=1}^{N_0}x_i$的时候取极值，同理可得$\hat\mu_1=\sum_{i=1}^{N_1}x_i$，符合我们猜测，然后我们来求一下难一些的$\Sigma$，已知化简后$\sum_{i=1}^{N_0}\log \mathcal{N}(\mu_{0}, \Sigma)+\sum_{i=1}^{N_1}\log \mathcal{N}\left(\mu_{1}, \Sigma\right)$，我们令$g=\sum_{i=1}^{N}\log \mathcal{N}(\mu, \Sigma)$，求其关于$\Sigma$的导数
$$
\begin{align*}
\mathrm{d}g&=\mathrm{d}\left(\sum_{i=1}^{N}\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)}\right)\\
&=\mathrm{d}\left(C+\sum_{i=1}^{N}\log|\Sigma|^{-\frac{1}{2}}-\sum_{i=1}^{N}\frac{1}{2}(x_i-\mu)^{T} \Sigma^{-1}(x_i-\mu)\right)\\
&=-\frac{1}{2}\mathrm{d}(N\log|\Sigma|)-\frac{1}{2}\mathrm{d}\left(\mathrm{tr}\left(\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^{T} \Sigma^{-1}\right)\right)\\
&=-\frac{N}{2}\frac{1}{|\Sigma|}|\Sigma|\mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)-\frac{1}{2}\mathrm{tr}\left(\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^{T}\mathrm{d}\Sigma^{-1}\right)\\
&=-\frac{N}{2}\mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)-\frac{1}{2}\mathrm{tr}(N\Sigma_x(-\Sigma^{-1})\mathrm {d}\Sigma\Sigma^{-1})\\
\Rightarrow\frac{\partial g}{\partial\Sigma}&=-\frac{N}{2}\left(\Sigma^{-1}-\Sigma^{-1}\Sigma_x^T\Sigma^{-1}\right)=\frac{N}{2}\left(\Sigma_x^T\Sigma^{-2}-\Sigma^{-1}\right)
\end{align*}
$$

> 注意$\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^{T}=\Sigma_x,\Sigma\neq\Sigma_x$，$\Sigma$和$\Sigma_x$的形状都是$p\times p$，且均为对称矩阵，因此可交换，同时其中使用到了行列式和逆矩阵的求导结果，使用了矩阵求导一章中的$(10)(11)$定理，可以对照着看

将求解结果带入原式，令导数为$0$，则可得到$\hat\Sigma$
$$
\begin{align*}
\frac{\partial\log\left(\prod_{i=1}^{N}P(x_i\mid \theta)\right)}{\partial\Sigma}=\frac{N_0}{2}\left(\Sigma_0^T\hat\Sigma^{-2}-\hat\Sigma^{-1}\right)+\frac{N_1}{2}\left(\Sigma_1^T\hat\Sigma^{-2}-\hat\Sigma^{-1}\right)&=0\\
\hat\Sigma&=\frac{1}{N}(N_0\Sigma_0^T+N_1\Sigma_1^T)
\end{align*}
$$
于是我们得出了模型三个参数的解析解，他们分别是$\hat\mu_0=\sum_{i=1}^{N_0}x_i,\hat\mu_1=\sum_{i=1}^{N_1}x_i,\hat\Sigma=\frac{1}{N}(N_0\Sigma_0^T+N_1\Sigma_1^T)$，如果感觉这样看上去有点抽象，那么结合到实际的问题中，意思就是现在我们有一堆样本点集$X$和其对应的分类结果$Y$，接着我们把$X$中同类的样本点分别取出来数一数他们的个数$N_0,N_1$，并求他们的均值$\mu_0,\mu_1$和方差$\Sigma_0,\Sigma_1$，均值部分我们直接拿来用，方差则把他根据种类点的个数加个权$\frac{1}{N}(N_0\Sigma_0^T+N_1\Sigma_1^T)$产生新的统一方差$\Sigma$，这样我们就构造出了两个高斯模型$\mathcal{N}(\mu_0,\Sigma),\mathcal{N}(\mu_1,\Sigma)$，之后我们碰到了一个新的样本点，就把他丢到这两个模型里面看看输出，哪个大就说明这个样本属于哪个分类

我们总结一下，**GDA就是假定分类样本遵循同方差高斯分布的前提下，求解各自高斯模型的过程**

### 逻辑回归

不过我们手动把样本代到两个模型再人力判断哪个大显然还是比较麻烦，有没有什么可以像一开始的符号函数一样一条算式表示出结果分类呢

我们首先得意识到我们求出来的两个高斯模型的含义是什么，没错，就是$P(x\mid y_0),P(x\mid y_1)$，意思就是我已知某一组样本是什么种类了，我们把样本点$x$带入模型中，就可以得到这个样本点属于这个种类的概率，但是我们的目标其实是需要当我知道$x$之后其属于某一分类$y$的概率也就是$P(y_0\mid x)$才对

听起来似乎有点绕，那么我们可以假设一个样本点$x$，我们把他带入两个模型的过程就相当于求解了这个样本点在两个模型下的概率$P(x\mid y_0),P(x\mid y_1)$，比如概率一个是$0.9$一个是$0.8$，于是我们通过比较猜想$x$属于第一个模型，由于两个模型输出的结果是不相关的，因此必须要求我们求解两次模型结果，但是如果模型是$P(y_0\mid x)$，因为只考虑二分类问题，有$P(y_0\mid x)+P(y_1\mid x)=1$，那么我们只需要任取一个模型比如$P(y_0\mid x)$，通过传给他$x$就可以获知属于该分类的概率，由于只有属于这个分类和属于另一个分类两种情况，因此只需要判断其输出是否大于$0.5$即可知道该点应该被如何分类，肉眼可见的减少了一个模型的计算量

所以如何把$P(x\mid y_0)$变成$P(y_0\mid x)$，这就需要借助我们强大的贝叶斯公式了
$$
P(y_0\mid x)=\frac{P(x\mid y_0)P(y_0)}{P(x\mid y_0)P(y_0)+P(x\mid y_1)P(y_1)}
$$
其中$P(y_0),P(y_1)$我们之前称之为先验概率，此处其实也可以这么理解，由于所求的$P(y_0\mid x)$是一个预测模型，我们得根据已有的样本来判断两种种类的个数分布情况，比如简单一点我们拿到的样本$X$里$N_0=N_1$，意味着我们假设获取到的样本是服从等分布的，之后我们就可以计算一下$P(y_0\mid x)$的结果是什么样的
$$
P(y_0\mid x)=\frac{\mathcal{N}(\mu_0,\Sigma)}{\mathcal{N}(\mu_0,\Sigma)+\mathcal{N}(\mu_1,\Sigma)}=\frac{1}{1+\frac{\mathcal{N}(\mu_1,\Sigma)}{\mathcal{N}(\mu_0,\Sigma)}}=\frac{1}{1+e^{-z}}\\
z=\log\frac{\mathcal{N}(\mu_0,\Sigma)}{\mathcal{N}(\mu_1,\Sigma)}
$$
其中$P(y_0\mid x)$的化简结果$\frac{1}{1+e^{-z}}$就是大名鼎鼎的Sigmoid函数，我们也知道了原来Sigmoid是由贝叶斯公式变过来的，但是由于这条式子里还是需要计算两遍模型结果，好像和传说中的逻辑回归还差点什么东西，这时候就需要我们尝试化简一下其中$z$部分了
$$
\begin{align*}
z &= \log\left(\frac{\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_0)^{T} \Sigma^{-1}(x-\mu_0)}}{\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_1)^{T} \Sigma^{-1}(x-\mu_1)}}\right)\\
&=-\frac{1}{2}\left((x-\mu_0)^{T} \Sigma^{-1}(x-\mu_0)-(x-\mu_1)^{T} \Sigma^{-1}(x-\mu_1)\right)\\
&=-\frac{1}{2}x^T\Sigma^{-1}x+\mu_0^T\Sigma^{-1}x-\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0+\frac{1}{2}x^T\Sigma^{-1}x-\mu_1^T\Sigma^{-1}x+\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1\\
&=\left(\mu_0^T\Sigma^{-1}-\mu_1^T\Sigma^{-1}\right)x+\left(\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1-\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0\right)\\
&=w^Tx+b
\end{align*}
$$
我们发现了一个让我们惊掉下巴的事实，这个$z$居然是一个线性方程，说好的高斯模型呢？我那么大个的两个高斯模型跑哪去了？没错，高斯模型确实消失了，不然为什么说逻辑回归牛逼呢，实际上对于两个方差一致的高斯模型来说，无论其是几维模型，模型的接触面都一定是一个超平面，而这一条线性方程所描述的正是这条高斯模型的接触面，而这个面正是在两个高斯模型下分割分类点的最优超平面

不止如此，我们之前一直假定两种种类点的分布是符合高斯分布的，实际上谁说点的分布一定是高斯分布呢，我们发现，在这个线性方程中，如果把$b$看作$w^T$的一部分，实际上我们从需要求解$\mu_0,\mu_1,\Sigma$三个参数变成了只需要求解一个参数$w^T$，虽然可能这个参数的解析解算式比起那三个参数要复杂，但是至少他是一个参数，意味着Sigmoid的存在给了我们一种可能，可以让我们不止跳过代入两个模型比较解答的过程，更可以让我们跳过求解$\mu_0,\mu_1,\Sigma$参数的过程

理论存在，我们不如尝试实践看看，在求解$\mu_0,\mu_1,\Sigma$三个参数的时候，我们曾经构造出了一条$P(x\mid \theta)=\mathcal{N}(\mu_0,\Sigma)^{1-y_i}\cdot\mathcal{N}(\mu_1,\Sigma)^{y_i}$的算式，如今我们依葫芦画瓢，令Sigmoid函数为$\sigma(w^Tx)$，则构造$P(x\mid \theta)=\sigma(w^Tx)^{1-y_i}\cdot(1-\sigma(w^Tx))^{y_i}$，接着同样通过MLE尝试求解解析解$\hat w$
$$
\begin{align*}
\hat w&=\arg\underset{\theta}{\max}\log\left(\prod_{i=1}^{N}P(x\mid \theta)\right)\\
&=\arg\underset{\theta}{\max}\sum^N_{i=1}\log\left(\sigma(w^Tx_i)^{1-y_i}\cdot(1-\sigma(w^Tx_i))^{y_i}\right)\\
&=\arg\underset{\theta}{\max}\sum^N_{i=1}\left((1-y_i)\log\sigma(w^Tx_i)+y_i\log(1-\sigma(w^Tx_i))\right)
\end {align*}
$$
在这边我们不妨先暂停一下，虽然还没解完，但是到此为止其实已经证明了$\hat w$的可解性，之所以在此停顿，是为了顺便引出另一个重量级公式$\sum^N_{i=1}\left((1-y_i)\log\sigma(w^Tx_i)+y_i\log(1-\sigma(w^Tx_i))\right)$，交叉熵公式，比起直接令其导数为零解出其解析解，因为其非线性的特征（例如我很难直接得出$\sigma(x)=0.6$的时候$x$应该取多少），我们更喜欢使用梯度下降的方法求解其极值，再往后，就得谈及深度学习了

回过头来，我们确实把三个参数变成了一个参数，但毕竟求解的参数减少了，难道对模型就一点影响都没有吗？并不然，参数的减少实际上不再把点约束在高斯分布之内，而是涵盖了所有化简后会产生此类结果的模型，因此此处参数的减少不仅不会对求解精度造成损失，而且还做到了缓解过拟合的作用，令模型的泛化能力更加出色

说了那么多，我们按照惯例做一个总结，**所谓逻辑回归，从结构上看就是一个套了Sigmoid函数的线性模型，而其推导本质则是在GDA的基础上，泛化高斯模型之后化简得出的**

## 降维

在机器学习中降维是一个非常重要的思想，在LDA中我们初窥降维的用途，我们用它提供了一种高维分类的方法，实际上，LDA只能算是降维方法一种比较粗浅的应用，降维的主要应用还是集中在提取特征方面，一方面合理化的降维可以去除一组高维数据中的可能冗余特征保证提取特征的精度，另一方面在进行特征提取的时候，合理化的降维，制作可视化样本分布图也可以帮助我们提取正确的特征，提高分类工作的成功率

### 拉格朗日乘子法（等式约束）

求极值我们都很熟悉，但是在这一节，我们需要探讨一下如何求解约束条件下的极值问题

那么什么是约束条件下的极值问题，这是一个典型的约束条件的极值问题：在保证$x^2y=3$的情况下，求$x^2+y^2$的最小值，如果用数学语言表达，就是这样
$$
\begin{eqnarray}
&&\min(x^2+y^2)\\
&&s.t.\quad x^2y=3
\end{eqnarray}
$$
你说这还不简单，我把约束条件的$y$用$x$来表示，接着代换回求解的函数，然后求导求解最小值不就完了，这种思想就是求解约束条件下极值问题的一种解决思路——消元法，但是其中无论是讨论$x,y$是否为$0$的情况，还是约束条件到底方不方便让一个变量由另一个变量表示都是十分随机的，那么是否存在一种无脑一些的方法可以不用考虑那么多直接求解呢

这就需要我们换一个角度理解这个问题，我们令$f(x,y)=x^2+y^2,g(x,y)=x^2y$，接着先来看求解函数，$x^2+y^2$实际上就是在三维空间的一条曲面，且当没有约束条件的情况下，我们易得$x=0,y=0$时取得曲面在$z$方向时的最小值$0$，然后再来看约束条件，$x^2y=3$实际上是规定了$x,y$的取值范围，本来$g(x,y)$也是一个曲面，但是由于对其沿水平方向进行了切割，所得的刚好就是$g(x,y)$在$z=3$上的等高线，因此，我们要保证取值点的$x,y$落在$g(x,y)=3$这条等高线的前提下，使$f(x,y)$的取值最小

如果有点晕，那么可以看看以下这张图

![img](https://s2.loli.net/2023/02/22/yEwhe9rAi3DKWv8.png)

知道了求解目标的几何意义，对我们求解函数极值有什么帮助呢，由于$g(x,y)=3$看作是一个平面上的线，因此我们可以尝试把$f(x,y)$也投影到平面上，同时为了保留其大小的含义我们也对其添加等高线，等高线上的各点函数值相同

由于等高线有无数条，从直觉上来说，我们也应该知道想要取得极值，必须保证$g(x,y)=3$的线和某条等高线仅有一个交点，而这不就是相切的定义吗，接着我们再用一下幼儿园就学过的两条曲线相切的性质，我们发现其中一定有一条“交点处的切线斜率相同”的定义，引申出来即为相切曲线共用一条切线，而我们又有定义等高线的切线与其梯度垂直的定义，这不巧了，既然两条都是等高线，那我是不是也可以说，当$f(x,y)$在$g(x,y)=n$约束下取到极值时，极值点的梯度同向，即$\Delta f(x,y)=\lambda\Delta g(x,y),\lambda\in\mathbb{R}$

![img](https://s2.loli.net/2023/02/22/qhOMRbxvaoVsIH3.png)

> 上图紫线即为$f,g$在它们等高线的切点$A$的梯度方向，不过究竟梯度是向圆心还是向外取决于$f,g$三维图像函数值相对较高点，对于这个问题来说$x,y$越大$f$取值越大，因此梯度方向指向圆心反方向，同理可得$g$梯度方向和$f$同向
>
> 需要注意梯度向量虽然指向函数值较大方向，但是它是二维的，不存在$z$轴上的分量，只恒落在$x,y$轴组成的等高线平面上

到此为止，求解$f(x)$的极值我们就有了两个条件，根据它们我们可以构建方程组
$$
\begin{cases}
\Delta f(x,y)=\lambda\Delta g(x,y)\\
g(x,y)=3
\end{cases}\Rightarrow
\begin{cases}
\frac{\partial f}{\partial x}-\lambda\frac{\partial g}{\partial x}=0\\
\frac{\partial f}{\partial y}-\lambda\frac{\partial g}{\partial y}=0\\
x^2y-3=0
\end{cases}
$$
其中求解$f,g$梯度需要分别分为求解$x$方向偏导梯度和$y$方向梯度两条式子，三条式子求解三个变量明显是可解的，这样，我们就把一个有约束的极值问题转换为了固定的解方程问题

这种方法是完全正确的，不过有些人觉得这么写还是不够优雅，为了让求解过程变得足够直观，额外又构造了一个函数$\mathcal{L}(x,y,\lambda)=f-\lambda g',g'=g-3$，如果我们把$\mathcal{L}$看作是一个关于$x,y,\lambda$的无约束的优化问题，直接通过求偏导等于$0$的方式求解，就会发现其偏导结果正好对应了上面的三条式子
$$
\mathcal{L}'=0\Rightarrow
\begin{cases}
\frac{\partial L}{\partial x}=\frac{\partial f}{\partial x}-\lambda\frac{\partial g}{\partial x}=0\\
\frac{\partial L}{\partial y}=\frac{\partial f}{\partial y}-\lambda\frac{\partial g}{\partial y}=0\\
\frac{\partial L}{\partial \lambda}=-(g-3)=0
\end{cases}
$$
到此为止，我们终于把一个有约束的优化问题转化为了无约束的优化问题，这个方法同样适用于更高维的$f,g$，总结一下，**仅从做题结果来说，使用拉格朗日乘子法求解等式约束的优化问题分为如下四步**

1. **构造求解函数$f$和约束条件函数$g$，且应当把约束条件转换为$g=0$的形式**
2. **定义拉格朗日乘子$\lambda$，并构造函数$\mathcal{L}=f-\lambda g$**
3. **将$f,g$替换为多维的自变量函数，并对每一维变量和$\lambda$分别求偏导，令偏导数为$0$**
4. **联立求解多元方程组，获得的自变量取值即为所需极值（如果有多个，说明存在多个极大和极小值，分别带入$f$验证即可）**

### Centering Matrix

在线性分类的讨论中，我们曾经令所有样本点的排列为$X=(x_1,x_2\dots x_N)$，但是无论是在求解均值还是方差的过程中我们似乎都没有用到$X$，由于从小老师就只教了一个个$x_i$加起来然后除个数的算法因此这么做也是合理的，但是在程序中，我们往往用一个矩阵$X$表示所有的样本点，如果按照我们的方法就得把$X$先拆了再求，那还不如不构造$X$，是否有一种在保留$X$的情况下直接求解均值和方差的做法呢

我们不妨试一试，注意假设所有样本点$x_i$都是列向量
$$
\begin{align*}
\bar{X}_{p\times 1}&=\frac{1}{N}\sum_{i=1}^{N}x_i\\
&=\frac{1}{N}\begin{pmatrix}
 x_1 & x_2 & \cdots & x_N
\end{pmatrix}\begin{pmatrix}
 1 \\ 1 \\ \vdots \\ 1
\end{pmatrix}\\
&=\frac{1}{N}X\mathbb{1}_N
\end{align*}
$$

$$
\begin{align*}
\Sigma_{p \times p} 
&=\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}-\bar{X}\right)\left(x_{i}-\bar{X}\right)^{T} \\

& =\frac{1}{N}
\begin{pmatrix}
    x_{1}-\bar{X} & x_{2}-\bar{X} & \cdots & x_{N}-\bar{X}
\end{pmatrix}
\begin{pmatrix}
    \left(x_{1}-\bar{X}\right)^{T} \\
    \left(x_{2}-\bar{X}\right)^{T} \\
    \vdots \\
    \left(x_{N}-\bar{X}\right)^{T}
\end{pmatrix}\\

&=\frac{1}{N}
\left(
    \begin{pmatrix}
    x_{1} & x_{2} & \cdots & x_{N}
    \end{pmatrix}
    -\bar{X} \cdot \mathbb{1}_{N}^{T}
\right)\left(\begin{pmatrix}
x_{1}^{T} \\
x_{2}^{T} \\
\vdots \\
x_{N}^{T}
\end{pmatrix}-\mathbb{1}_{N} \cdot \bar{X}^{T}\right) \\

&=\frac{1}{N}\left(X^{T}-\bar{X} \cdot \mathbb{1}_{N}^{T}\right)\left(X-\mathbb{1}_{N} \cdot \bar{X}\right)^{T}\\

& =\frac{1}{N}\left(X^{T}-\frac{1}{N} X^{T} \mathbb{1}_{N} \mathbb{1}_{N}^{T}\right)\left(X-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T} X\right) \\

& =\frac{1}{N} X^{T}\left(I_{N}-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T}\right)\left(I_{N}-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T}\right) X
\end{align*}
$$

其中，我们令$I_{N}-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T}$是一个$N\times N$的方阵，我们称他为centering matrix，记作$H$，可以证明的是$H^T=H,H^n=H$，由于$X^{T}-\bar{X} \cdot \mathbb{1}_{N}^{T}=X^TH^T$，因此这个$H$矩阵在数学意义上其实就是把相乘矩阵的均值置零，矩阵$X$的方差可化简为$\Sigma=\frac{1}{N} X^{T}HX$

与此同时，通过这个式子我们也可以说明其实方差的计算可以被分为两步，第一步$HX$表示把所有样本点整体移动到原点附近，第二步$(XH)^THX$则是把移动后的每一个样本点计算和原点的距离的平方相加，换一个角度来说，如果样本点原本的均值就是零，则$X^TX$就可以表示为其方差矩阵

> 需要注意由于$X$不是方阵，因此$X^TX\ne XX^T$，一个形状是$N\times N$另一个是$p\times p$，$H$的大小也是同理

### 奇异值分解（SVD）

在说奇异值分解之前，我们需要先回顾一下特征值分解的过程，首先我们需要了解到一个方阵的特征值代表了什么含义，说到特征值，我们就不得不提矩阵，特征值和特征向量相互之间的关系式
$$
Av=\lambda v
$$
其中$A$是分解矩阵，$v$是特征向量，$\lambda$是特征值，线代中我们往往只是知道这么一条式子但不知道特征值和特征向量的具体含义，实际上，任何一个方阵乘以一个向量所得结果一定是和该向量同维的另一条向量，因此每一个方阵从另一种角度也可以看作是给一个向量做了一次线性变换，而线性变换的过程包括伸缩和旋转，接着我们再来看这条有关特征值的式子，我们发现了，在这个式子中矩阵对特征向量做的线性变换居然能通过一个常数乘这个特征向量得到，而我们又知道常数只会对向量做伸缩变换，因此找寻特征值的过程总结一下就是找寻某一条不受方阵旋转变换影响的特征向量

![Image](https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuypRuABCGAYIouIazEuNcZTibpdjY39e0kHWiaUF1PHjEfXovvTWaFmlopSu9RgPicvVzdpcZdqpyhiaA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

> 上面是$M=\begin{bmatrix}
>  1 & 1\\
>  1 &0
> \end{bmatrix}$在二维空间的变换过程，我们可以发现$(1,0)$这个方向的向量方向完全不受坐标轴的拉伸和扭曲影响，意味着其就可以作为方阵$M$的一个特征向量

接着我们再复习一下特征值的分解过程，我们的目标是将方阵$A$分解成$Q\Sigma Q^T$形式，其中$Q$是由特征列向量排列成的矩阵，$\Sigma$是对角矩阵，其每一个对角元素皆为一个$A$的特征值，且由左上至右下依次减小，其中的求解过程大致可以分为三步

1. 构造计算$|A-\lambda I|=0$求得多个$\lambda$特征值
2. 把求解出来的特征值带回$(A-\lambda I)x=0$解线性方程组，求得各个特征向量$x$，由于我们只需要特征向量方向，大小并不重要，因此一般来说我们会把特征向量缩放成单位向量作为最终结果
3. 把各个特征值从大到小沿对角排列构造出$\Sigma$，特征向量与其相对应排列为$Q$，如果有重根则排列多次，保证$A$和$\Sigma$同维度

但是我们也提到了，特征值分解强制要求$A$是方阵，这也符合理解，不然和向量相乘出来也不是另一个同维度向量了，但是终究说到底还是方阵，于是就有人想是否可以把他拓展到任一矩阵中，于是就产生了奇异值分解

对于一个$A_{m\times n}$的矩阵，我们要将其分解成$U_{m\times m}\Sigma_{m\times n} V_{n \times n}^T$，且这个分解是任一矩阵均存在的，奇异值分解的具体过程如下

1. 计算$AA^T,A^TA$，易得这两个矩阵为实对称方阵

2. 求解他们的特征值和特征向量，注意到实对称方阵的特征向量一定互相正交，这两个方阵的特征值一定只差0的个数（后一个的严格证明可看下图）

   ![img](https://iknow-pic.cdn.bcebos.com/4a36acaf2edda3cc014e752d02e93901213f9211?x-bce-process=image%2Fresize%2Cm_lfit%2Cw_600%2Ch_800%2Climit_1%2Fquality%2Cq_85%2Fformat%2Cf_auto)

3. 令$AA^T$特征向量方阵为$U$，$A^TA$特征向量方阵为$V$，非零特征值取根号后按大小排布构成矩阵$\Sigma'$，之后根据$A$的形状把$\Sigma'$补零成$\Sigma$，其构造证明可以看以下式子
   $$
   \begin{cases}
   A=U\Sigma V^T \\
   A^T=V\Sigma^T U^T
   \end{cases}\Rightarrow
   AA^T=U\Sigma V^T\cdot V\Sigma^T U^T=U\Sigma^2 U^T
   $$
   同理可得$U,V$分别就是$AA^T,A^TA$的特征向量组，$AA^T,A^TA$的任一特征值的排布正是$\Sigma^2$

对于奇异值的实际作用，可以在接下来的PCA推导中看到，此处我们从奇异值的分布情况稍微提一下，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上。也就是说，剩下的90%甚至99%的奇异值几乎没有什么作用，例如我随便打一个矩阵
$$
A=\begin{bmatrix}
 1 & 2 & 3 &4  & 4\\
 1 & 2 & 3 &4  & 6\\
 4 & 5 & 7 & 8 &4 \\
 9 & 4 & 5 &2  &1
\end{bmatrix}\Rightarrow \Sigma=\begin{bmatrix}
 18.49 & 0 & 0 &0  & 0\\
 0 & 7.64 & 0 &0  & 0\\
 0 & 0 & 2.93 & 0 &0 \\
 0 & 0 & 0 &0.06  &0
\end{bmatrix}
$$

### 主成分分析（PCA）

经过了这么多的知识铺垫，我们终于可以来看看本章的重点PCA了，在此之前我们先思考一个问题，假如我现在拿到了几组样本点，我们怎么知道这些样本点到底可不可分呢，有时候我们总会在某些论文上看到画的五颜六色的散点图，但是这些图又是怎么画出来的呢，数据的处理固然重要，但是如何把数据用可视化的方法展示出来同样是十分重要的一环

#### 最大投影方差

在LDA一章中，我们详细介绍了如何通过合理的降维完成分类，PCA和他的思想有点像，但是在LDA中我们已经存在前提就是样本点数据是线性可分的，我们的目标就是对其进行分类，而PCA的目标并不是区分样本点，只是把我们无法观测到的高维样本点用我们可以理解的方式表示出来而已

降维最容易碰到的问题就是数据点重合在一起，比如$(1,2,3),(1,2,4)$两个点只有第三维坐标不一样，如果我们把他们沿$x,y$平面降维，也就是投影到$x,y$平面上，就会出现两个点在二维平面上叠在了一起，仅看降维后的平面我们完全无法区分这两个点，从观测角度显然是不利的，但是如果我们将其沿别的平面上投影，两个点就可能可以被分开，而当数据点变多，每一个数据点之间的距离越大，显然也就越利于我们分析

降维后重叠在一起的点我们可以称之为一种特征的损失，不仅是对我们观测不利，由于他把高维中两个不相关的点投影到了同一个位置，在降维的其他使用领域，比如降噪，压缩等任务中同样是不利的，如何找到一个尽可能让数据点投影上去能够被分开的超平面就成了我们需要克服的问题

在LDA中，我们使用了投影后的均值和方差来找到投影直线，在此处我们同样可以使用这个思路，既然需要在超平面中尽可能被分开，是不是意味着投影到这个超平面的数据点之间的方差要尽可能大呢？于是就有了最大投影方差的解决思路

和LDA不一样的是，此处我们不再仅限于投影到一条直线上，而是要取能使样本点方差尽可能大的前几条投影直线，将样本点投影到这些若干条直线之后用这些直线当作另一组坐标基重构样本点，易得所重构的超平面一定是能够时样本投影方差最大的超平面，同时由于这些直线个数可以根据实际需求任取，意味着降维到的维数也是可控的

于是现在的问题就是如何求解这一组投影直线了，实际上我们完全可以用类似LDA的方法求解，除了少求一次方差之外可以说其余步骤都是完全一致的，由于我们只需要最大化投影方差，因此我们甚至可以直接把方差计算作为用于最大化的损失函数，我们定义投影向量为$w_1$，直接套用LDA的化简结果
$$
L=w^T\frac{1}{N}\sum^{N}_{i=1}(x_i-\bar x)(x_i-\bar x)^T w=w^T\Sigma w
$$
然后我们发现这玩意好像有点问题，由于我们没有限定投影向量的长度，因此好像只要$w$越长这玩意结果就越大，这样的话就不管$X$什么事了，更无法确定$w$的方向，这显然不是我们想要看到的，为了公平起见，我们令$w$是单位向量，即$w^Tw=1$

于是问题就转化为了在约束条件$w^Tw=1$求解$L$的最大值，接下来就是拉格朗日乘子法出场的时候了
$$
L'=L-\lambda(w^Tw-1)
$$
同时需要注意我们的目标只是确定$w$分量的方向，因此我们可以仅对$w$求偏导令其等于$0$
$$
\frac{\partial L'}{\partial w}=2\Sigma w-2\lambda w=0\\
\Rightarrow\Sigma w=\lambda w
$$
这玩意不就是$\Sigma$方阵的特征方程吗，对$w$求偏导之后得出来的这条等式，用人话说就是对于$w$分量来说他所有的极值点都集中在他自己是作为$\Sigma$的特征向量上了，也就是我们想取到$L'$的最大值，就得去这些特征向量上找，不止如此，从这条式子我们还能知道$\Sigma w$的大小是受$\lambda w$控制的，因为我们没办法改变$w$的大小，因此求最大值的重担就落到了$\lambda$身上了，从大到小，我们每取一个特征值，相对应的特征向量$w$就作为样本点的一条投影向量

#### 正交基

如果你仔细想想就会发现好像什么东西不太对，我们在介绍拉格朗日乘子法的时候曾经说过，这个方法只能获知所求函数的多个极值，想知道求出来的哪个是最大或者最小值，正常还应该把他带回到原始的式子中看看结果，与此同时我们还知道既然这个$L'$是可导的，那么其在高维空间中的函数图像也一定是连续的，既然他图像又连续，又存在最大值，甚至我们还知道最大值就是特征值最大的那一条特征向量，也就是说我们知道所有的样本点一定是在这一条特征向量上的投影方差可以达到最大值，那么按理说把这条最大的特征向量任意旋转一个极其微小的角度，落在这条被微调过的向量上的投影方差一定是可以无限接近原始向量的，也就是说，除了最大特征值对应的那个特征向量，其余的特征向量所获得的投影方差一定小于这条微调过的向量，既然如此，又为什么要逐个取每一个特征向量呢，而且如此一来只有投影方差最大的那条向量可以被确定，又如何把一组样本点投影到多条向量之后重构空间呢？

这就得说到方差矩阵的特征向量的另一个重要性质了，没错就是正交性，由于方差矩阵（或者明确说是协方差矩阵）是一个对称矩阵，意味着其每一个特征向量之间一定是相互垂直（正交）的，于是你会发现一个惊人的事实，我们如果把样本数据照特征向量顺序逐个做投影，然后把投影结果（是一个值）排在一起作为另一组样本特征（是一组向量），如果我们能把各个数据点画在一个超维空间中，那么以上的步骤实际上就是把所有的点旋转了一下而已

![img](https://s2.loli.net/2023/02/23/nH1KxfyhwlU3ZiS.png)

如上图所示，求解所得的特征值最大的特征向量比如是$w_1$，我们的确可以看到各个样本点（蓝点）在其上面的方差是最大的，另一条特征向量与之垂直，在二维平面上则一定是绿色这条$w_2$，然后我们发现在这两条特征向量的映射下，如果把$w_1,w_2$分别看作是新的$x,y$轴，则所有的蓝色点仅做了平移和旋转变换，而且其中的平移变换还是因为为了便于观察我把特征向量挪到了样本点均值处，正常求解出来的特征向量是从原点出发的

仅作旋转变换最显著的好处就是不改变样本的分布，并且其保留了投影方差最大的特征向量，意味着假如我想把上面的蓝色点降维到一条直线上，那么只需要保留$w_1$一条向量和其上的投影点即可，对于更高维的样本点，由于我们有了特征值这一个衡量维度去判断特征向量的好坏，因此同理只取前$k$大特征值对应的特征向量，就可以在保证不改变样本点分布且尽可能以投影方差最大的前提下将样本降维到$k$维

#### 最小重构代价

如果说最大投影方差是顺着思路一步步推导出特征向量作为基底的过程，那么最小重构代价就是倒着进行，我们先假设一堆样本点已经在$w$的影响下从$p$维被降维到了$q$，接着通过比较降维前后数据点的偏移情况求解最优化正交基

![img](https://s2.loli.net/2023/02/23/p427boKtyAFLNPc.png)

由于这种方法不需要求解样本方差，因此我可以取样本中的任意点单独讨论，比如现在我有一个样本点$A$，在上一节当中我们已经明确需要寻找的投影向量必须相互正交，因此此处我们就任意构造出两条正交单位向量$w_1,w_2$，$A$点在两条基上的投影分别记为$D,E$，如果假设$A$点坐标$(B,C)^T=a$，根据点乘等于投影长度，易得$OD,OE$长度分别为$w_1^Ta,w_2^Ta$，那么我们就可以得到$a$的另一组向量表示$a=(w_1^Ta)w_1+(w_2^Ta)w_2$，你可能奇怪这有啥意义，搞那么半天得到的不还是$a$，为什么要如此多此一举

先别急，接下来我们要把他降到一维，按照上一节的说法，只需要丢弃一个维度就行了，比如我可以不要$w_2$，那么现在我们就只有一条向量$w_1$了，没有了$OD$来控制$A$在$w_2$上的分量，等于说$A$点被降维到了$A'$点上，照着上面的样子，我们易得$A'$坐标$a'=(w_1^Ta)w_1$

现在我们用统一的$a$和正交基来表示了降维前后的两点，并且将其映射回了原空间中，于是我们就开始思考，降维这个操作让原本的$A$点变成了$A'$点，我们是否可以定量的表示这个变换中的特征损失情况呢，然后你可能发现还真能，我只需要计算两点之间的距离$AA'=d$不就行了吗，明眼人都看得出来，要是$d$比较大，明显和原本的$A$的距离就越远，距离越远就越不相似，相对应的降维特征损失也就越多了

**这种在原始空间中计算降维后的点在重构回降维前的样本点的距离最小值的方法就叫做最小重构距离**，下面是计算过程，距离的计算直接使用欧氏距离即可，同时注意样本点不止一个且维度也不止二维，我们假设有$N$个$p$维样本点$x_1,x_2\dots x_N$，需要在$w_1,w_2\dots w_p$这一组正交基上被降维到$q$维，即抛弃后$p-q$个$w$，只使用$w_1,w_2\dots w_q$表示降维点$x'_1,x'_2\dots x'_N$，需要注意$x'$的维数也是$p$，可以参考上面的$a'$推导过程
$$
\begin{align*}
L&=\frac{1}{N}\sum_{i=1}^N\left \| x_i-x'_i \right \| ^2\\
&=\frac{1}{N}\sum_{i=1}^N\left \|\sum_{j=1}^{p-q}(w_j^Tx_i)w_j\right \|^2\\
&=\frac{1}{N}\sum_{i=1}^N\sum_{j=q+1}^{N}(w_j^Tx_i)^2\\
&=\sum_{j=q+1}^{N}\frac{1}{N}\sum_{i=1}^N(w_j^T(x_i-0))^2
\end{align*}
$$
其中如果我们把$x_i$看作是一组均值为$0$的样本，那么$\frac{1}{N}\sum_{i=1}^N(w_j^T(x_i-0))^2$就可以看作是$w_j^T\Sigma w_j$，我们要取其最小值，因此问题就变成了下面这样
$$
L=\arg\underset{\theta}{\min}\sum_{j=q+1}^{N}w_j^T\Sigma w_j\\
s.t.\quad w_j^Tw_j=1
$$
由于每一组$w_j$都是相互独立的，因此这和上一节的求解式子是一样的，结果同样是$\Sigma w=\lambda w$，不过需要注意的是出我们求解的是最小值，同时由于我们前提已经假设了$w$之间相互正交，因此$w_j$是全不相同的，因此求解的结果就是取前$p-q$个最小的特征值对应的特征向量作为解，而我们又已经假设了第$q+1$到$N$个$w$其实是在降维中被丢弃的，因此此处求解出来的结果就是在$p$降到$q$维的过程中不需要的，换个角度，留下来的那些特征向量正是前$q$个最大的，这和在最大投影方差一节中求解的结果是完全一致的

不过我们在其中假设了一下所有$x$都是以均值为$0$排布的，这也是解释了为什么在上一节中需要把特征向量挪到样本点均值为起始的原因

通过两次推导，到此我们可以总结一下PCA的一般操作过程，假设我们要把样本点集$X_{p\times N}$降维到$q$维，则只需要先计算样本的方差$\Sigma$的特征值，取前$q$个最大特征值的单位特征向量作为基底，然后把所有样本点与这些基底做点积计算投影长度，把投影长度重新排布就形成了降维后的样本点矩阵$X'_{q\times N}$

#### SVD和PCoA

了解了便于理解的PCA做法，然而从写程序或者是计算角度来说，这种方法其实并不方便，不仅要逐个遍历样本计算特征值和点积，还要把算完的内容重新排列组合，是否有什么快捷的手段可以直接得出PCA降维后的坐标结果呢

为了避免歧义，以下把样本点$X$的方差记为$S$，根据前面的结论，有$S=\frac{1}{N}X^TH^THX$，由于$HX$不是方阵，我们对$HX$进行奇异值分解记为$HX=U\Sigma V^T$

把奇异值分解结果带回$S$，则有$S=V\Sigma^T\Sigma V^T$，这样一来，我们就可以跳过先求解$S$再求解其特征值的步骤，直接求解$HX$的奇异值分解矩阵$\Sigma$和右乘特征向量矩阵$V$即可，然而实际做下来你却发现看似简单一点实际也是骗人的，因为求奇异值的过程等于已经间接求了一次$S$和他的特征值，实际并没有简便多少，而且之后的坐标映射同样需要手动完成

于是我们可以再想想有没有什么办法可以直接拿到降维后的坐标，由于我们知道了需要求解的特征向量组就是$V$，因此坐标的映射关系可以写作$HX\cdot V=U\Sigma V^TV=U\Sigma$，诶这下我们发现了$U\Sigma$正是降维后的坐标，与此同时我们还发现如果我们构造一个$T=HXX^TH^T$，那么按照化简$S$的经验，他化简之后正是$U\Sigma\Sigma^TU^T$，因此为了得到最终的坐标，我们完全可以直接对$T$进行特征值分解，取前$q$大的特征值取根号之后直接和其对应的特征向量矩阵$U$相乘获得所有的映射坐标值，这种不借助样本方差，直接求解降维坐标的方法就叫做PCoA

至此，我们可以把PCA类的降维问题总结成四步

1. **把所有样本点$X$统一移动到原点附近，也就是构造新样本点$X_1=HX$**
2. **把新样本点矩阵$X_1$乘他自己的转置$X_2=X_1X_1^T$，这个结果的形状是$N\times N$**
3. **求出来的这个方阵进行特征值分解，$X_2=Q\Sigma Q^T$，接着只取降维维度的前$q$个特征值和特征向量$Q',\Sigma'$**
4. **求解$X'=Q'\sqrt{\Sigma'}$获得降维坐标，其形状为$q\times N$**

## 支持向量机（SVM）

在第三章中我们通过假设各组样本点的分布情况，推出了似乎十分厉害的逻辑回归，但是我们也提到了，线性分类之所以叫线性分类，就是因为他没有办法分类非线性可分的样本点，为了能够区分此类样本，深度学习提供了一种神经网络的结构，通过叠加多层感知机达到非线性划分的目的，但是在神经网络还没广泛流行起来的时期，说起非线性分类问题，就不得不提到在当时大名鼎鼎的SVM算法了

### 拉格朗日乘子法（不等式约束）

#### 梯度角度

上一章中我们曾经提到了拉格朗日乘子法在等式约束下的应用，但是实际情况中不止有约束条件是等式的情况，还可能出现不等式约束的求解问题，我们不妨把前面用过的式子变一变，尝试讨论一下不等式约束条件下的函数极值求解
$$
\begin{eqnarray}
&&\min(x^2+y^2)\\
&&s.t.\quad x^2y\ge 3
\end{eqnarray}
$$
同样的，我们也可以把他的等高线图画出来

![image-20230306171654238](https://s2.loli.net/2023/03/06/F24I51a7BQzmhbi.png)

由于我们限制了$x^2y\ge 3$，又因为梯度总是指向函数值增大的方向，因此在$\ge$约束条件下，梯度方向一定指向蓝色区域内，而我们又知道约束条件限制了$x$的取值范围，因此$x,y$同样只能取到蓝色范围的区域内，由于我们需要求解$f(x,y)$在约束条件下的最小值，因此此时易得最小值点$(x,y)$一定在蓝色区域的边缘线切点$B$上取到，也就是说我们分析了半天，对于这个问题来说，$x^2y\ge 3$实际上和$x^2y=3$的解法是完全一致的，即同样有
$$
\begin{cases}
\Delta f(x,y)=\lambda\Delta g(x,y)\\
g(x,y)=3
\end{cases}
$$
不过其实有人也想到了，这种解法并不是唯一的，比如我把约束条件的不等号改一改方向，变成限制$x^2y\le 3$

![image-20230308131241693](https://s2.loli.net/2023/03/08/LQnemkY2uAZGUJo.png)

此时我们发现，约束条件似乎并不对求解$f(x,y)$的最小值产生什么影响，甚至我们可以完全不管什么$x^2y\le 3$，直接求解$f(x,y)$的最小值$A$就是答案了

究其原因，我们发现控制最小值点究竟是在约束区域内还是约束区域边缘取到同样可以通过切点处的梯度方向来确定，**对于不等式约束来说，我们规定约束已经被化为$\le$的形式，则如果约束条件和原图像在切点处的梯度方向相反，则最小值在约束区域边缘处取到，反之最小值在约束区域内取到，且最小值就等于$f(x)$的最小值**，如上图中$f,g$在$B$点的梯度明显方向是相同的，因此约束$g$不起作用，直接求解$f$最小值即可

> 注意其中$f,g$都是关于$x$的函数，其中$x$是一个包含$x_1,x_2\dots x_i$的集合，即$f,g$都是$i$维函数，每一个自变量就是一个$x_i$，比如上面例子中$i=2$

#### 数学角度

说着容易，但是如果你想把他化作数学式子的形式，就会发现根据相切处梯度方向来分段两个函数似乎不太容易，意味着我们必须要先求解一次切点梯度方向，再根据求解结果选择最终求解函数求解一次最小值，由于求解过程必须被分成两步，因此在等式约束中的构造$\mathcal{L}=f-\lambda g'$然后直接求解一次偏导显然就不太对了，不过这并不代表这个函数的构造有问题，我们看下面这个转化方程组
$$
\begin{cases}
\underset{x}{\min} f\\
g\le0
\end{cases}\Rightarrow
\begin{cases}
\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}\\
\mathcal{L}=f+\lambda g\\
\lambda\ge 0
\end{cases}
$$
看上去晕晕的，怎么又$\min$又$\max$的，其实也非常好理解，我们从里往外看，$\underset{\lambda}{\max} \mathcal{L}$就是只把$\lambda$看成自变量，由于$f,g$和$\lambda$都没什么关系都可以看成常数，求解函数取到最大值时的$\lambda$就变得很简单了

- 当$g>0$的时候为了取得最大值则必须要求$\lambda\to +\infty$，于是$\underset{\lambda}{\max} \mathcal{L}\to +\infty$，此时我们发现$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$由于$\underset{\lambda}{\max} \mathcal{L}\to +\infty$了所以无论$x$取什么只要让$g>0$了最后的结果就一定是正无穷

- 当$g<0$的时候，由于存在约束$\lambda\ge 0$，因此$\lambda=0$时$\mathcal{L}$取最大，此时的$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}=\underset{x}{\min} f$，这就是我们原方程组的目标优化函数，而又有$\underset{x}{\min} f<+\infty$，因此只要用一个额外的约束让$\lambda$的最小值存在而最大值不存在，就可以保证$x$的取值不可能让$g$大于$0$，即满足原方程组的约束条件$g\le0$

  > 通过这一部分分析也可以发现从数学角度来说$\lambda\ge0$的原因和从图像角度的梯度反向是一个问题的两种解释，在数学方面最小值为$0$刚好可以约去$\mathcal{L}$的$g$部分使得求解式子恰巧等于原问题$\underset{x}{\min} f$

- 而当$g=0$的时候，我们发现无论$\lambda$取什么东西都不会对$\mathcal{L}$的结果产生影响，而又因为$\underset{\lambda}{\max} \mathcal{L}$的含义就是$\mathcal{L}$取到最大值时的$\lambda$取值，既然$\lambda$任取啥都不影响$\mathcal{L}$，因此我们完全可以忽略掉$\underset{\lambda}{\max}$了，这也是为什么在等式约束的时候我们只需要求解$\underset{x}{\min} \mathcal{L}$就行了，并不是两个式子有什么不同，而是在过程中我们省略了无作用的$\underset{\lambda}{\max}$而已

综上所述，我们推导出了原方程组和转化方程组的等价性，从结果上来说，我们将一个$x$约束$g\le0$的问题转化成了新增变量$\lambda\ge0$约束问题，并且重新构造了求解条件$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$来统一化$x,\lambda$的求解，看似这样的转化把原有的问题搞复杂了，但是在下一节我们就会详细讨论这样变化在特殊$f,g$场景下的优越性

事实上还可以证明对于等式和不等式的多条约束，同样可以进行如下的转化
$$
\begin{cases}
\underset{x}{\min} f\\
g_1=0\\
\vdots\\
g_N=0\\
k_1\le0\\
\vdots\\
k_M\le0\\
\end{cases}\Rightarrow
\begin{cases}
\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}\\
\mathcal{L}=f+\sum^N_{i=1}\eta_ig_i+\sum^M_{i=1}\lambda_ik_i\\
\lambda_1\ge 0\\
\vdots\\
\lambda_M\ge 0\\
\end{cases}
$$
而这就是拉格朗日乘子法的完整形态

### 函数对偶性

在上一节中我们提出了拉格朗日乘子法的完整形式，但是我们发现转化出的方程式比起原有的方程式并没有简便多少，照这样下去我们还是无法求解出不等式约束下的优化通解，在这一节中我们引入转化方程式求解函数$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$对偶函数$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L}$，看看这两个函数之间有什么关系

说是对偶函数，实际上其实就是改了一下$\max$和$\min$的顺序而已，这有什么难的，为了解释清楚对偶函数之间的关系，我令$x=g,y=f$，尝试通过参数方程的模式把$f,g$一起画在一个坐标系下，先别问为什么，我们就假设这两函数的参数方程画出来如图$ABCDE$这样

![image-20230309214058806](https://s2.loli.net/2023/03/09/wpxL1caAGKi5eJo.png)

看上去好像很奇怪，两个函数怎么会变成这个样子，都说了是假设，如果对参数方程不是很理解，那么可以类比比如$x=\sin x,y=\cos x$的时候画出来就是一个圆，那么$x=f,y=g$能画出这种图像也不是很奇怪了

此时我们来看看$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L},\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$两个对偶函数值在这个图像上会表示成什么样子，我们仍然假定约束为$\lambda\ge0$

- 先来看$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$，上一节我们提到它等价于$\begin{cases}
  \underset{x}{\min} f\\
  g\le0
  \end{cases}$，翻译成中文就是取$g$小于等于$0$的部分中的$f$的最小值作为输出，我们可以用一条直线从上往下扫过图形中$g\le0$的部分并取折线上投影到$y$（也就是$f$）的位置作为输出（如图中直线$G_1,G_2\dots $），然后我们发现当直线为$AG$的时候取到$f$最小值，我们不关心此时$x$取什么，但我们知道$\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}=G$

- 然后我们看复杂一些的$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L}$，其中的$\mathcal{L}=f+\lambda g=y+\lambda x$，我们假设$\mathcal{L}=0=y+\lambda x$此时就是一条过原点斜率为负（$\lambda\ge0$）的直线，同时输出值$0$是这条直线过纵轴的截距

  - 对于前半部分$\underset{x}{\min} \mathcal{L}$表示过图形上任意点的任意斜率为负的直线，使其截距最小，则其要么过$A$点要么过$D$点，正如图$H_1,H_2$所示，其中$DJ$表示斜率为$0$时的直线，确定了直线必过$A$或$D$
  - 我们再来看$\underset{\lambda}{\max}$部分，他表示在前面$\underset{x}{\min} \mathcal{L}$的基础上，通过改变斜率的大小，使直线的截距最大

  上面那么多翻译成人话就是找一条过$A$点或过$D$点的直线，令其截距最大，哦你这么说我就明白了，那不就是连接$AD$后过纵轴的截距吗，没错，于是我们推导出$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L}=H$

从图像中很直观的看出纵坐标$H<G$，即$\underset{\lambda}{\max} \underset{x}{\min} \mathcal{L}<\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$，而这就是传说中的函数弱对偶关系，既然有弱对偶关系，那一定有强对偶关系吧，没错，下面我们再来看一张图

![image-20230309220344313](https://s2.loli.net/2023/03/09/xriCujI2VHQTbNm.png)

如果$f,g$围成的图案如上面的蓝色区域这样，那么根据相同的规则求解出来的$=\underset{x}{\min} \underset{\lambda}{\max} \mathcal{L}$，这就是函数的强对偶关系，而这两个图像的区别就是其凸性，**若由求解问题$f$和约束条件$g$围成的图形在平面中呈现为凸函数或凸集，则其恒具有强对偶关系，我们可以通过求解其对偶问题的解来变相求解原问题的解**

另外可证明的是**对于任意函数弱对偶关系成立**，且**当$f$为凸函数，$g$为仿射函数**时恒有强对偶关系成立（即满足**凸优化问题\+Slater条件**，此处不证），因此对于求解问题为凸函数的问题来说，拉格朗日乘子法的求解过程可以再次转化为

> 凸函数的定义就是选取函数图线上的任意两点其连线部分恒在图像上方，凸集的定义则是闭合区域内选取任意两点连线均在闭合区域范围内，也可以用求二阶导大于零的方式判断凸性
>
> 而仿射函数可以简单理解为线性函数，即$g=Ax+b$此形式函数

$$
\begin{cases}
\underset{x}{\min} f\\
{\color{red}f\text{ is convex function}}\\
g_1=0\\
\vdots\\
g_N=0\\
k_1\le0\\
\vdots\\
k_M\le0\\
\end{cases}\Rightarrow
\begin{cases}
{\color{red}\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}}\\
\mathcal{L}=f+\sum^N_{i=1}\eta_ig_i+\sum^{M}_{i=1}\lambda_ik_i\\
\lambda_1\ge 0\\
\vdots\\
\lambda_M\ge 0\\
\end{cases}
$$

### KKT条件

当约束问题满足强对偶关系的情况下，我们求解对偶问题其实比起求解原问题要方便不少，原因是对偶问题的求解可以推出一些容易求解的结论，最著名的就是传说中的KKT条件，这东西听起来挺唬人，但是在我们有了前两节的只是积累之后会发现其实这玩意并没有什么难点

我们令$\mathcal{L}=f+\sum^N_{i=1}\eta_ig_i+\sum^M_{i=1}\lambda_ik_i$，而$\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}$在$x=x^*,\eta_i=\eta_i^*,\lambda_i=\lambda_i^*$上取到解值，即$\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}=\mathcal{L}(x^*,\eta^*,\lambda^*)$，同理令$\underset{\lambda,\eta}{\max} \underset{x}{\min} \mathcal{L}=\mathcal{L}(x^+,\eta^+,\lambda^+)$，此外由于$\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}$和原方程等价，因此也可以推出$\underset{x}{\min}f=f(x^*)=\mathcal{L}(x^*,\eta^*,\lambda^*)=\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}$，于是我们对对偶问题$\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}$进行如下推导
$$
\begin{aligned}
\max _{\eta,\lambda} \min_x \mathcal{L}(x,\eta,\lambda) 
&=\min_x \mathcal{L}(x,\eta^+,\lambda^+) \\
& \leq \mathcal{L}\left(x^{*}, \eta^{+}, \lambda^{+}\right) \\
& =f\left(x^{*}\right)+\sum_{i=1}^{N} \eta_{i}^{+} g_{i}+\sum_{i=1}^{M} \lambda_{i}^{+} k_{i}\\
& =f\left(x^{*}\right)+\sum_{i=1}^{M} \lambda_{i}^{+} k_{i}\\
& \leq f\left(x^{*}\right) \\
& =\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}(x,\eta,\lambda) 
\end{aligned}
$$

我们发现其中存在两个小于等于号，由于我们已经假设约束问题满足强对偶关系，有$\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}=\underset{x}{\min} \underset{\eta,\lambda}{\max} \mathcal{L}$，因此其中的两个小于等于号只能取等号，那么我们就可以得到以下两个结论

1. $x^*=x^+$，他们都能让$\mathcal{L}$取到最小值，根据极值点梯度为零，于是我们在等式约束中求解拉格朗日的办法仍旧能用，即$\left.\frac{\partial \mathcal{L}}{\partial x}\right|_{x=x^{*}}=0$，针对$\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}$我们仍能通过求$x$偏导为零的方式求解出最优值
2. $\sum_{i=1}^{M} \lambda_{i}^{+} k_{i}=0$意味着对于$k_i<0$的部分最优值$\lambda_i^+=0$恒成立，这其实在推导不等式约束中转化方程式的等价关系中也提到过，$\lambda$只会在$k=0$的时候取到非零值，进一步的，当$\lambda$取到非零值时$k=0$，$x$一定落在$f,k$的切点上，即问题转化为求解等式约束的问题

整合这么多结论，我们就可以得出最终求解对偶问题可以使用的五个求解条件
$$
\begin{cases}
\underset{\eta,\lambda}{\max} \underset{x}{\min} \mathcal{L}\\
\mathcal{L}=f+\sum^N_{i=1}\eta_ig_i+\sum^{M}_{i=1}\lambda_ik_i\\
\lambda_1\ge 0\\
\vdots\\
\lambda_M\ge 0\\
\end{cases}
\Rightarrow
\begin{cases}
\frac{\partial \mathcal{L}}{\partial x}=0\\
g_i(x)=0,i=1,\dots,N\\
k_j(x)\le0\\
\lambda_j\ge 0\\
\lambda_j k_j(x)=0,j=1,\dots,M
\end{cases}
$$
有了这些结论，在下一节通过实际求解SVM的最优值问题来看看KKT条件是如何运用在实际问题的求解中的

### 硬间隔

接下来我们考虑一个最简单的问题，即平面上有两组点，他们是线性可分的，既然是线性可分的，那么一定会有多条区分线，硬间隔要做的就是找到所有区分线当中最有可能是真实区分线的线

![image-20230311130755058](https://s2.loli.net/2023/03/11/AyX9ZrCoNhGFu6n.png)

如图我们有一堆蓝色和橙色的可线性分类的点，硬间隔假设了默认情况下区分线到两组分类点离这条线最近点的距离都是相同的，比如图中紫色和绿色实线，他们到离他们最近的两个样本点$A,B$的距离都是一样的，在保证距离相同的情况下，这个距离最大值对应的分隔线就是硬间隔所需要求解的最终直线，比如图中由满足要求的紫线和绿线产生的距离$AC>AD$因此我们判断紫线更符合硬间隔分类要求

如何用数学语言表示这么一个模型结构呢，我们不妨一个一个条件对要求进行拆分

1. 我们假设这条直线的方程为$w_1x_1+w_2x_2+b=0$，注意此处考虑了二维样本点，如果令$w=(w_1,w_2),x=(x_1,x_2)$则直线还可以表示为$w^Tx+b=0$，该式子可以表示更高维的样本点情况，同时需要注意向量$w$是直线的法向量（可以尝试把直线挪到原点则$w$和所有$x$点积结果为$0$可证）
2. 由于$A,B$点和这条直线距离相同，因此有$w^Tx_a+b=-c,w^Tx_b+b=c$，两式相减有$w^T(x_b-x_a)=2c$，我们发现该式相当于是直线的法向量和向量$\overrightarrow{AB}$点乘，根据点乘运算规则有$w^T(x_b-x_a)=\left \| AB \right \|\cos\alpha \| w  \|=\left \|  AC\right \| \| w  \|=2c$
3. 根据硬间隔规定条件，我们需要最大化$\left \| AC \right \|$长度，也就是需要最大化$\frac{2c}{\| w  
   \|}$，又因为$\| w  \|=\sqrt{w_1^2+w_2^2}$和把他放在分母都不太好求，且$c$一定是正数，因此问题还可以转化为求解$\frac{\|w\|^2}{4c^2}$的最小值
4. 当然仅有这一条件直接求解也是不行的，由于我们刚刚搞半天都是建立在这条直线能够区分两组分类点的基础上去寻找最好的那条直线，因此我们还需要给他添加一些约束条件，假设我们有$N$个样本$x_i$，他们对应的分类标签是$y_i$，当$y_i=-1$的时候样本点被分在直线下方，$y_i=1$的时候样本点被分在直线上方，且这些点到分隔线的距离必须大于等于间隔距离$c$，则约束条件可以这么写$y_i(w^Tx_i+b)\ge c,i=1,\dots,N$
5. 由于$c$一定是正数，因此我们可以把$c$除到左边来，并令$\frac{w}{c}=w',\frac{b}{c}=b'$，则有$y_i(w'^Tx_i+b')\ge 1$

综上，我们就写出了硬间隔的求解方程，注意由于之前也推导出$\frac{w}{c}$可以看作一个整体，因此下面直接让$w,c$进行了合并为一个参数$w$，$b$同理，另外$\|w\|^2=w^Tw$，在求解函数前加二分之一的系数是为了之后求导的时候好求一点，系数是多少不影响最后求解结果
$$
\begin{cases}
\min\frac{1}{2}w^Tw\\
1-y_i(w^Tx_i+b)\le 0,i=1,\dots,N\\
\end{cases}
$$
然后我们就惊讶的发现这玩意和我们前几节推导了半天的不等式约束下的拉格朗日乘子法完全吻合，更牛逼的是他的求解问题$\|w\|^2$明显是一个凸函数，约束条件$y_i(w^Tx_i+b)$明显是一个仿射函数，意味着这玩意还满足强对偶关系，那KKT条件不就来了吗，心动不如行动，我们直接带入KKT条件看看能不能求解一下
$$
\begin{align*}
&\begin{cases}
\underset{w,b}{\min}\frac{1}{2}w^Tw\\
1-y_i(w^Tx_i+b)\le 0,i=1,\dots,N\\
\end{cases}\\
\Rightarrow&
\begin{cases}
\underset{\lambda}{\max}\underset{w,b}{\min}\mathcal{L}\\
\mathcal{L}=\frac{1}{2}w^Tw+\sum^N_{i=1}\lambda_i\left(1-y_i(w^Tx_i+b)\right)\\
\lambda_i\ge0,i=1,\dots,N
\end{cases}\\
\Rightarrow&
\begin{cases}
\frac{\partial \mathcal{L}}{\partial w}=0\\
\frac{\partial \mathcal{L}}{\partial b}=0\\
1-y_i(w^Tx_i+b)\le 0\\
\lambda_i\ge 0\\
\lambda_i\left(1-y_i(w^Tx_i+b)\right)=0\\
i=1,\dots,N
\end{cases}
\end{align*}
$$

先计算偏导为零的部分
$$
\begin{align*}
\frac{\partial \mathcal{L}}{\partial w}=0&\Rightarrow w-\sum^N_{i=1}\lambda_iy_ix_i=0\\
&\Rightarrow w^*=\sum^N_{i=1}\lambda_iy_ix_i\\
\frac{\partial \mathcal{L}}{\partial b}=0&\Rightarrow\sum^N_{i=1}\lambda_iy_i=0\\
\end{align*}
$$

此处虽然只使用了偏导为零的部分，但是我们需要额外注意一个十分重要的结论，即$\lambda_i\left(1-y_i(w^Tx_i+b)\right)=0$这条式子，由于之前我们证明过了在KKT条件中要么$\lambda=0$要么$k=0$，对于这条式子也是适用的，而当$1-y_i(w^Tx_i+b)=0$的时候，此时我们发现$x_i$这一点刚好落在上图中的虚线上（即垂直距离分割线为$\pm c$的直线上），我们完全可以把其看作$A,B$中的一个，此外所有的点都必然不能使$1-y_i(w^Tx_i+b)=0$，因此他们的$\lambda$都是$0$，同时由于$\sum^N_{i=1}\lambda_iy_i=0$，因此还易得唯二的两个$\lambda\ne0$的点$A,B$的$\lambda$值一定相等

也就是说，**对于所有的样本点来说，除了$A,B$两个距离分割线最近的点之外，其余所有点的$\lambda_i=0$，进一步的，还有$\lambda_a=\lambda_b$，我们称这两个分属于两个类别的特殊样本点为支持向量**

之后我们可以把这两个偏导结果结果带入$\mathcal{L}$
$$
\begin{align*}
&\underset{\lambda}{\max}\underset{w,b}{\min}\mathcal{L}(w,b,\lambda)\\
=&\underset{\lambda}{\max}\mathcal{L}(w^*,b^*,\lambda)\\
=&\underset{\lambda}{\max}\left(\frac{1}{2}\left(\sum^N_{i=1}\lambda_iy_ix_i\right)^T\sum^N_{j=1}\lambda_jy_jx_j+\sum^N_{i=1}\lambda_i-\sum^N_{i=1}\lambda_iy_i\left(\sum^N_{j=1}\lambda_jy_jx_j\right)^Tx+\sum^N_{i=1}\lambda_iy_ib^*\right)\\
=&\underset{\lambda}{\max}\left(\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum^N_{i=1}\lambda_i-\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)\\
=&\underset{\lambda}{\max}\left(\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)\\
\Rightarrow&\begin{cases}
\underset{\lambda}{\max}\left(\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)\\
\sum^N_{i=1}\lambda_iy_i=0
\end{cases}
\end{align*}
$$
化简出来的这条式子是一个仅关于$N$个$\lambda$的等式约束优化问题，你说这我会，再来一次拉格朗日不就完了吗，可以是可以，不过在此处由于样本点可能比较多，导致在求解梯度的时候需要算一堆导数，况且你求一求也会发现这玩意关于一个$\lambda$的偏导结果乱七八糟的，求一个都够呛，要求$N$个并不是很方便

### SMO

因此在此处往往会使用SMO方法来计算最优化的$\lambda$，至于SMO是什么玩意，如果具体来说不只是篇幅不够可能说完了还懂不了，其推导过程确实有点复杂，但是可以简单说一说他的求解过程，整体过程比较类似梯度下降

1. 先把所有的$\lambda$都赋值为$0$，需要格外记住的是每一个$\lambda$对应一个$x$，他们是不可分割的，$\lambda$控制了一个$x$到底是支持向量还是其他划水向量

2. 另外由于之前求偏导为零的时候并不能直接获得由$\lambda$表示的$b$值，因此在SMO的过程中我们还需要顺便优化一下$b$，因此我们还要再赋值一个$b$，随机把他赋一个值就行了（也可以是$0$）

3. 接着就开始循环以下这些步骤，由于一开始我们的$\lambda$一开始都是随意取得，而我们知道KKT条件中已经规定了$\lambda$将要变成的样子了，只是不知道哪个$\lambda$会变而已，于是我们就可以对着KKT条件来看下面的步骤什么时候终止

   - 首先根据KKT条件找支持向量，即找$1-y_i(w^Tx_i+b)=0$的时候，$w^T$我们可以用$w^*$带掉，$b$带不掉就用我们第一步赋值的$b$代，于是我们遍历每一个$x_i$看看前面算出来的$1-y_i(w^Tx_i+b)$哪个离$0$最远
     $$
     \begin{cases}
     g_i=\sum^N_{i=1}\lambda_iy_ix_i^Tx_i+b\\
     \underset{x_i}{\max}|1-g_i|\\
     \end{cases}
     $$

     > 一开始由于所有$\lambda$都是$0$所以任取一个$x$就行了

   - 这样我们就找到了一个$x_1$，但是这还不够，我们还要找第二个$x$，第二个$x$的找法是求解下面式子的最大值，其中计算完的$E$在接下来还需要用
     $$
     \begin{cases}
     E_i=g_i-y_i\\
     \underset{x_i}{\max}|E_1-E_i|
     \end{cases}
     $$

   - 现在我们手握$x_1,x_2$，接下来就可以开始更新他们所属的$\lambda_1,\lambda_2$了，我们让新的$\lambda_1,\lambda_2$值为如下值
     $$
     \begin{cases}
     \eta=x_1^Tx_1+x_2^Tx_2-2x_1^Tx_2\\
     \lambda_1^{new}=\lambda_1+\frac{y_1(E_1-E_2)}{\eta}\\
     \lambda_2^{new}=\lambda_2+y_1y_2(\lambda_1-\lambda_1^{new})
     \end{cases}
     $$
     
     > 需要额外注意在更新$\lambda$的时候和梯度下降不一样的是更新结果是由偏导为$0$求出来的，也就是$\lambda^{new}$这个值一定是在这一次$b$下能够使$x_1,x_2$能满足KKT条件的最优值而不是相对更优值，只不过随着之后$b$的更新他们可能会发生偏差，这也是为什么看上去和梯度下降一样需要反复遍历更新的原因
     
   - 还没结束，接下来我们还要让$b$更新一下
     $$
     \begin{cases}
     b^{new}=-E_{1}-y_{1}x_1^Tx_1\left(\lambda_{1}^{new}-\lambda_{1}\right)-y_{2}x_1^Tx_2\left(\lambda_{2}^{new}-\lambda_{2}\right)+b&\lambda_1^{new}>0\\
     b^{new}=-E_{1}-y_{1}x_1^Tx_2\left(\lambda_{1}^{new}-\lambda_{1}\right)-y_{2}x_2^Tx_2\left(\lambda_{2}^{new}-\lambda_{2}\right)+b&\lambda_2^{new}>0\\
     b^{new}=\frac{1}{2}b^{new}&\lambda_1^{new},\lambda_2^{new}=0
     \end{cases}
     $$
     
     > 在两个新$\lambda$都大于零或都等于零的时候随便选一个算就行了，只不过在都等于零的时候需要取一半

4. 于是一直重复选样本$x_1,x_2$，更新$\lambda$，更新$b$的操作，直到某一时刻找不到合适的样本$x_1$了，也就是遍历$x$之后取到的$|1-g_i|$的最大值已经足够小了，就可以宣布已经完成了SMO的更新

5. 此时我们手中拿着一堆已经完成优化的$\lambda^*$和$b^*$，把其带入$w^*$表达式求解就完了
   $$
   w^*=\sum^N_{i=1}\lambda_iy_ix_i
   $$

6. 终于我们完成了SVM的优化问题，拿到了分割线函数方程$(w^*)^Tx+b^*=0$，接下来我们就可以愉快的带入预测样本点预测结果了，根据其输出的正负性判断类别即可

SMO的过程就像是一个拧螺丝的过程，当一块木板上有一堆螺丝孔的时候，我们往往会先把一个螺丝拧到最紧，接着一个一个拧其他螺丝，然而当我们拧完一圈之后，却发现在拧别的螺丝的时候一开始拧的若干螺丝在这个过程中又松了，于是我们继续取那个最松的螺丝继续拧紧，直到我们在加固了若干次之后感觉所有螺丝都紧了，才决定结束，如果我们可以额外长出一堆手同时拧紧所有螺丝当然只需要拧一次足以，但是面对无法多几只手的现实，一个一个螺丝拧也不失为一种好的选择

### 软间隔

之所以不把软间隔和硬间隔放在一起说，主要还是因为软间隔实际上就是在硬间隔的基础上引入了一个正则项，本质上的流程和硬间隔其实没什么区别，但是由于其中进一步引入了变量$\varepsilon $，导致无论是推导KKT条件还是SMO算法看上去都显得十分复杂，容易看着看着就晕了，因此其推导KKT条件的部分单独拿出来说，在结尾也会提到加入了软间隔之后SMO将会如何变化

在硬间隔中我们提到，我们假设所有的样本点都是完全线性可分的，意味着你用一条线，就一定可以把一类样本分在线的一边，另一堆样本分在另一边，然而大多数时候情况并没有我们想象中那么理想，有些时候样本确实是可分的，但是其边界线却不那么明显，会有个别不听话的样本跑到线的另一边去，软间隔要做的就是针对那些跑到线另一边去的样本一些容忍度，即我允许你跑过去，但是不能跑太多了

![image-20230313192021389](https://s2.loli.net/2023/03/13/DdrSqLnP8eE3gcM.png)

在图中我们可以发现，假设我们选择了$A,B$作为支持向量，此时比如$C$点其实并没有落在支持向量所控制黄虚线的下面，但是软间隔允许这种情况发生，只不过它会根据$C$点和黄虚线的距离$a$给一个惩罚值$\varepsilon $，期望函数可以在惩罚值和支持向量间隔最大值之间找一个平衡

想要求得每一个点和支持向量控制线的距离其实十分简单，利用点到直线距离公式$|y_i(w^Tx_i+b)-c|$即可，只不过这个绝对值可不能乱取，由于我们只需要给没被分好的点一个惩罚，而对于本身就已经分的挺好的比如$D$点惩罚值设为$0$就行了（即没有惩罚），因此最终这个惩罚函数$k$写出来是这样的
$$
\xi_i=\max(0,1-y_i(w^Tx_i+b))\\
k_i=C\xi_i
$$
其中$\xi_i$被称为铰链损失函数（Hinge Loss），而$C$就是其惩罚的程度，和深度学习中的ReLU有点像，只对落在直线上方的项起作用，且对于作用项，输出值和其与直线的偏移程度成正比

有了它，我们就可以改变我们的目标函数和约束条件了，顺便我们还可以直接推出加了惩罚项之后的KKT条件
$$
\begin{align*}
&\begin{cases}
\xi_i =\max\left(0,1-y_i(w^Tx_i+b)\right)\\
\min \frac{1}{2}w^Tw+C\sum^N_{i=1}\xi_i\\
y_i(w^Tx_i+b)\ge1-\xi_i\\
\xi_i\ge0
\end{cases}\\
\Rightarrow
&\begin{cases}
\mathcal{L}={\textstyle{\frac{1}{2}}}w^Tw+C\sum_{i=1}^{N}\mathbf{\xi}_{i}+\sum_{i=1}^{N}\lambda_{i}\left(1-y_{i}\left(w^Tx_i+b\right)+\xi_{i}\right)-\sum^N_{i=1}\mu_i\xi_i\\
\underset{\lambda,\mu}{\max}\underset{w,b,\xi}{\min}\mathcal{L}\\
\lambda\ge0\\
\mu\ge0
\end{cases}\\
\Rightarrow
&\begin{cases}
\frac{\partial \mathcal{L}}{\partial w}=w-\sum^N_{i=1}\lambda_iy_ix_i=0\\
\frac{\partial \mathcal{L}}{\partial b}=\sum^N_{i=1}\lambda_iy_i=0\\
\frac{\partial \mathcal{L}}{\partial\xi_i}=C-\lambda_i-\mu_i=0\\
\lambda_i\left(1-y_i(w^Tx_i+b)+\xi_i\right)=0\\
\mu_i\xi_i=0\\
i=1,\dots,N
\end{cases}
\end{align*}
$$
由于又多了$N$组互补松弛条件$\mu_i\xi_i=0$，因此我们似乎可以拿它来做点事情，我们都知道当$\xi_i>0$的时候样本点一定是越界了，此时根据互补松弛条件则一定有$\mu_i=0$，将其带入$C-\lambda_i-\mu_i=0\Rightarrow\lambda_i=C$，再结合之前已经推导过的松弛互补条件，则一定有
$$
\begin{cases}
x_i分类正确 & y_i(w^Tx_i+b)>1 & \lambda_i=0\\
x_i是支持向量 & y_i(w^Tx_i+b)=1 & 0<\lambda_i<C\\
x_i要被惩罚 & y_i(w^Tx_i+b)<1 & \lambda_i=C
\end{cases}
$$
拿着这玩意，就可以快乐的进行接下来的SMO了，什么你说你不会做？其实非常简单，因为我们发现搞到最后被影响的只有$\lambda$的范围而已，在进行$\lambda$更新的时候要注意其值不能超过$C$，即当$\lambda^{new}>C$的时候让其$=C$就完了，然后在选$x$的时候也不能无脑取最大，优先选择$0<\lambda_i<C$的$x_i$就行了，其余的包括$b$的更新，循环过程其实都是一样的，此处就不详细提了

### 核函数

我们一直都在假设所有数据点至少是线性可分的，然而实际当中大多数情况样本点其实并不线性可分，这么一来难道之前推了半天的式子都没用吗？当然不是，我们来看我们当时用来求解$\lambda$的式子
$$
\underset{\lambda}{\max}\left(\sum^N_{i=1}\lambda_i-\frac{1}{2}\sum^N_{i=1}\sum^N_{j=1}\lambda_i\lambda_jy_iy_j{\color{red}\boldsymbol{x_i^Tx_j}}\right)\\
$$
当时我们只关注到了这个式子是一个仅与$\lambda$有关的式子，但是我们没有注意到在在求解$\lambda$的过程中我们其实并不需要完全知道每一个样本点的向量长什么样，而是只需要他们互相之间的点乘结果！

你可能不知道这意味着什么，这意味着SVM的复杂度和样本维度无关！在前一节介绍SMO的时候我们就会发现，SVM求解模型时的大部分时间都拿去求解$\lambda$了，相对应的，在求解$\lambda$的过程中我们发现$x_i^Tx_j$永远都是绑定在一起的，也就是说，在这个过程中我们只需要一开始求解出每一个样本相互之间的点乘结果即可，之后直接拿着这个点乘结果去用，除了最后求解$w^*$的时候需要管一下$x$长什么样，其余时间真实样本长什么样根本就不重要！

既然样本维度和时间复杂度没什么关系，那么原本线性不可分的样本点就有救了，因为我们虽然不能像神经网络一样直接把非线性映射后的结果代替原有样本$x$，但是我们可以把这个结果拼接在$x$后面，反正$x$到后面都是点积出来一个值，哪怕我给他增加一亿维只要能算点积那对于求解就不是问题

![](https://www.pianshen.com/images/203/bc90b628d86cf6b885b51c2c3d3b0ce3.png)

更进一步，甚至我都可以不用知道对$x$增加维度之后$x$究竟长什么样，如果我们可以直接获得在某种增维方式之后$x'$的点乘结果和原有的$x$之间的关系，即有一个函数$\mathcal{K}(x_1,x_2)=(x'_1)^Tx'_2$，那么我们就可以跳过求解$x'$的过程，直接把$x$丢进这个函数$\mathcal{K}$里取他的输出就行了

**这种直接跳过升维操作直接以原维度样本作为参数进行计算获取点乘结果的函数就叫做核函数**

事实上这种函数也是容易找到的，我们先来看看点乘在数学运算中到底代表了什么，我们令两个向量$x=(x_1,x_2),y=(y_1,y_2)$，则$x^Ty=xy^T=x_1y_1+x_2y_2$这非常简单，然后我们要对$x,y$进行一些神奇的变化，我令$x'=(x_1^2,\sqrt2x_1^2x_2^2,x_2^2),y'=(y_1^2,\sqrt2y_1^2y_2^2,y_2^2)$，这样我们等于说就给原本二维的$x,y$升了一维，此时我们可以尝试算一算他们的点乘
$$
\begin{align*}
(x')^Ty'&=x_1^2y_1^2+2x_1^2x_2^2y_1^2y_2^2+x_2^2y_2^2\\
&=(x_1y_1+x_2y_2)^2\\
&=(x^Ty)^2\\
&=\mathcal{K}(x,y)
\end{align*}
$$
我们发现，原来我们只需要简单的给原样本的点乘结果加个平方就等于实现了一次升维，事实上容易证明你给他加多少次他实际上就升维了多少次，这种核函数叫做多项式核，这种核听起来挺美好，但是由于你还是得手动指定升到多少维因此仍有局限性

令一个著名的核函数叫做高斯核，他长这样
$$
\mathcal{K}(x,y)=e^{\left(-\frac{\left \|x-y\right \|^{2}}{2\sigma^{2}}\right)}
$$
我们从他的指数部分化简，且中间使用指数形式的泰勒展开，同时为了计算方便我们令$\sigma^2=1$（在实际情况中也是作为超参数进行调整）
$$
\begin{align*}
-\frac{\left \|x-y\right \|^{2}}{2\sigma^{2}}&=\frac{1}{2\sigma^2}(-x_{1}^{2}+2x_{1}y_{1}-y_{1}^{2}-x_{2}^{2}+2x_{2}y_{2}-y_{2}^{2})\\
&=\frac{1}{2\sigma^2}(-x^2-y^2+2x^Ty)\\
&=-\frac{x^2+y^2}{2}+x^Ty\\
\Rightarrow e^{\left(-\frac{\left \|x-y\right \|^{2}}{2\sigma^{2}}\right)}
&=e^{-\frac{1}{2}\left(x^2+y^{2}\right)}e^{x^Ty}\\
e^{x}=\sum_{n=0}^{\infty}{\frac{x^{n}}{n!}}&\Rightarrow
e^{-\frac{1}{2}\left(x^2+y^{2}\right)}\sum_{n=0}^{\infty}{\frac{\color{red}(x^Ty)^{n}}{n!}}
\end{align*}
$$
我们发现，高斯核在结合了泰勒展开之后展开的结果居然是一个趋向于无限次累加的多项式核（红色标注处），并且为了防止加的过多导致数值过大，他还贴心的为每一个多项式结果都加了一个阶乘级别的权值，使得次数越低的升维结果占的比重越大

从另一个角度来说，点乘也可以看作是衡量两个向量之间相似度的标准，**因此高斯核的计算结果也可以看作是两条向量在通过多项式核升到无限维之后相互之间的相似程度**

一般上我们会令$\frac{1}{2\sigma^2}=\gamma$作为更简单的超参数，并且由于其也是指数级别的，我们可以通过调整$\gamma$来控制模型的精度，比如当我增大$\gamma$，会发现原本看上去相似度还可以的两条向量的相似度急剧下降，反映到最终的模型上，就会让高维上的两组点更不容易区分（因为可能同类点之间的相似度也不高），导致最后得出的$w^*$维度过高，于是可能会出现过拟合的问题，相反，过小的$\gamma$会让高维中的点非常容易被区分，训练好的模型容易出现欠拟合

### SVM vs MLP

这是有史以来最长的一章，也体现出了SVM的重要性，接下来我们通过比较他和传统多层感知机模型，对本章进行总结，同时也可以进一步发现SVM的优劣势

- SVM理论上只能解决二分类问题，而MLP可以控制输出神经元的多少通过添加softmax来实现多分类
- SVM的求解过程使用了SMO，本质上还是通过偏导为零的方式即求部分参数解析解来进行最优化，而MLP则采用梯度下降逼近极值而非直接求解
- SVM的时间复杂度和样本数量正相关，但是与样本维度基本无关，且理论上无法减少必需的优化参数量，而MLP的时间复杂度其实与样本数量和样本维度都有关，但是可以通过增减神经元数量控制复杂度
- 因此SVM更适合被用于小型训练集且样本维数较多的场景，而MLP对小型训练集由于在不降维的情况下输入层参数永远与样本维度相同，因此十分容易产生过拟合
- SVM需要调整的超参数是核函数中的$\gamma$和软间隔中引入的$C$，而传统MLP中只需要调整梯度下降学习率$\epsilon$即可
