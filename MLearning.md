# Machine Learning

学深度学习学上瘾了，于是打算直接梭哈机器学习

> 虽然顺序有点不对，但是在看这个之前最好看过深度学习那个笔记不然可能看不懂

## 高斯分布

### 极大似然估计（MLE）

概率就是抛硬币之前我们知道加入我们抛出去一次有50%概率为正面，也就是我们知道一个模型的所有信息去预测可能的结果

而似然则是我们拿到了一万次抛硬币五千次正面的结果得到正面出现的可能是50%，也就是根据结果去重构模型的参数

他们的关系大概是**已知样本可以通过似然推导出模型参数，已知参数的模型可以通过概率预测出未知样本**

在机器学习中的高斯模型和极大似然估计就是在重复这个过程，我们会拿到我们也不知道从哪里随机抽取出来的一堆已知样本，然后我们假设这么多样本是从一个高斯模型当中抽取出来的，之后根据这些样本通过极大似然估计重构高斯模型，再用高斯模型去预测未知内容的概率

比如我现在收集了一万个人的身高信息，第$i$个人的身高记为$x_i$，即比如$x_1=180$表示第一个人身高180cm，此时我们根据这一堆$x$似然出一个模型，之后如果有需要模拟生成一万个人的身高，我们就可以用这个模型捏造一堆符合分布的样本

接下来的问题就是如何通过定量的样本求得模型的参数了，比起抛硬币，人的身高实际上是连续分布的，因此便有了概率密度函数$f$，需要注意的是概率密度函数的值并不是该点可能的概率（因为对于连续分布的每一点来说概率都是无限小），但是我们可以用这个值来表示该点和其他点相比被筛选出来的难易程度，因此我们有了极大似然估计求解的四步骤

1. 写出带未知参数的概率密度函数，对于高斯分布，他是 $f(x) = \frac{1}{\sqrt{2\pi }\sigma }e^{(-\frac{(x-\mu)^{2}}{2\sigma^{2}})}$

2. 把已知的所有样本$x$带入这个式子，表示每一点被筛选出来的比较概率（并不是真实概率）

3. 把这么多的带参数的$f$乘在一起加了$log$之后构造出似然函数（之后也可叫做损失函数）

   >至于为什么要相乘，我知道你很急但先别急，此处埋下一个伏笔，目前你只要知道似然函数就是把概率相乘取$log$即可

4. 对损失函数关于$\mu$和$\sigma^2$求偏导之后求解导数为零时的极大值$\hat{\mu},\hat{\sigma}^2$


之后我们可以得到通解$\mu_{MLE}=\bar x=\frac{1}{N}\sum^{N}_{i=1}x_i,\sigma^2_{MLE}=\frac{1}{N}\sum^{N}_{i=1}(x_i-\mu)^2$，不过用极大似然估计法推导出来的这个方差计算结果实际是偏小的（也叫有偏估计），真实的无偏估计方差应该是$\sigma^2=\frac{1}{N-1}\sum^{N}_{i=1}(x_i-\mu)^2$关于有偏和无偏估计是什么玩意此处先不提，只需要知道这个极大似然估计的方法并不是完美无瑕的就行

### 多维高斯分布

如果我们收集了不止一个身高信息，还有体重信息，此时求解的高斯分布也不再是一个维度了，就需要使用多维高斯分布模型，他的公式如下
$$
\mathcal{N}(\mu, \Sigma)=\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)}
$$
其中$x,\mu$都是向量，$\Sigma$是一个协方差矩阵（也就是半正定矩阵），$D$是$x$的维度，指数部分$\Delta=(x-\mu)^{T} \Sigma^{-1}(x-\mu)$我们可以将其看作是一组同心椭圆或者是等高线，椭圆上的每一个点带进去会有相同的结果，也可以把他看作是一个距离表达式，只不过此时的距离相等量不再是一个圆到圆心的距离而是椭圆，这被称为马氏距离，当$\Sigma$为单位矩阵的时候$\Delta=(x-\mu)^2$也就是我们熟知的欧氏距离，$\Sigma$控制了同心椭圆的原点位置和旋转角度

### 高斯分布概率定理

大部分高斯分布的推论都是以下这个式子推出来的，他表示所有的高斯分布经过了线性变换之后一定还是一个高斯分布
$$
x \sim \mathcal{N}(\mu, \Sigma), y \sim A x+b \Rightarrow y \sim \mathcal{N}\left(A \mu+b, A \Sigma A^{T}\right)
$$
之后我们如果令
$$
x=\begin{pmatrix}
x_a \\
x_b
\end{pmatrix},\mu=\begin{pmatrix}
\mu_a \\
\mu_b
\end{pmatrix}, \Sigma=\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}
$$
其中$x_a,x_b$是对向量$x$进行分块，原本$x,\mu$长度为$p$则$a+b=p$，则我们可以将$x$看作是$x_a$和$x_b$的联合分布概率，现在我们的需求是根据联合分布概率去求解边缘概率分布和条件概率分布，在此之前我们引入一条式子

首先我们需要搞清楚，我们求解的是分布情况，而我们又知道对于高斯分布的线性变换一定是高斯分布，因此我们的最终目标一定是求解边缘概率分布和条件概率分布的$\mu$和$\Sigma$两个值即可

1. 边缘概率分布$p(x_a)$其实非常好求，因为我们可以将$x_a$写作$x_a=\begin{pmatrix}\mathbb{I}&\mathbb{O}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}=Ax+b$，接着照着上面的式子，容易得到$x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$

2. 条件概率分布$p(x_b|x_a)$则需要花一点功夫，我们需要先引入一条式子
   $$
   x'=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a
   $$
   需要注意这条式子可以说没有任何意义，他只是为了辅助得到条件概率的工具罢了，同时之后如果要求解$p(x_a|x_b)$则需要把这几条式子的$a,b$都进行对换，然后我们可以得到
   $$
   x'=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbb{I}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}=Ax+b
   \\ \mu'=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\\Sigma'=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
   $$
   然后我们如果把第一条式子变一变，即可得到$x_b$的求解式
   $$
   x_b=\mathbb{I}\times x'+\Sigma_{ba}\Sigma_{aa}^{-1}x_a=Ax+b\\
   \mathbb{E}[x_b]=\mu'+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
   \mathbb{D}[x_b]=\Sigma'\\
   x_b\sim\mathcal{N}(\mathbb{E}[x_b],\mathbb{D}[x_b])
   $$
   由于这个$x_b$的$\mu$和$\Sigma$是由$x_a$控制的，因此其实这个$p(x_b)$正是等于$p(x_b|x_a)$，于是我们得到了
   $$
   x_b|x_a\sim\mathcal{N}(\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a+\Sigma_{ba}\Sigma_{aa}^{-1}x_a,\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab})
   $$

知道了如何用联合概率求解边缘概率和条件概率，我们不妨反过来再看看如何用条件概率和边缘概率求解联合概率

现在我们已知$x\sim\mathcal{N}(\mu,\Lambda^{-1}),y|x\sim\mathcal{N}(Ax+b,L^{-1}),y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})$，需要求解$p(y),p(\begin{pmatrix}x\\y\end{pmatrix})$

1. $p(y)$根据公式可以直接算出$y\sim \mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)$，计算过程中注意$\epsilon$和$x$相互独立因此均值和方差加法计算均可以分解后独立计算

2. 根据上面结论其实易得
   $$
   \mathbb{E}[\begin{pmatrix}x\\y\end{pmatrix}]=\begin{pmatrix}\mu\\A\mu+b\end{pmatrix}\\
   \mathbb{D}[\begin{pmatrix}x\\y\end{pmatrix}]=\begin{pmatrix}\Lambda^{-1} & Cov(x,y)\\Cov(y,x) &A\mu+b\end{pmatrix}
   $$
   由于$Cov(x,y)=Cov(y,x)$因此我们只需要求一个即可，根据协方差公式$Cov(x,y)=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])]$易得$Cov(x,y)=\Lambda^{-1}A^T=A\Lambda^{-1}$带入即可得$\begin{pmatrix}x\\y\end{pmatrix}\sim(\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1} & \Lambda^{-1}A^T\\A\Lambda^{-1} &A\mu+b\end{pmatrix})$

之后如果想求$p(x|y)$则可以根据上面联合概率求条件概率的方式求解，此处不提

## 线性回归

这一节的大部分内容在深度学习中都有提到过因此此处更注重于数学角度的分析

### 最小二乘法（LSE）

最小指的就是最小化损失函数，二乘指的就是均方误差，只是把这个过程换了一个名字罢了，以下是这个过程中的一些注意点

- $L(W)=\sum_{i=1}^{N}\left\|W^{T} x_{i}-y_{i}\right\|^{2}$，其中我们可以在$x_i$中增加一个$x_{i0}=1,w_0=b$实现线性关系中的$b$偏置
- 让$X=(x_1,x_2\dots x_N)^T$，则$L(W)=(W^TX^T-Y^T)(XW-T)$，$W$和$Y$都比$X$少一维，只有神经元的数量能控制$W,Y$第二维的大小
- $\frac{\partial L(W)}{\partial W}=2 X^{T} X W-2 X^{T} Y=0$可解得$W$的解析解$\hat{W}=(X^TX)^{-1}X^TY$，其中$(X^TX)^{-1}X^T$是$X$的左逆（或者是伪逆），这是用求导方式最小化损失函数
- 从另一种角度看$L(W)$还可以看作是在高维空间中找出一个可以垂直于指定向量$y$的超平面，即最小化向量$y$在$X$向量组构成超平面中的投影，从这个角度来说，我们应该让$x$的数量$N$大于等于维度$p$，保证在高维中可以形成平面（类比于我们无法拿一条二维向量在三维空间构成一个平面，至少得有两条线才能形成平面）
- 我们假设现在有一群数据点我们知道他满足$y=f(W)+\epsilon,\epsilon\sim \mathcal{N}(0,\sigma^2)$这个模型，它让产生的数据在高斯噪声的作用下落在实际直线两侧，由于我们知道了模型公式和样本，便可以使用最大似然估计求解参数$W$，我们会发现求解的结果关于$W$需要最小化的对象与最小二乘法的损失函数完全一致，这也能说明**最小二乘法隐含了样本噪声服从高斯分布的假设**

### 正则化

我们之前说根据求导结果$\hat{W}$具有解析解$(X^TX)^{-1}X^TY$但是一方面$X^TX$并不一定可逆，另一方面当样本数量少于数据特征的情况下容易出现过拟合，一种解决方式就是正则化

正则化的统一框架为$L'(W)=L(W)+\lambda P(W)$，当$P(W)=\left|  \right | W\left |  \right | ^2$的时候就是著名的Weight Decay（也叫L2正则化，Ridge正则化，岭回归，权重衰减等），此时如果再次计算解析解会发现结果变成了$\hat{W}=(X^TX+\lambda I)^{-1}X^TY$，由于给$X^TX$加了正则项保证其可逆性，同时从损失函数的角度也降低了过拟合风险

### 最大后验估计（MAP）

假设现在同样是求解抛硬币的概率模型，同样是抛了十次，只不过这回我们抛出了十次正面零次反面，如果这回我们什么也不管拿极大似然估计做，不出意外我们会得出正面概率百分百反面概率零的极端模型参数，但是我们常识知道，只能抛出正面的硬币似乎并不是那么常见，我们更希望这种极端模型的结果能够出现的少一些，这种样本过少造成的偶然事件导致的模型偏差是不是看上去很像是过拟合会造成的结果？在上一节当中我们讨论了传统意义上的正则化缓解模型过拟合的方法，而现在我们不妨从另一个角度——先验概率的角度重新审视一下正则化的数学推导

首先我们要把最大后验估计的计算公式列出来，他长这样
$$
\hat{\theta} = \arg\underset{\theta}{\max} P(\theta\mid X)
$$
其中$X$就是我们抛了十次的结果，$\theta$就是将要求解的正面的概率，整个式子用大白话说就是当我们抛出十次正面零次反面的情况下求解可以让出现这个情况的概率最大化的模型参数，这么一听怎么和极大似然估计没什么区别呢，确实，在介绍极大似然估计的时候为了方便理解我也说的是在已知样本的情况下求解参数，然而当时我也埋下了一个伏笔，相信单看似然函数你可能就十分奇怪为什么把样本代到模型里乘一乘就可以求解模型参数了，完全搞不懂是什么原理，实际上我换个问法你可能就明白了

已知当前抛硬币正面朝上的概率是60%，求解抛三次出现正正反情况的概率

理所当然的，因为每次抛硬币的操作都是独立的，我们只需要简单的$0.6*0.6*0.4$就可以计算出正正反的概率为$0.144$，你发现了，在这个问题中使用乘法显得那么理所当然，于是我们知道了，似然函数实际上根本就不是$P(\theta\mid X)$的函数式，而是$P(X\mid \theta)$（已知模型参数求解某一序列样本发生的概率）的函数式

这么说来难道说我们学了半天的极大似然估计本身就是错的吗，这可不一定，我们不妨来寻找一下$P(\theta\mid X)$和$P(X\mid \theta)$有什么关系，这时候就需要掏出伟大的贝叶斯公式了
$$
P(\theta\mid X)=\frac{P(X\mid \theta)P(\theta)}{P(X)}
$$
对于这个式子，在我们拿到了某次样本之后实际上$P(X)$由于只着眼于这一次的样本因此其值恒为常数1可略去，于是我们知道了，实际上$P(\theta\mid X)$和$P(X\mid \theta)$的差距仅仅在$P(\theta)$身上，而这个$P(\theta)$实际上就是大名鼎鼎的先验概率，当其为常函数时，在求解$\theta$最大值的时候$P(\theta\mid X)$和$P(X\mid\theta)$相等，最大后验估计也就变成极大似然估计了，对应到实际问题中就是每一种概率情况能取到的概率都是一样的，引申到抛硬币问题就是正面朝上的最终概率是50%和100%的情况都是等可能的，不存在一个人的理性思维觉得硬币正反面概率分别是50%的可能大些的情况（也就是没人事先看过硬币有一个大致的判断）

整合了贝叶斯公式之后，我们需要求解的模型参数也变成了这样
$$
\hat{\theta} = \underset{\theta}{\operatorname{argmax}} P(X\mid\theta) P(\theta)
$$
但是如果我们事先看了一眼硬币，发现这个硬币好像挺均匀的，发生投出去十次全是正面的概率直觉感觉就并不高，此时我们就可以给$P(\theta)$求解参数也定一个模型比如我们认为他也服从高斯模型$P(\theta)=\mathcal{N}(0.5,\sigma^2)$，结合高斯分布的图像，表示我认为正面出现概率为$0.5$的概率最高，其余概率随和$0.5$的距离增大而下降

为了计算方便，我还是令$P(\theta)=\mathcal{N}(0,\sigma^2)$，同时为了让结果直观一点，我再令上面的$\theta=W,X=y$，然后我们计算一下最大后验概率的化简结果
$$
\begin{aligned}
\hat{W} & =\underset{W}{\operatorname{argmax}} \log (P(y \mid W) P(W)) \\
& =\underset{W}{\operatorname{argmax}} \log \left(\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(y-W^{T} X\right)^{2}}{2 \sigma^{2}}} \frac{1}{\sqrt{2 \pi} \sigma_{0}} e^{-\frac{\|W\|^{2}}{2 \sigma_{0}^{2}}} \right)\\
& =\underset{W}{\operatorname{argmax}}\left(-\frac{\left(y-W^{T} X\right)^{2}}{2 \sigma^{2}}-\frac{\|W\|^{2}}{2 \sigma_{0}^{2}}\right) \\
& =\underset{W}{\operatorname{argmin}}\left( \frac{\left(y-W^{T} X\right)^{2}}{2 \sigma^{2}}+\frac{\|W\|^{2}}{2 \sigma_{0}^{2}}\right) \\
& =\underset{W}{\operatorname{argmin}}\left( \sum_{i=1}^{N}\left(y_{i}-W^{T} x_{i}\right)^{2}+\frac{\sigma^{2}}{\sigma_{0}^{2}}\|W\|^{2}\right)
\end{aligned}
$$
你没看错，化简结果就是Weight Decay，我们加上的先验概率反映到结果上竟就是给模型加上了一个正则化的条件

综上所述，我们总结一下，**LSE是模型噪声服从高斯分布的MLE，MLE是先验概率服从等概率分布的MAP，Weight Decay正则化方法则是先验概率服从和样本同方差高斯分布的MAP**

## 线性分类

在线性回归问题中，我们画了一条线用于拟合一堆样本点从而达到预测未知样本的目标，线性分类则是要画一条线去分割两堆不同的样本点，因为都是画线，因此笼统来说线性分类可以看作是线性回归的一个变形，只不过如果纯拿线性回归的方法来处理线性分类的任务效果并不佳，因此线性分类问题需要解决的就是如何画出这一条线的问题

于是我们需要回忆一下在线性回归问题中我们是怎么画出拟合线的，当时我们假设数据点将会围绕拟合直线模型呈高斯分布，之后推出了使用最小二乘法构造损失函数，求解损失函数的最小值获得了拟合线方程（即$w$参数），如今换到了线性分类问题，我们是否也可以通过某种假设构造出一个损失函数求解分类线方程参数呢
$$
\text{线性回归} \xrightarrow{激活函数} \text{线性分类}
$$
其实从道理上来说，线性分类的最终形态一定是逻辑回归，然而逻辑回归可不是从天上掉下来的，如果现在直接把一个sigmoid函数丢给你相信你一定懵的一批，要知道sigmoid为什么能够起作用，我们就得跟随着前人的脚步，一步步学习逻辑回归的演化史

首先我们得先定义几个分类问题中用得上的参数，通过几个参数也能够更直观的理解分类问题的解决内容

有$N$个在$p$维空间内的点集合$X=\{x_1,x_2\dots x_n\}$，其中$x_i$表示第$i$个数据点，是一个$p$长度的列向量，与每一个$x_i$匹配的是$N$个$y_i$集合$Y$，由于此处我们只考虑二分类问题，因此$y_i$不仅是一个标量而且仅能取到$0$或$1$，即$y_i \in \{0,1\}$，$(x_i,y_i)$表示第$i$个$x$和$y$的对应关系，同时由于我们定死了用一条线来分类的要求，因此只需要求解一个参数$w$，令某函数模型$f(w^TX)=Y$即可实现分类目标，其中$w$维度为$p\times 1$，也是一个向量，为了找到这个$w$，还需要有一个评价函数或者说是损失函数$L(f)$，通过最小化损失函数来找到最优参数$w$

一下子看到那么乱七八糟的参数一定十分摸不着头脑，接下来不妨用一个最傻瓜的解决分类问题的思路来看看这些参数在实际问题中大概能怎么用

### 感知机算法（PLA）

虽然我们之前一直说画线什么的，但是那只是在二维平面上便于理解的说辞罢了，实际上由于样本点的特征是多维的，因此无论是线性回归还是分类，所得到的所谓直线其实是高维空间下的一个超平面，想要用一个超平面分割两组数据点，其实就是让平面上方的点赋值$1$，下方的赋值$0$，用数学语言就是$f(x)=\mathrm{sign}(w^Tx)$，其中$\mathrm{sign}$叫符号函数同时也是激活函数，当$w^Tx>0$的时候输出$1$，不然输出$0$，我们假设现在已经找到了这么一条完美的平面$w^Tx$，这时候无论遇到什么未知样本点$x_i$，我们只需要丢给$f$，他的输出就可以直接作为我们的$y_i$，通过这个函数我们也能够理解为什么给线性回归问题套一个激活函数就可以实现分类的依据了，这个分类函数是完美的，但是不完美的地方就在于这个平面我们求不出来，由于符号函数是分段函数，意味着我们无法通过无脑带入连乘的似然函数之后求导求解解析最优解$w$，这时候我们就需要另外找一个损失函数，我们构造这样的函数，通过最小化函数结果寻找最佳值$w$
$$
L(w)=\sum_{(x_i,y_i)}-(y_i-0.5)w^Tx_i
$$
含义其实也很简单，由于需要秉承着符号函数的意志，因此这个损失函数做的就是想尽可能让$y_i-0.5$和$w^Tx_i$同号，这样从另一个角度同样也满足了类似符号函数的定义，同时该函数可导使得我们可以通过梯度下降的方法接近最小值点，至于为什么不通过导数为0求解最小值，你试试这玩意求导完是一个不含$w$的东西，说明在一组样本下他是没有最小值的，我们想一下也知道只要我$w$​向梯度方向增的越多，当然损失函数也会越大，因此这时候就需要我们多找几组样本进行重复实验

显然，这种所谓的感知机算法是有局限性的，不过他的局限性不在于激活函数$f$构造的不好，事实上这个$f$正是绝对理想情况下的理想结果，他的问题在于损失函数$L$构造的过于刻意了以至于效果也肉眼可见的不佳，但是他提供的梯度下降方法和理想情况下的符号函数作为区分函数给了后面研究者一个好的思路，为包括深度学习在内的复杂模型不易求参数解析解的情况下使用梯度下降方法逼近极值提供了解决思路

### 矩阵求导

上一节说是说让你试试求导，但是相信大多数人对于矩阵的求导还是一脸懵逼的，于是我们得花一点时间来说一说矩阵求导法则

首先我们得知道什么矩阵可以被求导，我们知道一个求导首先需要一个函数和自变量，大概写成这个样子$\frac{\partial f}{\partial x}$

$x$是一个矩阵相信应该没什么问题，主要就是$f$会长什么样，同样是传入一个矩阵，常见的$f$可以分为两个大种类，分别是全元素标量函数和逐元素标量函数，其中全元素标量函数传入一个矩阵，函数会提取出其中的所有元素进行计算得到一个标量值，比如$f(X)=a^TXb$，逐元素标量函数同样是传入一个矩阵，函数会对其中每一个元素进行同样的操作得到一个矩阵，比如$f(X)=\sin(X)=[\sin (X_{ij})]$

接下来就可以看一些直接可用的求导公式了，我们令$x_i$为标量，$x$为列向量，$X$为矩阵，$\mathrm {tr}  (X)$表示计算$X$的迹（矩阵对角元素相加和），$f$为全元素标量函数，$\sigma$表示逐元素标量函数，$\mathrm dX$表示$X$的全微分，$\odot$表示逐元素相乘，和$X$形状一致，基于这几个元素，有以下几个恒成立公式
$$
\begin{align}
x_i &= \mathrm {tr}(x_i)\tag1\\
\mathrm {tr}(x_1A+x_2B) &= x_1\mathrm {tr}(A)+x_2\mathrm {tr}(B)\tag2\\
\mathrm {tr}(X)&=\mathrm {tr}(X^T)\tag3\\
\mathrm {tr}(ABC)&=\mathrm {tr}(CAB)=\mathrm {tr}(BCA)\tag4\\
\mathrm {d}(AB)&=\mathrm {d}AB+A\mathrm {d}B\tag5\\
\mathrm {d}(X^T)&=(\mathrm {d}X)^T\tag6\\
\mathrm {d}f(X)&=\mathrm {tr}(\frac{\partial f(X)}{\partial X^T}\mathrm {d}X)\tag7\\
\frac{\partial f(X)}{\partial X^T}&=(\frac{\partial f(X)}{\partial X})^T\tag8\\
\mathrm {d}(AXB)&=A\mathrm {d}XB\tag9\\
\mathrm {d}|X|&=|X|\mathrm {tr}(X^{-1}\mathrm {d}X)\tag{10}\\
\mathrm {d}X^{-1}&=-X^{-1}\mathrm {d}XX^{-1}\tag{11}\\
\mathrm {d}(A\odot B)&=\mathrm {d}A\odot B+A\odot \mathrm {d}B\tag{12}\\
\mathrm {d}\sigma(X)&=\sigma'(X)\odot \mathrm {d}X\tag{13}\\
A^T(B\odot C)&=(A\odot B)^TC\tag{14}
\end{align}
$$

根据这些公式，我们不妨来求一求$f=a^TXX^Tb$关于$X$的导数

首先确认$f$是一个全元素标量函数，因为如果$X$的形状为$m\times m$则$a,b$形状一定是$m\times 1$的，最后结果是$1\times 1$的矩阵，即为标量，然后确认我们需要求解的结果是$\frac{\partial f(X)}{\partial X}$也是一个标量，之后开始求解
$$
\begin{align*}
(1)&\Rightarrow& \mathrm{d} f(X)&=\mathrm {tr}(\mathrm {d}f(X))=\mathrm {tr}(\mathrm {d}(a^TXX^Tb))\\
(9)&\Rightarrow&&=\mathrm {tr}(a^T\mathrm {d}(XX^T)b)\\
(5)&\Rightarrow&&=\mathrm {tr}(a^T(\mathrm {d}XX^T+X\mathrm {d}X^T)b)\\
(2)&\Rightarrow&&=\mathrm {tr}(a^T\mathrm {d}(X)X^Tb)+\mathrm {tr}(a^TX\mathrm {d}(X^T)b)\\
(6)&\Rightarrow&&=\mathrm {tr}(a^T\mathrm {d}(X)X^Tb)+\mathrm {tr}(a^TX(\mathrm {d}X)^Tb)\\
(4)&\Rightarrow&&=\mathrm {tr}(X^Tba^T\mathrm {d}X)+\mathrm {tr}((\mathrm {d}X)^Tba^TX)\\
(3)&\Rightarrow&&=\mathrm {tr}(X^Tba^T\mathrm {d}X)+\mathrm {tr}(X^Tab^T\mathrm {d}X)\\
(2)&\Rightarrow&&=\mathrm {tr}((X^Tba^T+X^Tab^T)\mathrm {d}X)\\
(7)&\Rightarrow& \frac{\partial f(X)}{\partial X^T}&=X^Tba^T+X^Tab^T\\
(8)&\Rightarrow&\frac{\partial f(X)}{\partial X}&=(\frac{\partial f(X)}{\partial X^T})^T=ab^TX+ba^TX
\end{align*}
$$
我们发现了，利用迹求解矩阵导数的要点就是通过取标量函数全微分的迹，通过分离$\mathrm dX$，构造出形如$(7)$一致的全微分和导数转换的形式，便可容易求得矩阵的导数结果

### 线性判别分析（LDA）

既然直接用符号函数区分样本无法求解超平面，研究者们开始了新一轮的尝试，这次他们决定直接跳过构造激活函数，用一种名为降维的方法，直接从损失函数的角度下手解决分类问题

这种方法的思想也十分简单，在上一节中我们是要找一个超平面（在样本点为二维时则为直线）分割两组数据点，这次我们不再拘泥于这个平面，而是实实在在的构造一条新的过原点的直线（注意这回无论是高维还是低维都是直线），然后我们让所有的数据点全部投影到这条直线上，接着我们只关注两组数据点在这条直线上的投影点，然后改变直线的参数$w$即斜率，看看哪条直线对应的投影点能够将两组数据点分得更开，我们就使用这条直线

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201121151154839.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0NzY2ODgz,size_16,color_FFFFFF,t_70#pic_center)

> 空间内相同的两组红蓝数据点，明显第二条的投影点更能够区分数据，此处只画了二维情况，高维同理

这条过原点的直线当然不是为了分割数据点，但是这种另辟蹊径的方法居然看上去还挺有效果（至少在这个二维图上），而想要求解这条投影直线，我们只需要解决两个问题，一是如何找到投影点，二是如何评价投影点的分布

对于投影点实际上非常简单，我们用幼儿园就学过的向量点积即可解决，我们令所求投影直线方向上的单位向量为$w$，接着取每一个样本点向量$x_i$与其做点乘$w^Tx_i$，向量的点乘结果即为样本点向量在这条直线上的投影距离，假如我们取这条投影直线为坐标轴，则投影距离即可转换为投影坐标，$w^TX$将会产生一条向量，其中每一维度的值就是一个样本点在直线上的投影，于是我们可以构造出一个映射$(w^Tx_i,y_i)$，表示这条向量的每一个值对应的类别$y_i$

![img](https://s2.loli.net/2023/02/15/5BZFm2QlXT1Dn4b.png)

如何评价这条直线的好坏呢，我们挑出映射当中所有$y_i=0$的点$w^Tx_i$（例如上图的红点）组成一个向量，记为$u_0$，同时记其中有$N_0$个元素，同理挑出另一个向量（蓝色的点）组成$u_1$，其中有$N_1$个元素，接着我们构造损失函数如下
$$
L(w)=\frac{(\bar{u}_0-\bar{u}_1)^2}{\Sigma_{u_0}+\Sigma_{u_1}}\\
\bar{u}_0=\frac{1}{N_0}\sum^{N_0}_{i=1}u_0,\bar{u}_1=\frac{1}{N_1}\sum^{N_1}_{i=1}u_1\\
\Sigma_{u_0}=\frac{1}{N_0}\sum^{N_0}_{i=1}(u_{0_i}-\bar{u}_0)(u_{0_i}-\bar{u}_0)^T\\
\Sigma_{u_1}=\frac{1}{N_1}\sum^{N_1}_{i=1}(u_{1_i}-\bar{u}_1)(u_{1_i}-\bar{u}_1)^T
$$
这个损失函数的意义就是让同类点之间的方差$\Sigma$尽量小，令异类点之间的均值距离$|\bar{u}_0-\bar{u}_1|$尽量大，这也是符合我们直觉的，更满意的是这个函数是可导的，意味着我们可以通过求导的方式获得这条投影直线的最优解，将参数和$u$都用$w^Tx$代换，可以化简出以下式子
$$
L(w)=\frac{w^T(\bar{x}_0-\bar{x}_1)(\bar{x}_0-\bar{x}_1)^Tw}{w^T(\Sigma_{x_0}+\Sigma_{x_1})w}
$$
其中$\bar{x}_0$指$X$里的所有输出值$y_i=0$的样本点（向量）的均值，$\Sigma_{x_0}$则为方差，$\bar{x}_1,\Sigma_{x_1}$同理，这个式子分子分母计算结果都是标量，因此可以写成分数形式，但是千万不要把上下的$w^T,w$约掉了，因为其虽然写成这样但是其本质还是分子矩阵乘分母矩阵的逆

由于分子和分母的中间项和$w$都没什么关系，我们令$(\bar{x}_0-\bar{x}_1)(\bar{x}_0-\bar{x}_1)^T=\Sigma_b,(\Sigma_{x_0}+\Sigma_{x_1})=\Sigma_w$，然后我们可以对其进行求导，尝试求解他的解析解，同时我们应该注意我们希望求解$w$的方向而非大小，因此对于其中的常系数部分我们可以用$C$代替
$$
\begin{align*}
\frac{\partial L(w)}{\partial w}=&(w^T\Sigma_bw)(w^T\Sigma_ww)^{-1}=0\\
&2\Sigma_bw\cdot(w^T\Sigma_ww)^{-1}+(w^T\Sigma_bw)\cdot(-1)(w^T\Sigma_ww)^{-2}\cdot2\Sigma_ww=0\\
&\Sigma_bw-(w^T\Sigma_bw)\cdot(w^T\Sigma_ww)^{-1}\cdot\Sigma_ww=0\\
&(\bar{x}_0-\bar{x}_1)(\bar{x}_0-\bar{x}_1)^Tw-C(\Sigma_{x_0}+\Sigma_{x_1})w=0\\
&(\bar{x}_0-\bar{x}_1)C=C(\Sigma_{x_0}+\Sigma_{x_1})w\\
&w=C(\Sigma_{x_0}+\Sigma_{x_1})^{-1}(\bar{x}_0-\bar{x}_1)
\end{align*}
$$

其中我们遇到了对于$w^T\Sigma_bw$的求导，我们令$g=w^T\Sigma_bw$，可得$g$是一个全元素标量函数，则他的详细求导过程如下
$$
\begin{align*}
\mathrm{d}g&=\mathrm{tr}(\mathrm{d}(w^T\Sigma_bw))\\
&=\mathrm{tr}(\mathrm{d}(w^T)\Sigma_bw)+\mathrm{tr}(w^T\Sigma_b\mathrm{d}w)\\
&=\mathrm{tr}(w^T(\Sigma_b)^T\mathrm{d}w)+\mathrm{tr}(w^T\Sigma_b\mathrm{d}w)\\
\Rightarrow \frac{\partial g}{\partial w}&=(\Sigma_b+(\Sigma_b)^T)w^T
\end{align*}
$$
又因为$\Sigma_b$是协方差矩阵，对角对称因此$\Sigma_b=(\Sigma_b)^T$，$\frac{\partial g}{\partial w}=2\Sigma_bw^T$得到推导的结果，于是，我们得到了只需要让$w$和$(\Sigma_{x_0}+\Sigma_{x_1})^{-1}(\bar{x}_0-\bar{x}_1)$同方向，就可以得到我们理想当中的投影向量

传统意义上的LDA到这就结束了，我们确实找到了一条向量$w$可以满足理论上的降维和分类要求，但是感觉好像缺了什么东西，没错，LDA只提供了求解损失函数的思路，但是没告诉我们分类的界限在哪里，也就是激活函数是什么，我们的确拿到了一个向量和在这个向量上的一堆投影点，但是此时如果再来一个点我们并没有一个判断标准去衡量这个点的输出分类结果

不过这并不是什么难事，我们不妨把投影向量和投影点看作是另一个样本模型，这回我们从需要在高维寻找一个超平面变成了在一条直线上寻找分离两组数据的分界点，这也是降维操作的核心

综上所述，**LDA就是让若干组高维的样本点能够在最有区分度的前提下投射到一条直线上的过程**

### 高斯判别分析（GDA）

那么如何找到这个分界点呢，聪明的人可能会有一个想法，我是否可以把一条直线上的两组数据看作是由两个一维高斯生成的点集呢，接着遇到一个新的点，我们只需要把这个点带入两个解出的高斯模型，看看它更可能由哪个高斯模型生成我们便判断他的类别

![img](https://s2.loli.net/2023/02/16/5IwNtoxkKpChqHQ.png)

例如上图我们根据橙蓝两组样本求解出了两个高斯分布模型（蓝线和橙线），取其交点的$x$坐标即为分界点，之后再来一个样本点，若落在绿线的左边，我们则认为他是蓝色这一组，落在绿线右边则认为是橙色一组

不过我们发现单求两个高斯模型似乎蓝色的模型好像明显肥一点，在之后的预测任务中也理所当然会占优一点，而控制胖瘦的主要原因还是高斯模型中的$\Sigma$方差一项，为了保证两个求解模型的公平，我们会令两个模型的方差相等

那么接下来的问题就在于怎么求解这两个模型了，同时由于两个模型具有相等的方差项，因此无法单独进行求解，我们不得不老老实实用MAP或者MLE求解，同时既然我们也学过了多维高斯分布了，那又为什么非要降维了以后求一维的高斯呢，我们不妨把原有的样本点集合$X$就看作是两个高维高斯模型抽取出来点集合，直接求解这两个高维高斯模型不就好了

这种思想就是GDA，他假设空间内的每一组样本点均服从同方差的高斯分布，求解出高斯模型之后根据未知点在哪个高斯模型被筛选出来的概率高来判断未知点的分类结果

明显的，这个问题可以通过MLE求解，在正统GDA方法中其实是用MAP做的，他假设分类结果$y$的先验概率符合伯努利分布，不过由于伯努利分布的参数和两个高斯模型都没什么关系，因此此处直接假设先验符合等概率分布，另MAP退化成MLE求解参数解析解

我们不妨复习一下MLE的公式，对于$x$独立分布的情况下，他往往长这样$\hat{\theta} = \arg\underset{\theta}{\max} P(x\mid \theta)=\arg\underset{\theta}{\max}\log\left(\prod_{i=1}^{N}P(x_i\mid \theta)\right)$，唯一需要动点脑子的就是其中的似然函数$P(x\mid \theta)$，在GDA中，我们令他为$P((x_i,y_i)\mid (\mu_0,\mu_1,\Sigma))=\mathcal{N}(\mu_0,\Sigma)^{1-y_i}\cdot\mathcal{N}(\mu_1,\Sigma)^{y_i}$，注意到由于存在两个模型，因此在已知参数的情况下为了获取某一样本点的概率，我们不止要传给似然函数样本点的数据，还需要给他的种类信息，他才能给我们返回正确的概率值，这也是符合常理的

接下来就是对其进行求导了，我们从简单的入手，先看看$\mu_0,\mu_1$怎么求解，其实我们不求也能大致猜到他们的最优值应该就是每一组分布点的均值向量，求解只是进一步证实我们的假设，和上一节类似，其中的$N_0,N_1$分别代表两种类别点的个数$N_0+N_1=N$，$C$表示和所求内容无关的常量，接下来以求解$\hat\mu_0$为例
$$
\begin{align*}
\arg\underset{\theta}{\max}\log\left(\prod_{i=1}^{N}P(x_i\mid \theta)\right)&=\arg\underset{\theta}{\max}\sum_{i=1}^{N}\left(\log \mathcal{N}\left(\mu_{0}, \Sigma\right)^{1-y_{i}}+\log \mathcal{N}\left(\mu_{1}, \Sigma\right)^{y_{i}}\right)\\
&=\arg\underset{\theta}{\max}\sum_{i=1}^{N}(1-y_i)\log\left(C e^{-\frac{1}{2}(x_i-\mu_0)^{T} \Sigma^{-1}(x_i-\mu_0)}+C\right)\\
&=\arg\underset{\theta}{\max}\sum_{i=1}^{N_0}(x_i-\mu_0)^{T} \Sigma^{-1}(x_i-\mu_0)
\end{align*}
$$
将$(x_i-\mu_0)^{T} \Sigma^{-1}(x_i-\mu_0)$求导后易得$\hat\mu_0=\sum_{i=1}^{N_0}x_i$的时候取极值，同理可得$\hat\mu_1=\sum_{i=1}^{N_1}x_i$，符合我们猜测，然后我们来求一下难一些的$\Sigma$，已知化简后$\sum_{i=1}^{N_0}\log \mathcal{N}(\mu_{0}, \Sigma)+\sum_{i=1}^{N_1}\log \mathcal{N}\left(\mu_{1}, \Sigma\right)$，我们令$g=\sum_{i=1}^{N}\log \mathcal{N}(\mu, \Sigma)$，求其关于$\Sigma$的导数
$$
\begin{align*}
\mathrm{d}g&=\mathrm{d}\left(\sum_{i=1}^{N}\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)}\right)\\
&=\mathrm{d}\left(C+\sum_{i=1}^{N}\log|\Sigma|^{-\frac{1}{2}}-\sum_{i=1}^{N}\frac{1}{2}(x_i-\mu)^{T} \Sigma^{-1}(x_i-\mu)\right)\\
&=-\frac{1}{2}\mathrm{d}(N\log|\Sigma|)-\frac{1}{2}\mathrm{d}\left(\mathrm{tr}\left(\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^{T} \Sigma^{-1}\right)\right)\\
&=-\frac{N}{2}\frac{1}{|\Sigma|}|\Sigma|\mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)-\frac{1}{2}\mathrm{tr}\left(\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^{T}\mathrm{d}\Sigma^{-1}\right)\\
&=-\frac{N}{2}\mathrm{tr}(\Sigma^{-1}\mathrm{d}\Sigma)-\frac{1}{2}\mathrm{tr}(N\Sigma_x(-\Sigma^{-1})\mathrm {d}\Sigma\Sigma^{-1})\\
\Rightarrow\frac{\partial g}{\partial\Sigma}&=-\frac{N}{2}\left(\Sigma^{-1}-\Sigma^{-1}\Sigma_x^T\Sigma^{-1}\right)=\frac{N}{2}\left(\Sigma_x^T\Sigma^{-2}-\Sigma^{-1}\right)
\end{align*}
$$

> 注意$\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^{T}=\Sigma_x,\Sigma\neq\Sigma_x$，$\Sigma$和$\Sigma_x$的形状都是$p\times p$，且均为对称矩阵，因此可交换，同时其中使用到了行列式和逆矩阵的求导结果，使用了矩阵求导一章中的$(10)(11)$定理，可以对照着看

将求解结果带入原式，令导数为$0$，则可得到$\hat\Sigma$
$$
\begin{align*}
\frac{\partial\log\left(\prod_{i=1}^{N}P(x_i\mid \theta)\right)}{\partial\Sigma}=\frac{N_0}{2}\left(\Sigma_0^T\hat\Sigma^{-2}-\hat\Sigma^{-1}\right)+\frac{N_1}{2}\left(\Sigma_1^T\hat\Sigma^{-2}-\hat\Sigma^{-1}\right)&=0\\
\hat\Sigma&=\frac{1}{N}(N_0\Sigma_0^T+N_1\Sigma_1^T)
\end{align*}
$$
于是我们得出了模型三个参数的解析解，他们分别是$\hat\mu_0=\sum_{i=1}^{N_0}x_i,\hat\mu_1=\sum_{i=1}^{N_1}x_i,\hat\Sigma=\frac{1}{N}(N_0\Sigma_0^T+N_1\Sigma_1^T)$，如果感觉这样看上去有点抽象，那么结合到实际的问题中，意思就是现在我们有一堆样本点集$X$和其对应的分类结果$Y$，接着我们把$X$中同类的样本点分别取出来数一数他们的个数$N_0,N_1$，并求他们的均值$\mu_0,\mu_1$和方差$\Sigma_0,\Sigma_1$，均值部分我们直接拿来用，方差则把他根据种类点的个数加个权$\frac{1}{N}(N_0\Sigma_0^T+N_1\Sigma_1^T)$产生新的统一方差$\Sigma$，这样我们就构造出了两个高斯模型$\mathcal{N}(\mu_0,\Sigma),\mathcal{N}(\mu_1,\Sigma)$，之后我们碰到了一个新的样本点，就把他丢到这两个模型里面看看输出，哪个大就说明这个样本属于哪个分类

我们总结一下，**GDA就是假定分类样本遵循同方差高斯分布的前提下，求解各自高斯模型的过程**

### 逻辑回归

不过我们手动把样本代到两个模型再人力判断哪个大显然还是比较麻烦，有没有什么可以像一开始的符号函数一样一条算式表示出结果分类呢

我们首先得意识到我们求出来的两个高斯模型的含义是什么，没错，就是$P(x\mid y_0),P(x\mid y_1)$，意思就是我已知某一组样本是什么种类了，我们把样本点$x$带入模型中，就可以得到这个样本点属于这个种类的概率，但是我们的目标其实是需要当我知道$x$之后其属于某一分类$y$的概率也就是$P(y_0\mid x)$才对

听起来似乎有点绕，那么我们可以假设一个样本点$x$，我们把他带入两个模型的过程就相当于求解了这个样本点在两个模型下的概率$P(x\mid y_0),P(x\mid y_1)$，比如概率一个是$0.9$一个是$0.8$，于是我们通过比较猜想$x$属于第一个模型，由于两个模型输出的结果是不相关的，因此必须要求我们求解两次模型结果，但是如果模型是$P(y_0\mid x)$，因为只考虑二分类问题，有$P(y_0\mid x)+P(y_1\mid x)=1$，那么我们只需要任取一个模型比如$P(y_0\mid x)$，通过传给他$x$就可以获知属于该分类的概率，由于只有属于这个分类和属于另一个分类两种情况，因此只需要判断其输出是否大于$0.5$即可知道该点应该被如何分类，肉眼可见的减少了一个模型的计算量

所以如何把$P(x\mid y_0)$变成$P(y_0\mid x)$，这就需要借助我们强大的贝叶斯公式了
$$
P(y_0\mid x)=\frac{P(x\mid y_0)P(y_0)}{P(x\mid y_0)P(y_0)+P(x\mid y_1)P(y_1)}
$$
其中$P(y_0),P(y_1)$我们之前称之为先验概率，此处其实也可以这么理解，由于所求的$P(y_0\mid x)$是一个预测模型，我们得根据已有的样本来判断两种种类的个数分布情况，比如简单一点我们拿到的样本$X$里$N_0=N_1$，意味着我们假设获取到的样本是服从等分布的，之后我们就可以计算一下$P(y_0\mid x)$的结果是什么样的
$$
P(y_0\mid x)=\frac{\mathcal{N}(\mu_0,\Sigma)}{\mathcal{N}(\mu_0,\Sigma)+\mathcal{N}(\mu_1,\Sigma)}=\frac{1}{1+\frac{\mathcal{N}(\mu_1,\Sigma)}{\mathcal{N}(\mu_0,\Sigma)}}=\frac{1}{1+e^{-z}}\\
z=\log\frac{\mathcal{N}(\mu_0,\Sigma)}{\mathcal{N}(\mu_1,\Sigma)}
$$
其中$P(y_0\mid x)$的化简结果$\frac{1}{1+e^{-z}}$就是大名鼎鼎的Sigmoid函数，我们也知道了原来Sigmoid是由贝叶斯公式变过来的，但是由于这条式子里还是需要计算两遍模型结果，好像和传说中的逻辑回归还差点什么东西，这时候就需要我们尝试化简一下其中$z$部分了
$$
\begin{align*}
z &= \log\left(\frac{\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_0)^{T} \Sigma^{-1}(x-\mu_0)}}{\frac{1}{(2 \pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_1)^{T} \Sigma^{-1}(x-\mu_1)}}\right)\\
&=-\frac{1}{2}\left((x-\mu_0)^{T} \Sigma^{-1}(x-\mu_0)-(x-\mu_1)^{T} \Sigma^{-1}(x-\mu_1)\right)\\
&=-\frac{1}{2}x^T\Sigma^{-1}x+\mu_0^T\Sigma^{-1}x-\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0+\frac{1}{2}x^T\Sigma^{-1}x-\mu_1^T\Sigma^{-1}x+\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1\\
&=\left(\mu_0^T\Sigma^{-1}-\mu_1^T\Sigma^{-1}\right)x+\left(\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1-\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0\right)\\
&=w^Tx+b
\end{align*}
$$
我们发现了一个让我们惊掉下巴的事实，这个$z$居然是一个线性方程，说好的高斯模型呢？我那么大个的两个高斯模型跑哪去了？没错，高斯模型确实消失了，不然为什么说逻辑回归牛逼呢，实际上对于两个方差一致的高斯模型来说，无论其是几维模型，模型的接触面都一定是一个超平面，而这一条线性方程所描述的正是这条高斯模型的接触面，而这个面正是在两个高斯模型下分割分类点的最优超平面

不止如此，我们之前一直假定两种种类点的分布是符合高斯分布的，实际上谁说点的分布一定是高斯分布呢，我们发现，在这个线性方程中，如果把$b$看作$w^T$的一部分，实际上我们从需要求解$\mu_0,\mu_1,\Sigma$三个参数变成了只需要求解一个参数$w^T$，虽然可能这个参数的解析解算式比起那三个参数要复杂，但是至少他是一个参数，意味着Sigmoid的存在给了我们一种可能，可以让我们不止跳过代入两个模型比较解答的过程，更可以让我们跳过求解$\mu_0,\mu_1,\Sigma$参数的过程

理论存在，我们不如尝试实践看看，在求解$\mu_0,\mu_1,\Sigma$三个参数的时候，我们曾经构造出了一条$P(x\mid \theta)=\mathcal{N}(\mu_0,\Sigma)^{1-y_i}\cdot\mathcal{N}(\mu_1,\Sigma)^{y_i}$的算式，如今我们依葫芦画瓢，令Sigmoid函数为$\sigma(w^Tx)$，则构造$P(x\mid \theta)=\sigma(w^Tx)^{1-y_i}\cdot(1-\sigma(w^Tx))^{y_i}$，接着同样通过MLE尝试求解解析解$\hat w$
$$
\begin{align*}
\hat w&=\arg\underset{\theta}{\max}\log\left(\prod_{i=1}^{N}P(x\mid \theta)\right)\\
&=\arg\underset{\theta}{\max}\sum^N_{i=1}\log\left(\sigma(w^Tx_i)^{1-y_i}\cdot(1-\sigma(w^Tx_i))^{y_i}\right)\\
&=\arg\underset{\theta}{\max}\sum^N_{i=1}\left((1-y_i)\log\sigma(w^Tx_i)+y_i\log(1-\sigma(w^Tx_i))\right)
\end {align*}
$$
在这边我们不妨先暂停一下，虽然还没解完，但是到此为止其实已经证明了$\hat w$的可解性，之所以在此停顿，是为了顺便引出另一个重量级公式$\sum^N_{i=1}\left((1-y_i)\log\sigma(w^Tx_i)+y_i\log(1-\sigma(w^Tx_i))\right)$，交叉熵公式，比起直接令其导数为零解出其解析解，因为其非线性的特征（例如我很难直接得出$\sigma(x)=0.6$的时候$x$应该取多少），我们更喜欢使用梯度下降的方法求解其极值，再往后，就得谈及深度学习了

回过头来，我们确实把三个参数变成了一个参数，但毕竟求解的参数减少了，难道对模型就一点影响都没有吗？并不然，参数的减少实际上不再把点约束在高斯分布之内，而是涵盖了所有化简后会产生此类结果的模型，因此此处参数的减少不仅不会对求解精度造成损失，而且还做到了缓解过拟合的作用，令模型的泛化能力更加出色

说了那么多，我们按照惯例做一个总结，**所谓逻辑回归，从结构上看就是一个套了Sigmoid函数的线性模型，而其推导本质则是在GDA的基础上，泛化高斯模型之后化简得出的**

## 降维

在机器学习中降维是一个非常重要的思想，在LDA中我们初窥降维的用途，我们用它提供了一种高维分类的方法，实际上，LDA只能算是降维方法一种比较粗浅的应用，降维的主要应用还是集中在提取特征方面，一方面合理化的降维可以去除一组高维数据中的可能冗余特征保证提取特征的精度，另一方面在进行特征提取的时候，合理化的降维，制作可视化样本分布图也可以帮助我们提取正确的特征，提高分类工作的成功率

### 拉格朗日乘子法

求极值我们都很熟悉，但是在这一节，我们需要探讨一下如何求解约束条件下的极值问题

那么什么是约束条件下的极值问题，这是一个典型的约束条件的极值问题：在保证$x^2y=3$的情况下，求$x^2+y^2$的最小值，如果用数学语言表达，就是这样
$$
\begin{eqnarray}
&&\min(x^2+y^2)\\
&&s.t.\quad x^2y=3
\end{eqnarray}
$$
你说这还不简单，我把约束条件的$y$用$x$来表示，接着代换回求解的函数，然后求导求解最小值不就完了，这种思想就是求解约束条件下极值问题的一种解决思路——消元法，但是其中无论是讨论$x,y$是否为$0$的情况，还是约束条件到底方不方便让一个变量由另一个变量表示都是十分随机的，那么是否存在一种无脑一些的方法可以不用考虑那么多直接求解呢

这就需要我们换一个角度理解这个问题，我们令$f(x,y)=x^2+y^2,g(x,y)=x^2y$，接着先来看求解函数，$x^2+y^2$实际上就是在三维空间的一条曲面，且当没有约束条件的情况下，我们易得$x=0,y=0$时取得曲面在$z$方向时的最小值$0$，然后再来看约束条件，$x^2y=3$实际上是规定了$x,y$的取值范围，本来$g(x,y)$也是一个曲面，但是由于对其沿水平方向进行了切割，所得的刚好就是$g(x,y)$在$z=3$上的等高线，因此，我们要保证取值点的$x,y$落在$g(x,y)=3$这条等高线的前提下，使$f(x,y)$的取值最小

如果有点晕，那么可以看看以下这张图

![img](https://s2.loli.net/2023/02/22/yEwhe9rAi3DKWv8.png)

知道了求解目标的几何意义，对我们求解函数极值有什么帮助呢，由于$g(x,y)=3$看作是一个平面上的线，因此我们可以尝试把$f(x,y)$也投影到平面上，同时为了保留其大小的含义我们也对其添加等高线，等高线上的各点函数值相同

由于等高线有无数条，从直觉上来说，我们也应该知道想要取得极值，必须保证$g(x,y)=3$的线和某条等高线仅有一个交点，而这不就是相切的定义吗，接着我们再用一下幼儿园就学过的两条曲线相切的性质，我们发现其中一定有一条“交点处的切线斜率相同”的定义，引申出来即为相切曲线共用一条切线，而我们又有定义等高线的切线与其梯度垂直的定义，这不巧了，既然两条都是等高线，那我是不是也可以说，当$f(x,y)$在$g(x,y)=n$约束下取到极值时，极值点的梯度同向，即$\Delta f(x,y)=\lambda\Delta g(x,y),\lambda\in\mathbb{R}$

![img](https://s2.loli.net/2023/02/22/qhOMRbxvaoVsIH3.png)

> 上图紫线即为$f,g$在它们等高线的切点$A$的梯度方向，不过究竟梯度是向圆心还是向外取决于$f,g$三维图像函数值相对较高点，对于这个问题来说$x,y$越大$f$取值越大，因此梯度方向指向圆心反方向，同理可得$g$梯度方向和$f$同向
>
> 需要注意梯度向量虽然指向函数值较大方向，但是它是二维的，不存在$z$轴上的分量，只恒落在$x,y$轴组成的等高线平面上

到此为止，求解$f(x)$的极值我们就有了两个条件，根据它们我们可以构建方程组
$$
\begin{cases}
\Delta f(x,y)=\lambda\Delta g(x,y)\\
g(x,y)=3
\end{cases}\Rightarrow
\begin{cases}
\frac{\partial f}{\partial x}-\lambda\frac{\partial g}{\partial x}=0\\
\frac{\partial f}{\partial y}-\lambda\frac{\partial g}{\partial y}=0\\
x^2y-3=0
\end{cases}
$$
其中求解$f,g$梯度需要分别分为求解$x$方向偏导梯度和$y$方向梯度两条式子，三条式子求解三个变量明显是可解的，这样，我们就把一个有约束的极值问题转换为了固定的解方程问题

这种方法是完全正确的，不过有些人觉得这么写还是不够优雅，为了让求解过程变得足够直观，额外又构造了一个函数$L(x,y,\lambda)=f-\lambda g',g'=g-3$，如果我们把$L$看作是一个关于$x,y,\lambda$的无约束的优化问题，直接通过求偏导等于$0$的方式求解，就会发现其偏导结果正好对应了上面的三条式子
$$
L'=0\Rightarrow
\begin{cases}
\frac{\partial L}{\partial x}=\frac{\partial f}{\partial x}-\lambda\frac{\partial g}{\partial x}=0\\
\frac{\partial L}{\partial y}=\frac{\partial f}{\partial y}-\lambda\frac{\partial g}{\partial y}=0\\
\frac{\partial L}{\partial \lambda}=-(g-3)=0
\end{cases}
$$
到此为止，我们终于把一个有约束的优化问题转化为了无约束的优化问题，这个方法同样适用于更高维的$f,g$，总结一下，**仅从做题结果来说，使用拉格朗日乘子法求解有约束的优化问题分为如下四步**

1. **构造求解函数$f$和约束条件函数$g$，且应当把约束条件转换为$g=0$的形式**
2. **定义拉格朗日乘子$\lambda$，并构造函数$L=f-\lambda g$**
3. **将$f,g$替换为多维的自变量函数，并对每一维变量和$\lambda$分别求偏导，令偏导数为$0$**
4. **联立求解多元方程组，获得的自变量取值即为所需极值（如果有多个，说明存在多个极大和极小值，分别带入$f$验证即可）**

### Centering Matrix

在线性分类的讨论中，我们曾经令所有样本点的排列为$X=(x_1,x_2\dots x_N)$，但是无论是在求解均值还是方差的过程中我们似乎都没有用到$X$，由于从小老师就只教了一个个$x_i$加起来然后除个数的算法因此这么做也是合理的，但是在程序中，我们往往用一个矩阵$X$表示所有的样本点，如果按照我们的方法就得把$X$先拆了再求，那还不如不构造$X$，是否有一种在保留$X$的情况下直接求解均值和方差的做法呢

我们不妨试一试，注意假设所有样本点$x_i$都是列向量
$$
\begin{align*}
\bar{X}_{p\times 1}&=\frac{1}{N}\sum_{i=1}^{N}x_i\\
&=\frac{1}{N}\begin{pmatrix}
 x_1 & x_2 & \cdots & x_N
\end{pmatrix}\begin{pmatrix}
 1 \\ 1 \\ \vdots \\ 1
\end{pmatrix}\\
&=\frac{1}{N}X\mathbb{1}_N
\end{align*}
$$

$$
\begin{align*}
\Sigma_{p \times p} 
&=\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}-\bar{X}\right)\left(x_{i}-\bar{X}\right)^{T} \\

& =\frac{1}{N}
\begin{pmatrix}
    x_{1}-\bar{X} & x_{2}-\bar{X} & \cdots & x_{N}-\bar{X}
\end{pmatrix}
\begin{pmatrix}
    \left(x_{1}-\bar{X}\right)^{T} \\
    \left(x_{2}-\bar{X}\right)^{T} \\
    \vdots \\
    \left(x_{N}-\bar{X}\right)^{T}
\end{pmatrix}\\

&=\frac{1}{N}
\left(
    \begin{pmatrix}
    x_{1} & x_{2} & \cdots & x_{N}
    \end{pmatrix}
    -\bar{X} \cdot \mathbb{1}_{N}^{T}
\right)\left(\begin{pmatrix}
x_{1}^{T} \\
x_{2}^{T} \\
\vdots \\
x_{N}^{T}
\end{pmatrix}-\mathbb{1}_{N} \cdot \bar{X}^{T}\right) \\

&=\frac{1}{N}\left(X^{T}-\bar{X} \cdot \mathbb{1}_{N}^{T}\right)\left(X-\mathbb{1}_{N} \cdot \bar{X}\right)^{T}\\

& =\frac{1}{N}\left(X^{T}-\frac{1}{N} X^{T} \mathbb{1}_{N} \mathbb{1}_{N}^{T}\right)\left(X-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T} X\right) \\

& =\frac{1}{N} X^{T}\left(I_{N}-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T}\right)\left(I_{N}-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T}\right) X
\end{align*}
$$

其中，我们令$I_{N}-\frac{1}{N} \mathbb{1}_{N} \mathbb{1}_{N}^{T}$是一个$N\times N$的方阵，我们称他为centering matrix，记作$H$，可以证明的是$H^T=H,H^n=H$，由于$X^{T}-\bar{X} \cdot \mathbb{1}_{N}^{T}=X^TH^T$，因此这个$H$矩阵在数学意义上其实就是把相乘矩阵的均值置零，矩阵$X$的方差可化简为$\Sigma=\frac{1}{N} X^{T}HX$

与此同时，通过这个式子我们也可以说明其实方差的计算可以被分为两步，第一步$HX$表示把所有样本点整体移动到原点附近，第二步$(XH)^THX$则是把移动后的每一个样本点计算和原点的距离的平方相加，换一个角度来说，如果样本点原本的均值就是零，则$X^TX$就可以表示为其方差矩阵

> 需要注意由于$X$不是方阵，因此$X^TX\ne XX^T$，一个形状是$N\times N$另一个是$p\times p$，$H$的大小也是同理

### 奇异值分解（SVD）

在说奇异值分解之前，我们需要先回顾一下特征值分解的过程，首先我们需要了解到一个方阵的特征值代表了什么含义，说到特征值，我们就不得不提矩阵，特征值和特征向量相互之间的关系式
$$
Av=\lambda v
$$
其中$A$是分解矩阵，$v$是特征向量，$\lambda$是特征值，线代中我们往往只是知道这么一条式子但不知道特征值和特征向量的具体含义，实际上，任何一个方阵乘以一个向量所得结果一定是和该向量同维的另一条向量，因此每一个方阵从另一种角度也可以看作是给一个向量做了一次线性变换，而线性变换的过程包括伸缩和旋转，接着我们再来看这条有关特征值的式子，我们发现了，在这个式子中矩阵对特征向量做的线性变换居然能通过一个常数乘这个特征向量得到，而我们又知道常数只会对向量做伸缩变换，因此找寻特征值的过程总结一下就是找寻某一条不受方阵旋转变换影响的特征向量

![Image](https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuypRuABCGAYIouIazEuNcZTibpdjY39e0kHWiaUF1PHjEfXovvTWaFmlopSu9RgPicvVzdpcZdqpyhiaA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

> 上面是$M=\begin{bmatrix}
>  1 & 1\\
>  1 &0
> \end{bmatrix}$在二维空间的变换过程，我们可以发现$(1,0)$这个方向的向量方向完全不受坐标轴的拉伸和扭曲影响，意味着其就可以作为方阵$M$的一个特征向量

接着我们再复习一下特征值的分解过程，我们的目标是将方阵$A$分解成$Q\Sigma Q^T$形式，其中$Q$是由特征列向量排列成的矩阵，$\Sigma$是对角矩阵，其每一个对角元素皆为一个$A$的特征值，且由左上至右下依次减小，其中的求解过程大致可以分为三步

1. 构造计算$|A-\lambda I|=0$求得多个$\lambda$特征值
2. 把求解出来的特征值带回$(A-\lambda I)x=0$解线性方程组，求得各个特征向量$x$，由于我们只需要特征向量方向，大小并不重要，因此一般来说我们会把特征向量缩放成单位向量作为最终结果
3. 把各个特征值从大到小沿对角排列构造出$\Sigma$，特征向量与其相对应排列为$Q$，如果有重根则排列多次，保证$A$和$\Sigma$同维度

但是我们也提到了，特征值分解强制要求$A$是方阵，这也符合理解，不然和向量相乘出来也不是另一个同维度向量了，但是终究说到底还是方阵，于是就有人想是否可以把他拓展到任一矩阵中，于是就产生了奇异值分解

对于一个$A_{m\times n}$的矩阵，我们要将其分解成$U_{m\times m}\Sigma_{m\times n} V_{n \times n}^T$，且这个分解是任一矩阵均存在的，奇异值分解的具体过程如下

1. 计算$AA^T,A^TA$，易得这两个矩阵为实对称方阵

2. 求解他们的特征值和特征向量，注意到实对称方阵的特征向量一定互相正交，这两个方阵的特征值一定只差0的个数（后一个的严格证明可看下图）

   ![img](https://iknow-pic.cdn.bcebos.com/4a36acaf2edda3cc014e752d02e93901213f9211?x-bce-process=image%2Fresize%2Cm_lfit%2Cw_600%2Ch_800%2Climit_1%2Fquality%2Cq_85%2Fformat%2Cf_auto)

3. 令$AA^T$特征向量方阵为$U$，$A^TA$特征向量方阵为$V$，非零特征值取根号后按大小排布构成矩阵$\Sigma'$，之后根据$A$的形状把$\Sigma'$补零成$\Sigma$，其构造证明可以看以下式子
   $$
   \begin{cases}
   A=U\Sigma V^T \\
   A^T=V\Sigma^T U^T
   \end{cases}\Rightarrow
   AA^T=U\Sigma V^T\cdot V\Sigma^T U^T=U\Sigma^2 U^T
   $$
   同理可得$U,V$分别就是$AA^T,A^TA$的特征向量组，$AA^T,A^TA$的任一特征值的排布正是$\Sigma^2$

对于奇异值的实际作用，可以在接下来的PCA推导中看到，此处我们从奇异值的分布情况稍微提一下，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上。也就是说，剩下的90%甚至99%的奇异值几乎没有什么作用，例如我随便打一个矩阵
$$
A=\begin{bmatrix}
 1 & 2 & 3 &4  & 4\\
 1 & 2 & 3 &4  & 6\\
 4 & 5 & 7 & 8 &4 \\
 9 & 4 & 5 &2  &1
\end{bmatrix}\Rightarrow \Sigma=\begin{bmatrix}
 18.49 & 0 & 0 &0  & 0\\
 0 & 7.64 & 0 &0  & 0\\
 0 & 0 & 2.93 & 0 &0 \\
 0 & 0 & 0 &0.06  &0
\end{bmatrix}
$$

### 主成分分析（PCA）

经过了这么多的知识铺垫，我们终于可以来看看本章的重点PCA了，在此之前我们先思考一个问题，假如我现在拿到了几组样本点，我们怎么知道这些样本点到底可不可分呢，有时候我们总会在某些论文上看到画的五颜六色的散点图，但是这些图又是怎么画出来的呢，数据的处理固然重要，但是如何把数据用可视化的方法展示出来同样是十分重要的一环

#### 最大投影方差

在LDA一章中，我们详细介绍了如何通过合理的降维完成分类，PCA和他的思想有点像，但是在LDA中我们已经存在前提就是样本点数据是线性可分的，我们的目标就是对其进行分类，而PCA的目标并不是区分样本点，只是把我们无法观测到的高维样本点用我们可以理解的方式表示出来而已

降维最容易碰到的问题就是数据点重合在一起，比如$(1,2,3),(1,2,4)$两个点只有第三维坐标不一样，如果我们把他们沿$x,y$平面降维，也就是投影到$x,y$平面上，就会出现两个点在二维平面上叠在了一起，仅看降维后的平面我们完全无法区分这两个点，从观测角度显然是不利的，但是如果我们将其沿别的平面上投影，两个点就可能可以被分开，而当数据点变多，每一个数据点之间的距离越大，显然也就越利于我们分析

降维后重叠在一起的点我们可以称之为一种特征的损失，不仅是对我们观测不利，由于他把高维中两个不相关的点投影到了同一个位置，在降维的其他使用领域，比如降噪，压缩等任务中同样是不利的，如何找到一个尽可能让数据点投影上去能够被分开的超平面就成了我们需要克服的问题

在LDA中，我们使用了投影后的均值和方差来找到投影直线，在此处我们同样可以使用这个思路，既然需要在超平面中尽可能被分开，是不是意味着投影到这个超平面的数据点之间的方差要尽可能大呢？于是就有了最大投影方差的解决思路

和LDA不一样的是，此处我们不再仅限于投影到一条直线上，而是要取能使样本点方差尽可能大的前几条投影直线，将样本点投影到这些若干条直线之后用这些直线当作另一组坐标基重构样本点，易得所重构的超平面一定是能够时样本投影方差最大的超平面，同时由于这些直线个数可以根据实际需求任取，意味着降维到的维数也是可控的

于是现在的问题就是如何求解这一组投影直线了，实际上我们完全可以用类似LDA的方法求解，除了少求一次方差之外可以说其余步骤都是完全一致的，由于我们只需要最大化投影方差，因此我们甚至可以直接把方差计算作为用于最大化的损失函数，我们定义投影向量为$w_1$，直接套用LDA的化简结果
$$
L=w^T\frac{1}{N}\sum^{N}_{i=1}(x_i-\bar x)(x_i-\bar x)^T w=w^T\Sigma w
$$
然后我们发现这玩意好像有点问题，由于我们没有限定投影向量的长度，因此好像只要$w$越长这玩意结果就越大，这样的话就不管$X$什么事了，更无法确定$w$的方向，这显然不是我们想要看到的，为了公平起见，我们令$w$是单位向量，即$w^Tw=1$

于是问题就转化为了在约束条件$w^Tw=1$求解$L$的最大值，接下来就是拉格朗日乘子法出场的时候了
$$
L'=L-\lambda(w^Tw-1)
$$
同时需要注意我们的目标只是确定$w$分量的方向，因此我们可以仅对$w$求偏导令其等于$0$
$$
\frac{\partial L'}{\partial w}=2\Sigma w-2\lambda w=0\\
\Rightarrow\Sigma w=\lambda w
$$
这玩意不就是$\Sigma$方阵的特征方程吗，对$w$求偏导之后得出来的这条等式，用人话说就是对于$w$分量来说他所有的极值点都集中在他自己是作为$\Sigma$的特征向量上了，也就是我们想取到$L'$的最大值，就得去这些特征向量上找，不止如此，从这条式子我们还能知道$\Sigma w$的大小是受$\lambda w$控制的，因为我们没办法改变$w$的大小，因此求最大值的重担就落到了$\lambda$身上了，从大到小，我们每取一个特征值，相对应的特征向量$w$就作为样本点的一条投影向量

#### 正交基

如果你仔细想想就会发现好像什么东西不太对，我们在介绍拉格朗日乘子法的时候曾经说过，这个方法只能获知所求函数的多个极值，想知道求出来的哪个是最大或者最小值，正常还应该把他带回到原始的式子中看看结果，与此同时我们还知道既然这个$L'$是可导的，那么其在高维空间中的函数图像也一定是连续的，既然他图像又连续，又存在最大值，甚至我们还知道最大值就是特征值最大的那一条特征向量，也就是说我们知道所有的样本点一定是在这一条特征向量上的投影方差可以达到最大值，那么按理说把这条最大的特征向量任意旋转一个极其微小的角度，落在这条被微调过的向量上的投影方差一定是可以无限接近原始向量的，也就是说，除了最大特征值对应的那个特征向量，其余的特征向量所获得的投影方差一定小于这条微调过的向量，既然如此，又为什么要逐个取每一个特征向量呢，而且如此一来只有投影方差最大的那条向量可以被确定，又如何把一组样本点投影到多条向量之后重构空间呢？

这就得说到方差矩阵的特征向量的另一个重要性质了，没错就是正交性，由于方差矩阵（或者明确说是协方差矩阵）是一个对称矩阵，意味着其每一个特征向量之间一定是相互垂直（正交）的，于是你会发现一个惊人的事实，我们如果把样本数据照特征向量顺序逐个做投影，然后把投影结果（是一个值）排在一起作为另一组样本特征（是一组向量），如果我们能把各个数据点画在一个超维空间中，那么以上的步骤实际上就是把所有的点旋转了一下而已

![img](https://s2.loli.net/2023/02/23/nH1KxfyhwlU3ZiS.png)

如上图所示，求解所得的特征值最大的特征向量比如是$w_1$，我们的确可以看到各个样本点（蓝点）在其上面的方差是最大的，另一条特征向量与之垂直，在二维平面上则一定是绿色这条$w_2$，然后我们发现在这两条特征向量的映射下，如果把$w_1,w_2$分别看作是新的$x,y$轴，则所有的蓝色点仅做了平移和旋转变换，而且其中的平移变换还是因为为了便于观察我把特征向量挪到了样本点均值处，正常求解出来的特征向量是从原点出发的

仅作旋转变换最显著的好处就是不改变样本的分布，并且其保留了投影方差最大的特征向量，意味着假如我想把上面的蓝色点降维到一条直线上，那么只需要保留$w_1$一条向量和其上的投影点即可，对于更高维的样本点，由于我们有了特征值这一个衡量维度去判断特征向量的好坏，因此同理只取前$k$大特征值对应的特征向量，就可以在保证不改变样本点分布且尽可能以投影方差最大的前提下将样本降维到$k$维

#### 最小重构代价

如果说最大投影方差是顺着思路一步步推导出特征向量作为基底的过程，那么最小重构代价就是倒着进行，我们先假设一堆样本点已经在$w$的影响下从$p$维被降维到了$q$，接着通过比较降维前后数据点的偏移情况求解最优化正交基

![img](https://s2.loli.net/2023/02/23/p427boKtyAFLNPc.png)

由于这种方法不需要求解样本方差，因此我可以取样本中的任意点单独讨论，比如现在我有一个样本点$A$，在上一节当中我们已经明确需要寻找的投影向量必须相互正交，因此此处我们就任意构造出两条正交单位向量$w_1,w_2$，$A$点在两条基上的投影分别记为$D,E$，如果假设$A$点坐标$(B,C)^T=a$，根据点乘等于投影长度，易得$OD,OE$长度分别为$w_1^Ta,w_2^Ta$，那么我们就可以得到$a$的另一组向量表示$a=(w_1^Ta)w_1+(w_2^Ta)w_2$，你可能奇怪这有啥意义，搞那么半天得到的不还是$a$，为什么要如此多此一举

先别急，接下来我们要把他降到一维，按照上一节的说法，只需要丢弃一个维度就行了，比如我可以不要$w_2$，那么现在我们就只有一条向量$w_1$了，没有了$OD$来控制$A$在$w_2$上的分量，等于说$A$点被降维到了$A'$点上，照着上面的样子，我们易得$A'$坐标$a'=(w_1^Ta)w_1$

现在我们用统一的$a$和正交基来表示了降维前后的两点，并且将其映射回了原空间中，于是我们就开始思考，降维这个操作让原本的$A$点变成了$A'$点，我们是否可以定量的表示这个变换中的特征损失情况呢，然后你可能发现还真能，我只需要计算两点之间的距离$AA'=d$不就行了吗，明眼人都看得出来，要是$d$比较大，明显和原本的$A$的距离就越远，距离越远就越不相似，相对应的降维特征损失也就越多了

**这种在原始空间中计算降维后的点在重构回降维前的样本点的距离最小值的方法就叫做最小重构距离**，下面是计算过程，距离的计算直接使用欧氏距离即可，同时注意样本点不止一个且维度也不止二维，我们假设有$N$个$p$维样本点$x_1,x_2\dots x_N$，需要在$w_1,w_2\dots w_p$这一组正交基上被降维到$q$维，即抛弃后$p-q$个$w$，只使用$w_1,w_2\dots w_q$表示降维点$x'_1,x'_2\dots x'_N$，需要注意$x'$的维数也是$p$，可以参考上面的$a'$推导过程
$$
\begin{align*}
L&=\frac{1}{N}\sum_{i=1}^N\left \| x_i-x'_i \right \| ^2\\
&=\frac{1}{N}\sum_{i=1}^N\left \|\sum_{j=1}^{p-q}(w_j^Tx_i)w_j\right \|^2\\
&=\frac{1}{N}\sum_{i=1}^N\sum_{j=q+1}^{N}(w_j^Tx_i)^2\\
&=\sum_{j=q+1}^{N}\frac{1}{N}\sum_{i=1}^N(w_j^T(x_i-0))^2
\end{align*}
$$
其中如果我们把$x_i$看作是一组均值为$0$的样本，那么$\frac{1}{N}\sum_{i=1}^N(w_j^T(x_i-0))^2$就可以看作是$w_j^T\Sigma w_j$，我们要取其最小值，因此问题就变成了下面这样
$$
L=\arg\underset{\theta}{\min}\sum_{j=q+1}^{N}w_j^T\Sigma w_j\\
s.t.\quad w_j^Tw_j=1
$$
由于每一组$w_j$都是相互独立的，因此这和上一节的求解式子是一样的，结果同样是$\Sigma w=\lambda w$，不过需要注意的是出我们求解的是最小值，同时由于我们前提已经假设了$w$之间相互正交，因此$w_j$是全不相同的，因此求解的结果就是取前$p-q$个最小的特征值对应的特征向量作为解，而我们又已经假设了第$q+1$到$N$个$w$其实是在降维中被丢弃的，因此此处求解出来的结果就是在$p$降到$q$维的过程中不需要的，换个角度，留下来的那些特征向量正是前$q$个最大的，这和在最大投影方差一节中求解的结果是完全一致的

不过我们在其中假设了一下所有$x$都是以均值为$0$排布的，这也是解释了为什么在上一节中需要把特征向量挪到样本点均值为起始的原因

通过两次推导，到此我们可以总结一下PCA的一般操作过程，假设我们要把样本点集$X_{p\times N}$降维到$q$维，则只需要先计算样本的方差$\Sigma$的特征值，取前$q$个最大特征值的单位特征向量作为基底，然后把所有样本点与这些基底做点积计算投影长度，把投影长度重新排布就形成了降维后的样本点矩阵$X'_{q\times N}$

#### SVD和PCoA

了解了便于理解的PCA做法，然而从写程序或者是计算角度来说，这种方法其实并不方便，不仅要逐个遍历样本计算特征值和点积，还要把算完的内容重新排列组合，是否有什么快捷的手段可以直接得出PCA降维后的坐标结果呢

为了避免歧义，以下把样本点$X$的方差记为$S$，根据前面的结论，有$S=\frac{1}{N}X^TH^THX$，由于$HX$不是方阵，我们对$HX$进行奇异值分解记为$HX=U\Sigma V^T$

把奇异值分解结果带回$S$，则有$S=V\Sigma^T\Sigma V^T$，这样一来，我们就可以跳过先求解$S$再求解其特征值的步骤，直接求解$HX$的奇异值分解矩阵$\Sigma$和右乘特征向量矩阵$V$即可，然而实际做下来你却发现看似简单一点实际也是骗人的，因为求奇异值的过程等于已经间接求了一次$S$和他的特征值，实际并没有简便多少，而且之后的坐标映射同样需要手动完成

于是我们可以再想想有没有什么办法可以直接拿到降维后的坐标，由于我们知道了需要求解的特征向量组就是$V$，因此坐标的映射关系可以写作$HX\cdot V=U\Sigma V^TV=U\Sigma$，诶这下我们发现了$U\Sigma$正是降维后的坐标，与此同时我们还发现如果我们构造一个$T=HXX^TH^T$，那么按照化简$S$的经验，他化简之后正是$U\Sigma\Sigma^TU^T$，因此为了得到最终的坐标，我们完全可以直接对$T$进行特征值分解，取前$q$大的特征值取根号之后直接和其对应的特征向量矩阵$U$相乘获得所有的映射坐标值，这种不借助样本方差，直接求解降维坐标的方法就叫做PCoA

至此，我们可以把PCA类的降维问题总结成四步

1. **把所有样本点$X$统一移动到原点附近，也就是构造新样本点$X_1=HX$**
2. **把新样本点矩阵$X_1$乘他自己的转置$X_2=X_1X_1^T$，这个结果的形状是$N\times N$**
3. **求出来的这个方阵进行特征值分解，$X_2=Q\Sigma Q^T$，接着只取降维维度的前$q$个特征值和特征向量$Q',\Sigma'$**
4. **求解$X'=Q'\sqrt{\Sigma'}$获得降维坐标，其形状为$q\times N$**
